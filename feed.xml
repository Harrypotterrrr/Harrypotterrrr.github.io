<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://harrypotterrrr.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://harrypotterrrr.github.io//" rel="alternate" type="text/html" /><updated>2022-05-14T01:21:04-04:00</updated><id>https://harrypotterrrr.github.io//feed.xml</id><title type="html">Haolin Jia’s Homepage</title><subtitle>Keep working hard!
</subtitle><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><entry><title type="html">Feature Extraction and Disentanglement</title><link href="https://harrypotterrrr.github.io//2022/02/21/gan-4.html" rel="alternate" type="text/html" title="Feature Extraction and Disentanglement" /><published>2022-02-21T00:00:00-05:00</published><updated>2022-02-21T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2022/02/21/gan-4</id><content type="html" xml:base="https://harrypotterrrr.github.io//2022/02/21/gan-4.html"><![CDATA[<p>Feature Extraction and Disentanglement</p>

<!--more-->

<h2 id="feature-disentangle">Feature Disentangle</h2>

<ul>
  <li>The GAN receives one vector as input and output a desirable result.
    <ul>
      <li>We hope to control the characteristic of the output by mopdifying each specific value in the input vector.</li>
    </ul>
  </li>
  <li>For general GAN, modifying a specific dimension of the vector commonly change the feature of the result <strong>unconsciously</strong>
    <ul>
      <li>Because the actual distribution of each feature are intricate and <strong>entangled</strong> in the latent space.</li>
    </ul>
  </li>
</ul>

<h3 id="infogan">InfoGAN</h3>

<ul>
  <li>Split input <code class="language-plaintext highlighter-rouge">z</code> to two parts, <code class="language-plaintext highlighter-rouge">c</code> encodes the different feature in each dimension and <code class="language-plaintext highlighter-rouge">z'</code> as input noise.</li>
  <li>Classifier recover the predict <code class="language-plaintext highlighter-rouge">c</code> from the output <code class="language-plaintext highlighter-rouge">x</code> from the Generator, which supervises the generate to output <code class="language-plaintext highlighter-rouge">x</code> with the feature of <code class="language-plaintext highlighter-rouge">c</code>.</li>
  <li>Discriminator still output a scalar to represent the result good or not, but shares the parameter with Classifier except for the last output layer.
    <ul>
      <li>Without Discriminator, the output from Generator will only focus on <code class="language-plaintext highlighter-rouge">c</code> which benefit for the Classifier to predict, but generate bad results.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/03/OFJjEV.png" alt="gan-4-1" style="margin:auto;display:block;width:70%;" /></p>

<p><span class="ref"><a href="#reference-2">*reference-2</a></span></p>

<h3 id="vae-gan">VAE-GAN</h3>

<ul>
  <li>Based on VAE (variational auto-encoder), VAE-GAN combines VAE and GAN
    <ul>
      <li>VAE only generates obscure results.</li>
      <li>Discriminator</li>
    </ul>
  </li>
  <li>Encoder is to encode the input image from the dataset to a normal distribution code <code class="language-plaintext highlighter-rouge">z</code>, regularized by imposing a prior distribution over the latent distribution $p(z)</li>
  <li>Generator is to generate images and cheat the distriminator
    <ul>
      <li>Output the reconstruction image from the output <code class="language-plaintext highlighter-rouge">z</code> of Encoder: minimize the reconstruction</li>
      <li>Output the generated image from the noise sampled from the prior distribution: get <code class="language-plaintext highlighter-rouge">z</code> as closed to the normal distribution as possible.</li>
    </ul>
  </li>
  <li>
    <p>Discriminator is to distinguish the real, generated or reconstructed images.</p>
  </li>
  <li>Algorithm of the training the VAE-GAN
    <ul>
      <li>Initialize <code class="language-plaintext highlighter-rouge">Enc</code>, <code class="language-plaintext highlighter-rouge">Dec</code>, <code class="language-plaintext highlighter-rouge">D</code></li>
      <li>In each training iteration:
        <ul>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> images $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database distribution $P_{data}(x)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> codes $\{\tilde{z}^{1}, \tilde{z}^{2}, \ldots, \tilde{z}^{m}\}$ from encoder, $\tilde{z}^i = Enc(x^i)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> images $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}$ from decoder, $\tilde{x}^i = Dec(\tilde{z}^i)$.</li>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> images $\{\hat{x}^{1}, \hat{x}^{2}, \ldots, \hat{x}^{m}\}$ from decoder, $\hat{x}^i = Dec(z^i)$.</li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Update <code class="language-plaintext highlighter-rouge">Enc</code> to decrease reconstruction error of <code class="language-plaintext highlighter-rouge">MSE</code> $\lVert \tilde{x}^i - x^i \rVert$, decrease $\textit{KL-divergence}(P(\tilde{z}^i</td>
                  <td>x^i) \Vert P(z))$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>Update <code class="language-plaintext highlighter-rouge">Dec</code> to decrease reconstruction error of <code class="language-plaintext highlighter-rouge">MSE</code> $\lVert \tilde{x}^i - x^i \rVert$, increase binary cross entropy $D(\tilde{x}^i)$ and $D(\hat{x}^i)$.</li>
          <li>Update <code class="language-plaintext highlighter-rouge">D</code> to increase binary cross entropy $D(x^i)$, decrease $D(\tilde{x}^i)$ and $D(\hat{x}^i)$.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/03/OFJOH0.png" alt="gan-4-2" style="margin:auto;display:block;width:70%;" /></p>

<p class="success"><strong>Info</strong>: Another kind of discriminator can be implemented to output three labels of the result: real, generated and reconstructed.</p>

<ul>
  <li>Algorithm of the training VAE-GAN
    <ul>
      <li>Initialize <code class="language-plaintext highlighter-rouge">Enc</code>, <code class="language-plaintext highlighter-rouge">Dec</code>, <code class="language-plaintext highlighter-rouge">D</code></li>
      <li>In each training iteration:
        <ul>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> images $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database distribution $P_{data}(x)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> codes $\{\tilde{z}^{1}, \tilde{z}^{2}, \ldots, \tilde{z}^{m}\}$ from encoder, $\tilde{z}^i = Enc(x^i)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> images $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}$ from decoder, $\tilde{x}^i = Dec(\tilde{z}^i)$.</li>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
          <li>Generate <code class="language-plaintext highlighter-rouge">m</code> images $\{\hat{x}^{1}, \hat{x}^{2}, \ldots, \hat{x}^{m}\}$ from decoder, $\hat{x}^i = Dec(z^i)$.</li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Update <code class="language-plaintext highlighter-rouge">Enc</code> to decrease reconstruction error of <code class="language-plaintext highlighter-rouge">MSE</code> $\lVert \tilde{x}^i - x^i \rVert$, decrease $\textit{KL-divergence}(P(\tilde{z}^i</td>
                  <td>x^i) \Vert P(z))$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>Update <code class="language-plaintext highlighter-rouge">Dec</code> to decrease reconstruction error of <code class="language-plaintext highlighter-rouge">MSE</code> $\lVert \tilde{x}^i - x^i \rVert$, increase binary cross entropy $D(\tilde{x}^i)$ and $D(\hat{x}^i)$.</li>
          <li>Update <code class="language-plaintext highlighter-rouge">D</code> to increase binary cross entropy $D(x^i)$, decrease $D(\tilde{x}^i)$ and $D(\hat{x}^i)$.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p class="success"><strong>Info</strong>: Another kind of discriminator can be implemented to output three labels of the result: real, generated and reconstructed.</p>

<p><span class="ref"><a href="#reference-3">*reference-3</a></span></p>

<h3 id="bigan">BiGAN</h3>

<ul>
  <li>Make pair of the input and output from Encoder and Decoder to feed into the discriminator, distinguishing the the input come from the encoder or the decoder</li>
  <li>Encoder takes the image <code class="language-plaintext highlighter-rouge">x</code> from the dataset and generate code <code class="language-plaintext highlighter-rouge">z</code>, and make a pair <code class="language-plaintext highlighter-rouge">(x, z)</code> (the corresponding distribution <code class="language-plaintext highlighter-rouge">P(x, z)</code>) for discriminator.
    <ul>
      <li>Deceive Discriminator that the <code class="language-plaintext highlighter-rouge">P(x, z)</code> is from eecoder.</li>
    </ul>
  </li>
  <li>Decoder takes the code <code class="language-plaintext highlighter-rouge">z'</code> sampled from the prior distribution and generate image <code class="language-plaintext highlighter-rouge">x'</code>, and make a pair <code class="language-plaintext highlighter-rouge">(x', z)</code> (the corresponding distribution <code class="language-plaintext highlighter-rouge">Q(x', z')</code>) for discriminator.
    <ul>
      <li>Deceive Discriminator that the <code class="language-plaintext highlighter-rouge">Q(x', z')</code> is from dncoder.</li>
    </ul>
  </li>
  <li>Discriminator evaluate the difference between the distribution from encoder and decoder.</li>
  <li>After the well training, the discriminator can not distinguish the distribution from encoder and decoder.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">P(x, z)</code> will be the same as <code class="language-plaintext highlighter-rouge">Q(x', z')</code>.</li>
      <li>The embedding code <code class="language-plaintext highlighter-rouge">z</code> will be similar to the code generated from the prior distribution, and the image <code class="language-plaintext highlighter-rouge">x'</code> from decoder will be real.</li>
      <li>The cycle consistency holds, which is similar to <a href="/2022/01/25/gan-2#cyclegan">cycleGAN</a>:
        <ul>
          <li>Also, <code class="language-plaintext highlighter-rouge">Enc(x) = z</code> =&gt; <code class="language-plaintext highlighter-rouge">Dec(z') = x</code> for all <code class="language-plaintext highlighter-rouge">x'</code>.</li>
          <li>Similarly, <code class="language-plaintext highlighter-rouge">Dec(z) = x</code> =&gt; <code class="language-plaintext highlighter-rouge">Enc(x) = z</code> for all <code class="language-plaintext highlighter-rouge">z</code>.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/03/OFJvNT.png" alt="gan-4-3" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>Algorithm of the training Bi-GAN</li>
  <li>Initialize <code class="language-plaintext highlighter-rouge">Enc</code>, <code class="language-plaintext highlighter-rouge">Dec</code>, <code class="language-plaintext highlighter-rouge">D</code></li>
  <li>In each training iteration:
    <ul>
      <li>Sample <code class="language-plaintext highlighter-rouge">m</code> images $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database distribution $P_{data}(x)$.</li>
      <li>Generate <code class="language-plaintext highlighter-rouge">m</code> codes $\{\tilde{z}^{1}, \tilde{z}^{2}, \ldots, \tilde{z}^{m}\}$ from encoder, $\tilde{z}^i = Enc(x^i)$.</li>
      <li>Sample <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
      <li>Generate <code class="language-plaintext highlighter-rouge">m</code> images $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}$ from decoder, $\tilde{x}^i = Dec(z^i)$.</li>
      <li>Update <code class="language-plaintext highlighter-rouge">D</code> to increase $D(x^i, \tilde{z}^i)$, decrease $D(\tilde{x}^i, z^i)$.</li>
      <li>Update <code class="language-plaintext highlighter-rouge">Enc</code> to decrease $D(x^i, \tilde{z}^i)$.</li>
      <li>Update <code class="language-plaintext highlighter-rouge">Dec</code> to increase $D(\tilde{x}^i, z^i)$.</li>
    </ul>
  </li>
</ul>

<p class="info"><strong>Note</strong>: It doesn’t matter that the <code class="language-plaintext highlighter-rouge">D</code> gives the positive score to the pair from <code class="language-plaintext highlighter-rouge">Enc</code> or another. What matters is that the score <code class="language-plaintext highlighter-rouge">D</code> gives to <code class="language-plaintext highlighter-rouge">Enc</code> or <code class="language-plaintext highlighter-rouge">Dec</code> should be <strong>opposite</strong>. The objective of <code class="language-plaintext highlighter-rouge">Enc</code> and <code class="language-plaintext highlighter-rouge">Dec</code> should also be the opposite of the objective of <code class="language-plaintext highlighter-rouge">D</code> so that <code class="language-plaintext highlighter-rouge">D</code> could not discriminate between the generated pair after the advesarial training.</p>

<ul>
  <li>Based on the <strong>self-supervised</strong> way ofBiGAN, <a href="/2022/01/25/gan-2#cyclegan">cycleGAN</a> introduces the cycle consistency loss.</li>
</ul>

<p><span class="ref"><a href="#reference-4">*reference-4</a></span></p>

<h3 id="triple-gan">Triple GAN</h3>

<ul>
  <li>Semi-supervised learning way of training GAN consists of three parts: classifier, generator, discriminator.</li>
  <li><strong>Generator</strong> receives the sampling noise $Z_g$ and conditional distribution $Y_g$ from the prior distribution, and output the pair of the image with the label $(X_g, Y_g)$</li>
  <li><strong>Classifier</strong> outputs the label of the image,and  disentangle the class and style of the input in both supervised and unsupervised way:
    <ul>
      <li>Take the output image-label pairs from generator, calculate the cross entropy reconstruction loss between predicted label $\tilde{Y}_g$ with $Y_g$. This process is using weak self-supervised sample to train the classifier.</li>
      <li>Take sampling pair $(X_l, Y_L)$ from the dataset and supervised learning with the cross entropy reconstruction loss between the predicted $\tilde{Y}_l$ and $Y$.</li>
      <li>Classify sampling image $X_c$ as the predicted label $Y_c$. The predicted pair is sent to the discriminator.</li>
    </ul>
  </li>
  <li><strong>Discriminator</strong> solely identifies fake image-label pairs.
    <ul>
      <li>Distinguish the real image-label pair sampled from the dataset directly.</li>
      <li>Distinguish the generated image-label pair from the generator.</li>
      <li>Distinguish the image and predicted label pair from the classifier.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/03/OFJx4U.png" alt="gan-4-4" style="margin:auto;display:block;width:70%;" /></p>

<p><span class="ref"><a href="#reference-5">*reference-5</a></span></p>

<h3 id="domain-adversarial-training">Domain-adversarial Training</h3>

<ul>
  <li>The network consists of three parts:
    <ul>
      <li><strong>Feature extractor</strong>: extract and map the input $x$ to the latent space.
        <ul>
          <li>To maximize label classification accuracy</li>
          <li>To minimize domain lcassification accuracy</li>
        </ul>
      </li>
      <li><strong>Label predictor</strong>: predict the class label of the feature from the extractor.
        <ul>
          <li>To maximize label classification accuracy</li>
        </ul>
      </li>
      <li><strong>Domain classifier</strong>: classify the source domain of the feature from the extractor.
        <ul>
          <li>To maximize domain classification accuracy</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Gradient reversal layer is an identity function during the forward propagation, but it multiplies its input by -1 during backpropagation.
    <ul>
      <li>Because it leads the gradient ascent on the feature extractor with respect to the classification loss of Domain predictor, but gradient descent on the predictor itself.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/03/OFYSCF.png" alt="gan-4-5" style="margin:auto;display:block;width:70%;" /></p>

<p><span class="ref"><a href="#reference-6">*reference-6</a></span></p>

<h3 id="feature-disentangle-1">Feature Disentangle</h3>

<ul>
  <li>The encoding from the original encoder includes mixture information, which is <strong>entangled</strong>.</li>
  <li>Embedding audio signal can be roughly classified as two parts
    <ul>
      <li>Phonetic information: audio content, structures and semantic meaning</li>
      <li>Speaker information: audio speaker acoustic characteristics</li>
    </ul>
  </li>
  <li>Utilize two encoder to <strong>disentangle</strong> the above two information in the latent space: Phonetic encoder and Speaker encoder.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/14/OyWYff.png" alt="gan-4-6" style="margin:auto;display:block;width:80%;" /></p>

<ul>
  <li>For the audio source from the same or different speakers, the speaker vector $v_s$ from Speaker Encoder $E_s$ will be constrained as close or far apart as possible.
    <ul>
      <li>To guide the Speaker Encoder $E_s$ <strong>only</strong> extract the <strong>acoustic characteristic</strong> to the speaker vector $v_s$.</li>
    </ul>
  </li>
  <li>For inputs with different content and structures, the Phonetic Encoder $E_p$ embedding the Phonetic vector $v_p$, which is fed into Speaker Discriminator $D_s$ to distinguish if the source is from the same speaker or not.
    <ul>
      <li>To instruct the Phonetic Encoder $E_p$ embeds <strong>all information except for the speaker characteristic</strong>.</li>
    </ul>
  </li>
  <li>Speaker Classifier $D_s$ is inspired from domain adversarial training, which outputs the score, representing how <strong>confident</strong> the discriminator $D_s$ will be that two audio source are from the identical speaker.
    <ul>
      <li>The higher score means more confident on the same speaker.</li>
      <li>Phonetic encoder $E_p$ learns to <strong>confuse</strong> Speaker Classifier $D_s$.</li>
      <li>After the well training, $D_s$ fail to distinguish whether the source is from the same speaker or not, and $E_p$ only generates embeddings with phonetic information.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/14/OyWNp8.png" alt="gan-4-7" style="margin:auto;display:block;width:70%;" /></p>

<p><span class="ref"><a href="#reference-7">*reference-7</a></span></p>

<ul>
  <li>Each dimension of the input vector <strong>represents some characteristics</strong>, but the explicit relationship is <strong>unknown</strong>.
    <ul>
      <li><strong>Disentangle</strong>: understand the meaning of each dimension so as to control the output.</li>
    </ul>
  </li>
</ul>

<h3 id="attribute-modification">Attribute Modification</h3>

<ul>
  <li>Compute the center of each sampling with the same attributes and compute the distance between different sampling centers.</li>
</ul>

\[z_{\text {long }}=\frac{1}{N_{1}} \sum_{x \in \text { long }} \operatorname{En}(x)-\frac{1}{N_{2}} \sum_{x^{\prime} \notin \text { long }} \operatorname{En}\left(x^{\prime}\right)\]

<p><img src="https://s1.ax1x.com/2022/05/14/O6NOAJ.png" alt="gan-4-8" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>Transform the characteristic by <strong>compensating the distance</strong> in the latent space.</li>
</ul>

\[x \Rightarrow \operatorname{En}(x)+z_{\text {long }}=z^{\prime} \Rightarrow \operatorname{Gen}\left(z^{\prime}\right)\]

<h3 id="photo-editing">Photo Editing</h3>

<ul>
  <li>Basic idea: find the optimum code $z^*$ in the latent space, closed to the original input image, at the same time fulltill the constraint.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/14/O6NXN9.png" alt="gan-4-9" style="margin:auto;display:block;width:60%;" /></p>

<ul>
  <li>Three methods to backtrace code $z^*$ from the input $x$:
    <ol>
      <li>Gradient descent to find the optimal target:<br />
 $z^{*}=\arg \min _{z} L\left(G(z), x\right)$<br />
 The difference between $G(z)$ and $x$ can be measured by:</li>
    </ol>
    <ul>
      <li>Pixel-wise distance</li>
      <li>Another extractor network to evaluate the high level distance
        <ol>
          <li>Well-trained auto-encoder structure to extract the latent code, transforming the input image back.</li>
          <li>Using the result from method 2 as the initialization of method 1 and fine-tune.</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Editing photos with input constraint:
    <ul>
      <li>$z_0$ is the code of the input image previously found.</li>
      <li>$U$ is the function to judge if the final generation fulfill the constaint of the editing.</li>
      <li>$\left|z-z_{0}\right|$ make sure the result is not too far away from the original image.</li>
      <li>$D$ discriminator is to check the image realistic or not.
\(z^{*}=\arg \min _{z} U(G(z))+\lambda_{1}\left\|z-z_{0}\right\|^{2}-\lambda_{2} D(G(z))\)</li>
    </ul>
  </li>
</ul>

<p><span class="ref"><a href="#reference-8">*reference-8</a></span>
<span class="ref"><a href="#reference-9">*reference-9</a></span></p>

<h3 id="others">Others</h3>

<p><strong>TODO</strong></p>

<ul>
  <li>Many tasks as <a href="#reference-10">Image super resolution</a>, <a href="#reference-11">Image completion</a>, etc. are based on condition GAN.</li>
</ul>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php">Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee</a><a name="reference-1"></a></li>
  <li><a href="https://arxiv.org/abs/1606.03657">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a><a name="reference-2"></a></li>
  <li><a href="https://arxiv.org/abs/1512.09300">Autoencoding beyond pixels using a learned similarity metric</a><a name="reference-3"></a></li>
  <li><a href="https://arxiv.org/abs/1605.09782">Adversarial Feature Learning</a><a name="reference-4"></a></li>
  <li><a href="https://arxiv.org/abs/1703.02291">Triple Generative Adversarial Nets</a><a name="reference-5"></a></li>
  <li><a href="https://arxiv.org/abs/1505.07818">Domain-Adversarial Training of Neural Networks</a><a name="reference-6"></a></li>
  <li><a href="https://arxiv.org/abs/1811.02775">Improved Audio Embeddings by Adjacency-Based Clustering with Applications in Spoken Term Detection</a><a name="reference-7"></a></li>
  <li><a href="https://arxiv.org/abs/1609.03552">Generative Visual Manipulation on the Natural Image Manifold</a><a name="reference-8"></a></li>
  <li><a href="https://arxiv.org/abs/1609.07093">Neural Photo Editing with Introspective Adversarial Networks</a><a name="reference-9"></a></li>
  <li><a href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a><a name="reference-10"></a></li>
  <li><a href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf">Globally and Locally Consistent Image Completion</a><a name="reference-11"></a></li>
</ol>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer vision" /><category term="generative model" /><category term="notes" /><summary type="html"><![CDATA[Feature Extraction and Disentanglement]]></summary></entry><entry><title type="html">Advanced Thoery and Problems of GAN</title><link href="https://harrypotterrrr.github.io//2022/02/11/gan-3.html" rel="alternate" type="text/html" title="Advanced Thoery and Problems of GAN" /><published>2022-02-11T00:00:00-05:00</published><updated>2022-02-11T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2022/02/11/gan-3</id><content type="html" xml:base="https://harrypotterrrr.github.io//2022/02/11/gan-3.html"><![CDATA[<p>Advanced Thoery and Problems of GAN</p>

<!--more-->

<h2 id="general-framework-of-gan">General Framework of GAN</h2>

<h3 id="f-divergence">F-divergence</h3>

<ul>
  <li>$P$ and $Q$ are two distributions. $p(x)$ and $q(x)$ are the probability of sampling $x$.</li>
  <li>f-divergence $D_{f}(P | Q)$ evaluates the difference of $P$ and $Q$.
    <ul>
      <li>$f$ is convex function</li>
      <li>when $P$ and $Q$ are the same distributions, $p(x) = q(x)$, $f(\frac{p(x)}{q(x)}) = f(1) = 0$</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
D_{f}(P \| Q) &amp;=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x \\
&amp; \geq f\left(\int_{x} q(x) \frac{p(x)}{q(x)} d x\right) \color{red}{\text{Jessen Inequality}} \\
&amp; = f\left(\int_{x}p(x) d x\right) \\
&amp; = f(1) \\
&amp; = 0
\end{aligned}\]

<p class="info"><strong>Note</strong>: Considering $f$ is the convex function, the above inequation can be established according to <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jessen inequation</a>:<br />
$ \varphi\left(\frac{1}{b-a} \int_{a}^{b} f(x) d x\right) \leq \frac{1}{b-a} \int_{a}^{b} \varphi(f(x)) d x $</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">f</code> function can be different</li>
</ul>

\[\begin{aligned}
&amp;\text{When } f(x)=x \log x \text{ , KL-divergence:}\\
&amp;D_{f}(P \| Q)=\int_{x} q(x) \frac{p(x)}{q(x)} \log \left(\frac{p(x)}{q(x)}\right) d x=\int_{x} p(x) \log \left(\frac{p(x)}{q(x)}\right) d x \\[1em]
&amp;\text{When }f(x)=-\log x \text{ , reversed KL-divergence:}\\
&amp;D_{f}(P \| Q)=\int_{x} q(x)\left(-\log \left(\frac{p(x)}{q(x)}\right)\right) d x=\int_{x} q(x) \log \left(\frac{q(x)}{p(x)}\right) d x \\[1em]
&amp;\text{When }f(x)=(x-1)^{2} \text{ , Chi Square-divergence:}\\
&amp;D_{f}(P \| Q)=\int_{x} q(x)\left(\frac{p(x)}{q(x)}-1\right)^{2} d x=\int_{x} \frac{(p(x)-q(x))^{2}}{q(x)} d x
\end{aligned}\]

<h3 id="fenchel-conjugate">Fenchel Conjugate</h3>

<ul>
  <li><strong>Fenchel Conjugate</strong> (Convex conjugate 凸共轭): the paired appearance of two functions with a strong relationship.
    <ul>
      <li>Duality (对偶) relationship is built on the same linear hyperplane.</li>
    </ul>
  </li>
  <li>The <strong>purpose</strong> of using conjugate functions:
    <ul>
      <li>The advantage is that even if a function is not convex, <strong>a convexed function</strong> can be obtained by the conjugate method.</li>
      <li>Even better, through the conjugation again, we can get a <strong>good approximation</strong> function with the original function.</li>
      <li>The convex hull function of the original function can be obtained by conjugating <strong>twice</strong>, where many excellent properties of the new convex hull will be obtained in the optimization solution with just a little loss.</li>
    </ul>
  </li>
  <li>Every convex function $f$ has a conjugate function $f^*$</li>
</ul>

\[f^{*}(t) =\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\}\]

<ul>
  <li>The variable of $f^* (t)$ is $t$, and get the maximum for every possible value of $x$.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/18/LdZtRP.png" alt="gan-3-1" style="margin:auto;display:block;width:80%;" /></p>

<ul>
  <li>We can get a good approximation function with the original function
    <ul>
      <li>E.g. $f(x) = xlogx$ can be approximated as $f^*(t) = e^{t-1}$.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/18/LdZYGt.png" alt="gan-3-2" style="margin:auto;display:block;width:60%;" /></p>

\[\begin{aligned}
&amp;\text{Take } f(x) \text{ as } xlogx \text{ :}\\
&amp;\quad f^{*}(t) =\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\} = \max _{x \in \operatorname{dom}(f)}\{x t-x \log x\}\\
&amp;\text{Let } g(x) =x t-x \log x \text {, Given } t, \text { find } x \text { to maximize } g(x) \text{:}\\
&amp;\quad t-\log x-1=0 \quad \\ 
&amp;\quad x= e^{t-1} \\
&amp;\quad f^{*}(t)= e^{t-1} \times t-e^{t-1} \times(t-1)=e^{t-1} \\
\end{aligned}\]

<h3 id="connection-with-gan">Connection with GAN</h3>

<ul>
  <li>Derivation of GAN’s derivation:
    <ul>
      <li>From the convex conjugate:<br />
 \(f^{*}(t)=\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\} \longleftrightarrow f(x)=\max _{t \in \operatorname{dom}\left(f^{*}\right)}\left\{x t-f^{*}(t)\right\}\)</li>
      <li>
        <p>Take $x$ as $\frac{p(x)}{q(x)}$ :<br />
 \(\begin{aligned}
 D_{f}(P \| Q) &amp;=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x \\
 &amp;=\int_{x} q(x)\left(\max _{t \in \operatorname{dom}\left(f^{*}\right)}\left\{\frac{p  (x)}{q(x)} t-f^{*}(t)\right\}\right) d x \\
 &amp;\color{red}{\geq} \int_{x} q(x)\left(\frac{p(x)}{q(x)} \color{red}{D(x)}-f^{*}( \color{red}{D(x)})\right) d x \\
 &amp;=\int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*}(D(x)) d x \\
 \end{aligned}\)</p>
      </li>
      <li><strong>The reason of</strong> replacing $\max$ item with $D(x)$ as above in red color:
        <ul>
          <li>$D(x)$ takes $x$ as input and output $t$.</li>
          <li>The limited capcity of $D$ determines the lower bound of $\max$ item.</li>
        </ul>
      </li>
      <li>
        <p>For the divergence between $P$ and $Q$:<br />
\(\begin{aligned}
 D_{f}(P \| Q) &amp;\approx \max _{\mathrm{D}} \int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*} (D(x)) d x \\
 &amp;=\max _{\mathrm{D}}\left\{E_{x \sim P}[D(x)]-E_{x \sim Q}\left[f^{*}(D(x))\right] \right\} \\
 &amp;\quad\text { Samples from P } \quad \text { Samples from Q } \\
 \end{aligned}\)</p>
      </li>
      <li>Convert the conjugate to the more <strong>general</strong> form of GAN’s objective by sampling from $P_{data}$ and $P_G$:
\(D_{f}\left(P_{\text {data}} \| P_{G}\right)=\max _{\mathrm{D}}\left\{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}\left[f^{*}(D(x))\right]\right\} \\
 \begin{aligned}
 G^{*} &amp;=\arg \min _{G} D_{f}\left(P_{d a t a} \| P_{G}\right) \\
 &amp;=\arg \min _{G} \max _{D}\left\{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}\left[f^{*}(D(x))\right]\right\} \\
 &amp;=\arg \min _{G} \max _{D} V(G, D)
 \end{aligned}\)
        <ul>
          <li><strong>Therefore</strong>, according to the different <strong>definition</strong> of $V(G,D)$, we can have different <strong>divergence</strong>, which corresponds to different $f^*$ <strong>function</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/18/LdZaM8.png" alt="gan-3-3" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-2">*reference-2</a></span></p>

<ul>
  <li><strong>What benefit we can get to try different divergence</strong> (and corresponding $f^*$ function)?</li>
</ul>

<h3 id="mode-collapseand--dropping">Mode Collapseand &amp; Dropping</h3>

<ul>
  <li>Mode collapse and Mode Dropping refer to <strong>reduced variety</strong> in the samples produced by a generator.
    <ul>
      <li><strong>Mode Collapse</strong>: The generator synthesizes samples with <strong>intra-mode</strong> variety, but some modes are missing.
        <ul>
          <li>E.g. Generator outputs limited variaty of faces with many duplicates.</li>
        </ul>
      </li>
      <li><strong>Mode Dropping</strong>: The generator synthesizes samples with <strong>inter-mode</strong> variety, but each mode lacks variety.
        <ul>
          <li>E.g. Generator outputs various faces at once, but all with the same one feature, like white skins.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/18/LdZdsS.png" alt="gan-3-4" style="margin:auto;display:block;width:60%;" /></p>

<ul>
  <li>
    <p>The problem may be caused by the bad selection of <strong>divergence</strong></p>
  </li>
  <li>
    <p>As the below picture shows:</p>
    <ul>
      <li>The left figure shows the problem of <code class="language-plaintext highlighter-rouge">KL-divergence</code>, where the distribution of the well-trained generator falls between the peak of the true data distribution.
        <ul>
          <li><strong>This is also one explanation</strong> of why traditional generation, like autoencoder, output obscure result by choosing KL-divergence and minimizing it (aka. maximizing the likelihood estimation).</li>
          <li>The distribution between the peak refers to those obscure generated images.</li>
        </ul>
      </li>
      <li>The right figure shows the mode collapse and dropping problem, where the trained distribution mainly lies in one single peak.
        <ul>
          <li>This is one explanation of why advesarial training often encounters the limited diversity or limited features problems.</li>
          <li>The missing distribution of generator referes to the missing features.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/18/LdZNxf.png" alt="gan-3-5" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>But choosing different divergence can not solve the problem completely.
    <ul>
      <li>One triky workarounds is train the different generator, each with limited diversity and mode collapse or dropping problems. Gather to be the better generator.</li>
    </ul>
  </li>
</ul>

<h2 id="improving-gan">Improving GAN</h2>

<h3 id="problem-of-js-divergence">Problem of JS-divergence</h3>

<ul>
  <li>In most cases $P_G$ and $P_{data}$ are not overlapped for the following reason:
    <ul>
      <li>The <strong>nature</strong> of data determines that $P_{data}$ and $P_G$ are low-dimensional manifold in high-dimensional space. Thus the overlap of two distribution can be ignored</li>
      <li><strong>Limited sampling</strong> cause that even though the distribution of $P_{data}$ and $P_G$ have overlap, the discriminator frequently fails to measure the divergence between the distribution of sampling which is still sparse without enough overlap.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/28/LO9iXn.png" alt="gan-3-6" style="margin:auto;display:block;width:70%;" /></p>

<p class="info"><strong>Info</strong>: Images are the low-dimensional manifold in high-dimensional space. If we hypothesis the space is only 3D, the images is a line or plane in this 3D space.</p>

<ul>
  <li>Considering the objective of Discriminator:
 \(V(G,D)=-2log2 + \color{red}{2JS\left(P_{\text {data}} \| P_{\text {G}}\right)}\)
    <ul>
      <li>If there is nearly no overlap between two distribution, the value of JS-divergence would be <code class="language-plaintext highlighter-rouge">2log2</code> constant item, the <strong>gradient</strong> of which would be <em>zero</em>, which could cause the training stop.</li>
      <li>And it is very common that two distribution have little overlap in high-dimensional space.</li>
    </ul>
  </li>
  <li>As the following figure shows:
    <ul>
      <li>$P_{G_1}$ performs better than $P_{G_0}$ theoretically because it is closer to $P_{data}$, but the JS-divergence of two are all <code class="language-plaintext highlighter-rouge">log2</code> (ignore the coefficient <code class="language-plaintext highlighter-rouge">2</code> ahead).</li>
      <li>Only two distribution are exactly the same, the <code class="language-plaintext highlighter-rouge">JS-divergence</code> will be <code class="language-plaintext highlighter-rouge">0</code>, which is the final target of generator.</li>
    </ul>
  </li>
</ul>

<p class="warning"><strong>Note</strong>: Strictly, according to the above equation of the objective of the discriminator, the JS-divergence is closed to <code class="language-plaintext highlighter-rouge">0</code> and the total objective is <code class="language-plaintext highlighter-rouge">log2</code> if there is barely enough overlap between distributions.</p>

<ul>
  <li>The generator is to minimize the divergence, but the same value <code class="language-plaintext highlighter-rouge">log2</code> of the objective gives no guide for the generator to judge which situation is worse. In other words, $P_{G_0}$ and $P_{G_1}$ are considered <strong>equally bad</strong>. Therefore, the generator will not update from $P_{G_0}$ to $P_{G_1}$.</li>
  <li>On the other hand, the discriminator also consider $P_{G_0}$ and $P_{G_1}$ <strong>equally bad</strong>. Considering only when two distribution are exactly the same, <code class="language-plaintext highlighter-rouge">JS-divergence</code> is <code class="language-plaintext highlighter-rouge">0</code>, binary classifier will always achieves <code class="language-plaintext highlighter-rouge">100%</code> accuracy.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/28/LO9P6s.png" alt="gan-3-7" style="margin:auto;display:block;width:70%;" /></p>

<p class="error"><strong>Most important</strong>: For the discriminator, a binary classifier, as long as the output of the classification is the false label, the loss is constant regardless of the deviation between the current $P_G$ and $P_{data}$. This property of <code class="language-plaintext highlighter-rouge">JS-divergence</code> makes it impossible to measure the distance between the non-overlapping distributions and hence the generator cannot be optimized properly.</p>

<h4 id="lsgan">LSGAN</h4>

<ul>
  <li>In the binary classifier point of view, the <strong>gradient vanishing</strong> occurs when generate distribution is distinguished as fake too well after the <code class="language-plaintext highlighter-rouge">sigmoid</code> nonlinear transform.
    <ul>
      <li>Low-dimensional manifold in the high-dimensional space has almost no overlap, so the discriminator can clearly classified into 0 and 1, so that the gradient vanishing concentrates at both ends.</li>
    </ul>
  </li>
  <li><strong>Least Square GAN</strong> (LSGAN): replace the sigmoid function with the linear, converting the <strong>binary classification</strong> problem into the <strong>regression</strong> problem.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/Opj58K.png" alt="gan-3-8" style="margin:auto;display:block;width:70%;" /></p>

<h2 id="wasserstein-gan">Wasserstein GAN</h2>

<h3 id="wasserstein-distance">Wasserstein Distance</h3>

<ul>
  <li>Instead of using divergence to measure the distance between distributions, Wasserstein GAN (WGAN) use <strong>Earth Mover’s Distance</strong>.
    <ul>
      <li>Consider one distribution $P$ as a pile of earth and another distribution $Q$ as the target.</li>
      <li><strong>Wasserstein Distance</strong>, also descriptively called <strong>Earth Mover’s Distance</strong> (EMD): the <strong>minimal</strong> total amount of work it takes to transform one heap into the other (initial distribution to the target).<br />
\(W(P, Q) = d\)</li>
      <li>The work defines as the amount of earth in a chunk times the distance it was moved.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/OpjIgO.png" alt="gan-3-9" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>
    <p>Assume two discrete distributions $P_r$ and $P_{\theta}$, each with $l$ possible states $x$ or $y$ respectively.</p>
  </li>
  <li>
    <p>There are infinitely ways to move the earth around, but we only need to find the optimal one for EMD.</p>
    <ul>
      <li>Denote $\gamma(x, y)$ as the amount of earth distributed from $x$ to the domain of $y$.</li>
      <li>The constraints $\sum_{x} \gamma(x, y)=P_{r}(y)$ and $\sum_{y} \gamma(x, y)=P_{\theta}(x)$ holds.</li>
      <li>We require that $\gamma \in \Pi (P_{r}, P_{\theta})$, where $\Pi (P_{r}, P_{\theta})$ is the set of all distributions whose margains are $P_r$, $P_{\theta}$ respectively.</li>
      <li>We multiply every value of $\gamma$ with the Euclidian distance between $x$ and $y$.</li>
    </ul>
  </li>
</ul>

\[\operatorname{EMD}\left(P_{r}, P_{\theta}\right)=\inf _{\gamma \in \Pi} \sum_{x, y}\|x-y\| \gamma(x, y)=\inf _{\gamma \in \Pi} \mathbb{E}_{(x, y) \sim \gamma}\|x-y\|=\inf _{\gamma \in \Pi}\langle\mathbf{D}, \mathbf{\Gamma}\rangle_{\mathrm{F}}\]

<ul>
  <li>$\inf$ stands for <code class="language-plaintext highlighter-rouge">infimum</code>, or the greatest lower bound (下确界), roughly meaning the minimum, the opposite of which is $\sup$ for <code class="language-plaintext highlighter-rouge">supremum</code> (上确界)</li>
  <li>The distribution strategy is set as $\mathbf{\Gamma}=\gamma(x, y)$, and distance is $\mathbf{D}=\lVert x-y \rVert$, with $ \boldsymbol{\Gamma}, \mathbf{D} \in \mathbb{R}^{l \times l} $, then the definition of EMD will be:</li>
  <li>${\langle , \rangle}_{F}$ is the <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius inner product</a>, the sum of all the element-wise products.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/Opjf4x.png" alt="gan-3-10" style="margin:auto;display:block;width:70%;" /></p>

<p><span class="ref"><a href="#reference-3">*reference-3</a></span></p>

<h3 id="wgan">WGAN</h3>

<ul>
  <li>Instead of the Divergence $D_f (P_{data} \Vert P_G)$, which consider the generated distribution equally bad when there is not much overlap in space, the Wasserstein Distance $W(P_{data}, P_G)$ gives the exact distance function between $P_G$ and $P_{data}$</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/Opj4C6.png" alt="gan-3-11" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>By solving the optimization problem and finding the best $\gamma(x, y)$, we can evaluate wasserstein distance between $P_{data}$ and $P_G$:
    <ul>
      <li>maximize the $D(x)$ where $x$ is from $P_{data}$</li>
      <li>minimize the $D(x)$ where $x$ is from $P_G$</li>
    </ul>
  </li>
</ul>

\[V(G,D) = \max_{D \in \textit{1-Lipschitz}} \left\{ E_{x \sim P_{\text {data}}} [D(x)] - E_{x \sim P_G} [D(x)] \right\}\]

<p><strong>TODO</strong>: the derivation.</p>

<ul>
  <li>$D \in \textit{1-Lipschitz}$: D has to be <strong>smooth</strong> enough, so that the generated distribution and the real distribution will not be distinguished too much.
    <ul>
      <li>Without this constraint, the generated score will become small to $-\inf$, while the real will be large as $+\inf$, which will make network hard to train due to the large weight and never converge.</li>
      <li>Thus, keep discriminator smooth is one way to keep the score from $\inf$.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/OpjWU1.png" alt="gan-3-12" style="margin:auto;display:block;width:40%;" /></p>

<ul>
  <li><strong>Lipschitz Continuity</strong>: the function limited how fast it can change.
    <ul>
      <li>When $ K = 1$ is $\textit{1-Lipschitz}$</li>
    </ul>
  </li>
</ul>

\[\underset{\text{output change}}{\lVert f(x_1) - f(x_2) \rVert} \le K \underset{\text{input change}}{\lVert x_1 - x_2 \rVert}\]

<ul>
  <li><strong>Weight Clipping</strong>: a simple but efficient way to <strong>enforce</strong> a Lipschitz constraint.
    <ul>
      <li>Force the parameters $w$ between $c$ and $-c$</li>
      <li>After parameters update, $ \text{if } w \gt c, w = c, \text{if } w \lt -c, w = -c$</li>
    </ul>
  </li>
</ul>

<p><span class="ref"><a href="#reference-4">*reference-4</a></span></p>

<h3 id="wgan-gp">WGAN-GP</h3>

<ul>
  <li><strong>WGAN-GP</strong>: Wasserstin GAN with the gradient penalty.
    <ul>
      <li>A  differentiable function is <code class="language-plaintext highlighter-rouge">1-Lipschitz</code> if and only if it has gradients with a norm less than or equal to 1 everywhere.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
&amp;D \in \textit{1-Lipschitz}\Longleftrightarrow  \left\|\nabla_{x} D(x)\right\| \le 1, \quad \text{for all } x\\[8px]
&amp;\begin{aligned}
V(G, D) 
&amp;=\max _{D \in \textit { 1-Lipschitz }}\left\{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)]\right\} \\
&amp; \approx \max _{D} \{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)] -\lambda \int_{x} \max _{x}\left(0,\left\|\nabla_{x} D(x)\right\|-1\right) d x\} \\
\end{aligned}
\end{aligned}\]

<ul>
  <li>Instead of promising all constraints of the gradient for all $x$ because enforcing the Lipschitz constraint <strong>everywhere</strong> is intractable, we only guarantee $x \in P_{penalty}$.</li>
</ul>

\[\begin{aligned}
V(G, D) 
&amp; \approx \max _{D} \{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)] -\underset{\color{red}{\text{Prefer } \left\|\nabla_{x} D(x)\right\|\le 1, \text{ for all } x}}{\lambda \int_{x} \max _{x}\left(0,\left\|\nabla_{x} D(x)\right\|-1\right) d x}\} \\
&amp; \approx \max _{D} \{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)] -\underset{\color{red}{\text{Prefer } \left\|\nabla_{x} D(x)\right\|\le 1, \text{ for } x \text{ sampling from } x \sim P_{penalty}}}{\lambda E_{x \sim  P_{\text {penalty}}} [\max _{x}\left(0,\left\|\nabla_{x} D(x)\right\|-1\right)]} \}
\end{aligned}\]

<ul>
  <li>By randomly sampling, enforcing the $x$ only along the straight lines, as following figure shows, is sufficient and experimentally results in good performance.</li>
  <li>Only give gradient constraint to the region between $P_{data}$ and $P_G$:
    <ul>
      <li>This region influences how $P_G$ moves to $P_{data}$.</li>
      <li>$P_G$ will move towards $P_{data}$, passing through the $P_{penalty}$ region.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/OpjovD.png" alt="gan-3-13" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li>In practical, the penalty is set as <code class="language-plaintext highlighter-rouge">L2-norm</code>, which constrain the largest gradient to 1
    <ul>
      <li>Overly large gradients penalty works but too small gradient will also cause gradient vanishing, which will slow down the convergence.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
V(G, D) 
&amp; \approx \max _{D} \{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)] -\lambda E_{x \sim  P_{\text {penalty}}} [\max _{x}\left(0,\left\|\nabla_{x} D(x)\right\|-1\right)] \} \\
&amp; \approx \max _{D} \{E_{x \sim P_{\text {data}}}[D(x)]-E_{x \sim P_{G}}[D(x)] -\lambda E_{x \sim  P_{\text {penalty}}} [(\left\|\nabla_{x} D(x)\right\|-1 )^{2}] \}
\end{aligned}\]

<ul>
  <li>Now discriminator will replace the last layer of the <strong>sigmoid function</strong> with wasserstain function to measure the distance between distributions, transmitting the problem from the <strong>binary classification</strong> to <strong>regression</strong>.</li>
</ul>

<p><span class="ref"><a href="#reference-5">*reference-5</a></span></p>

<h3 id="spectrum-normalization">Spectrum Normalization</h3>

<ul>
  <li><strong>Spectral Norm</strong>: keep the gradient norm smaller than <code class="language-plaintext highlighter-rouge">1</code> everywhere.</li>
  <li>We keep Lipschitz continuity for the weight $W$ of each layer of the neural network $f(x) = Wx$.
    <ul>
      <li>Each matrix of $W$ divide by the <strong>sigular value</strong> of itself will guarantee <code class="language-plaintext highlighter-rouge">1-Lipschitz</code> continuity.</li>
    </ul>
  </li>
</ul>

<p><strong>TODO</strong>: paper, https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/87220341</p>

<p><span class="ref"><a href="#reference-6">*reference-6</a></span></p>

<h2 id="other-improvement">Other Improvement</h2>

<h3 id="ebgan">EBGAN</h3>

<ul>
  <li><strong>Energy-based GAN</strong> (EBGAN): the same generator but the discriminator is with an auto-encoder framework.
    <ul>
      <li>EBGAN outputs the reconstruction image and get the <strong>reconstruction error</strong> to the input of the discriminator.</li>
      <li>The error is the energy to determine the goodness of the generated result. The energy takes low values when it is the correct label and higher values for incorrect label.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/Opj7Ke.png" alt="gan-3-14" style="margin:auto;display:block;width:70%;" /></p>

<ul>
  <li><strong>Benefit</strong>: The auto-encoder discriminator can be pre-trained by real images without generator.
    <ul>
      <li>The discriminator can be expert at distinguish the real or fake at the beginning of the training the generator.</li>
    </ul>
  </li>
  <li>
\[\begin{aligned}
&amp;\mathcal{L}_{D}(x, z)=D(x)+\max (m-D(G(z))) \\
&amp;\mathcal{L}_{G}(z)=D(G(z))
\end{aligned}\]
  </li>
  <li>For discriminators, to minimize $\mathcal{L}_{D}$ is to maximize $D(G(z))$, which means to raise the following curve in blue region.</li>
  <li>For generators, it makes effort to generate results with low energy.</li>
  <li>Without constraint, the curve will be lifted without limitation.
    <ul>
      <li>It is <strong>easy</strong> for the discriminator to increase the energy of fake result, lifting the curve, but <strong>hard</strong> for the generator to decrease the energy.</li>
      <li>Set the <strong>margain</strong> to constrain the energy lifted by the discriminator.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/05/01/OpjHDH.png" alt="gan-3-15" style="margin:auto;display:block;width:50%;" /></p>

<p><strong>TODO</strong> paper read https://blog.csdn.net/a312863063/article/details/88125429</p>

<p><span class="ref"><a href="#reference-8">*reference-8</a></span></p>

<h3 id="loss-sensitive-gan">Loss-sensitive GAN</h3>

<p><strong>TODO</strong> paper read</p>

<h3 id="be-gan">BE GAN</h3>

<p><strong>TODO</strong> paper read https://www.cnblogs.com/king-lps/p/8552177.html</p>

<h2 id="improved-algorithm">Improved Algorithm</h2>

<ul>
  <li>Comparing to the original version of <a href="/2022/01/20/gan-1#conclusion">GAN algorithm</a>:
    <ul>
      <li>Replace the <code class="language-plaintext highlighter-rouge">log</code> divergence with Wasserstein Distance.</li>
      <li>Add <strong>Weight Clipping</strong> and <strong>Gradient Penalty</strong>.</li>
    </ul>
  </li>
  <li>In each training iteration:
    <ul>
      <li><strong>Learning D</strong>: find <strong>lower bound</strong> of $\max_D V(G,D)$ and repeat <code class="language-plaintext highlighter-rouge">k</code> times for fully trained.
        <ul>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> examples $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database distribution $P_{data}(x)$.</li>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
          <li>Obtaining generated data $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}, \tilde{x}^{i}=G\left(z^{i}\right), \tilde{x}^i = G(z^i)$.</li>
          <li>Update discriminator paramters $\theta_{d}$ to maximize.<br />
\(\begin{aligned}
&amp;\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \color{red}{D\left(x^{i}\right)-}\frac{1}{m} \sum_{i=1}^{m}  \color{red}{D\left(\tilde{x}^{i}\right) } \\
&amp;\theta_{d} \leftarrow \theta_{d}\color{red}{+}\eta \nabla \tilde{V}\left(\theta_{d}\right)
\end{aligned}\)</li>
        </ul>
      </li>
      <li><strong>Learning G</strong>: find the minimum of $\min_G V(G,D)$ <strong>only once</strong>.</li>
      <li>Sample <strong>another</strong> <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
      <li>Update generator parameters $\theta_{g}$ to maximize<br />
 \(\begin{aligned}
 &amp;\tilde{V}= \color{red}{-} \frac{1}{m} \sum_{i=1}^{m} \color{red}{D(G\left(z^{i}\right))} \\
 &amp;\theta_{g} \leftarrow \theta_{g}\color{red}{-}\eta \nabla \tilde{V}\left(\theta_{g}\right)
 \end{aligned}\)</li>
    </ul>
  </li>
</ul>

<p><strong>TODO</strong>: update $\tilde{V}$ with gradient penalty, check code.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php">Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee</a><a name="reference-1"></a></li>
  <li><a href="https://arxiv.org/abs/1606.00709">fGAN: General Framework of GAN</a><a name="reference-2"></a></li>
  <li><a href="https://vincentherrmann.github.io/blog/wasserstein/">Wasserstein GAN and the Kantorovich-Rubinstein Duality</a><a name="reference-3"></a></li>
  <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a><a name="reference-4"></a></li>
  <li><a href="https://arxiv.org/abs/1704.00028">Improved Training of Wasserstein GANs</a><a name="reference-5"></a></li>
  <li><a href="https://arxiv.org/abs/1802.05957">Spectral Normalization for Generative Adversarial Networks</a><a name="reference-6"></a></li>
  <li><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/87220341">Spectral Norm Explaination</a></li>
  <li><a href="https://arxiv.org/abs/1609.03126">Energy-based Generative Adversarial Network</a><a name="reference-8"></a></li>
</ol>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer vision" /><category term="generative model" /><category term="notes" /><summary type="html"><![CDATA[Advanced Thoery and Problems of GAN]]></summary></entry><entry><title type="html">Conditional Generative Model</title><link href="https://harrypotterrrr.github.io//2022/01/25/gan-2.html" rel="alternate" type="text/html" title="Conditional Generative Model" /><published>2022-01-25T00:00:00-05:00</published><updated>2022-01-25T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2022/01/25/gan-2</id><content type="html" xml:base="https://harrypotterrrr.github.io//2022/01/25/gan-2.html"><![CDATA[<p>Conditional Generative Model</p>

<!--more-->

<h2 id="supervised-conditional">Supervised Conditional</h2>

<ul>
  <li>Use paired data to train the network</li>
</ul>

<h3 id="conditional-gan">Conditional GAN</h3>

<ul>
  <li>
    <p>Reason to add noise: Without noise, the network only learn the map from the input (e.g. text ‘train’, ‘car’, etc.) to the average of multiple positive samples (e.g. the average of the front and side train), which is very bad.</p>
  </li>
  <li>Generator: take a normal (Gaussian) distribution and a constraint as inputs</li>
  <li>Discriminator: take generated output and a <strong>constraint</strong> as inputs, to check <strong>both</strong>:
    <ul>
      <li>if the result is realistic</li>
      <li>if the result <strong>matches</strong> the constraint</li>
    </ul>
  </li>
  <li>Algorithm:</li>
</ul>

<ol>
  <li>Sample m examples $\{ (c^1, x^{1}), (c^2, x^{2}), \ldots, (c^m, x^{m}) \}$ from database</li>
  <li>Sample m noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from a distribution</li>
  <li>Obtaining generated data $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}, \tilde{x}^{i}=G\left(z^{i}\right)$</li>
  <li>Sample m objects $\{\hat{x}^{1}, \hat{x}^{2}, \ldots, \hat{x}^{m}\}$ from database</li>
  <li>Update discriminator paramters $\theta_{d}$ to maximize<br />
 \(\begin{aligned}
  &amp;\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D\left(c^{i}, x^{i}\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(c^{i}, \tilde{x}^{i}\right)\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(c^{i}, \hat{x}^{i}\right)\right) \\
  &amp;\theta_{d} \leftarrow \theta_{d}+\eta \nabla \tilde{V}\left(\theta_{d}\right)
  \end{aligned}\)</li>
  <li>Sample m noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from a distribution</li>
  <li>Sample m conditions $\{c^{1}, c^{2}, \ldots, c^{m}\}$ from a database</li>
  <li>Update generator parameters $\theta_{g}$ to maximize<br />
 \(\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(c^{i}, z^{i}\right)\right)\right), \theta_{g} \leftarrow \theta_{g}-\eta \nabla \tilde{V}\left(\theta_{g}\right)\)</li>
</ol>

<h4 id="discriminator">Discriminator</h4>

<p>There are two commonly used prototype of structure of Discriminator:</p>
<ol>
  <li>Use two different network to receive object and condition. Green Network receive the object encoding and condition embedding and output final result which represent if generation is realistic and matched or not.
 <img src="https://s1.ax1x.com/2022/04/13/LuDAPI.png" alt="gan-2-1" style="margin:auto;display:block;" /></li>
  <li>Two network output different scores. Blue network receives object encoding and condition to output match result.
 <img src="https://s1.ax1x.com/2022/04/13/LuDEGt.png" alt="gan-2-2" style="margin:auto;display:block;" /></li>
</ol>

<ul>
  <li>The first structure may confuse between the bad generation and bad matching.</li>
</ul>

<h4 id="stack-gan">Stack GAN</h4>

<p>Progressively generate large resolution image by stacking generator.</p>
<ul>
  <li>low resolution generator take embedding and noise to output small image</li>
  <li>The small images from the previous generator will be taken as input, replacing the noise, to refine the detail and enlarge the resolution.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/13/LuDZxf.png" alt="gan-2-3" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference">*reference-3</a></span></p>

<h4 id="image-to-image">Image-to-image</h4>

<ol>
  <li><strong>Non-adversarial approach</strong>: (<strong>Traditional supervised approach using autoencoder</strong>) directly mapping from the input to image
 <img src="https://s1.ax1x.com/2022/04/13/LuDVRP.png" alt="gan-2-4" style="margin:auto;display:block;" />
 The result is blurry because as stated in <a href="#comparing-to-vae">Comparing to VAE</a>, it is the average of several samples.
 <img src="https://s1.ax1x.com/2022/04/13/LuDmM8.png" alt="gan-2-5" style="margin:auto;display:block;" /></li>
  <li><strong>Use GAN to generate</strong>: generated image and conditions are fed into discriminator to judge realisitc or not. At the same time, <em>close constraint</em> is to guide generator to output as closed to the paired image in the training sample as possible.
 <img src="https://s1.ax1x.com/2022/04/13/LuDnsS.png" alt="gan-2-6" style="margin:auto;display:block;" /></li>
</ol>

<ul>
  <li>As the above figure shows, without <em>close constraint</em> part, which is implemented in <em>L1 distance</em> to measure the difference between results and paired real images, the output will be more diversed but something <strong>unrelated</strong> to the original input will be generated.</li>
</ul>

<h3 id="patchgan">PatchGAN</h3>

<p><strong>TODO</strong></p>

<h3 id="speech-enhancement">Speech Enhancement</h3>

<ul>
  <li>More generally, the paired samples can be adopted to train GAN, e.g. noisy speech and clean voice pair.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/13/LuDuqg.png" alt="gan-2-7" style="margin:auto;display:block;" /></p>

<h3 id="video-generation">Video Generation</h3>

<ul>
  <li>Generator predict the future frame</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/13/LuDMZQ.png" alt="gan-2-8" style="margin:auto;display:block;" /></p>

<h2 id="unsupervised-conditional">Unsupervised Conditional</h2>

<ul>
  <li>Paired data for training is expensive or difficult to build.
    <ul>
      <li>Different drawing style with the same contents.</li>
      <li>Same sentence spoke by male and female.</li>
    </ul>
  </li>
  <li><strong>Unsupervised conditional generation</strong>: transform from <strong>one domain</strong> to <strong>another</strong> without paired data
    <ul>
      <li>Approach 1: direct transformation
        <ul>
          <li>change the texture or color, low level modification using shallow generator</li>
        </ul>
      </li>
      <li>Approach 2: Projection to a common space
        <ul>
          <li>only keep semantics</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="direction-transformation">Direction Transformation</h3>

<ul>
  <li>Transform the input image from domain $X$ directly to domain $Y$.</li>
  <li>Discriminator distinguishes if the generated image belongs to domain $Y$.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUm0H.png" alt="gan-2-9" style="margin:auto;display:block;" /></p>

<ul>
  <li>This approach may lead to the generated image closed to domain $Y$ but not similar to the original image, since discriminator still believes results belong to $Y$ in this case.
    <ul>
      <li>This can be avoided by generator design: <strong>simpler</strong> generator makes the input and output m<strong>ore closely related</strong>.</li>
    </ul>
  </li>
</ul>

<p><span class="ref"><a href="#reference-4">*reference-4</a></span></p>

<ul>
  <li>One workaround: use other module to guide the network output the image with the <strong>similar semantic representation</strong>
    <ul>
      <li>Backbone is feature extractor, such as VGG, which is pretrained</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUA1K.png" alt="gan-2-10" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-5">*reference-5</a></span></p>

<h4 id="cyclegan">CycleGAN</h4>

<ul>
  <li>CycleGAN is another solution, based on cycle consistency loss transforming the images between two domains.</li>
</ul>

\[\begin{aligned}
\mathcal{L}_{\mathrm{GAN}}\left(G_X, D_{Y}, X, Y\right) &amp;=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log D_{Y}(y)\right]+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log \left(1-D_{Y}(G_X(x))\right]\right. \\

\mathcal{L}_{\mathrm{GAN}}\left(G_Y, D_{X}, Y, X\right) &amp;=\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log D_{X}(x)\right]+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log \left(1-D_{X}(G_Y(y))\right]\right. \\

\mathcal{L}_{\text {cyc }}(G_X, G_Y) &amp;=\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|G_Y(G_X(x))-x\|_{1}\right]+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G_X(G_Y(y))-y\|_{1}\right] \\

\mathcal{L}\left(G_X, G_Y, D_{X}, D_{Y}\right) &amp;=\mathcal{L}_{\mathrm{GAN}}\left(G_X, D_{Y}, X, Y\right)+\mathcal{L}_{\mathrm{GAN}}\left(G_Y, D_{X}, Y, X\right) +\lambda \mathcal{L}_{\mathrm{cyc}}(G_X, G_Y) \\

G_X^{*}, G_Y^{*} &amp;= \arg \min _{G_X, G_Y} \max _{D_{X}, D_{Y}} \mathcal{L}\left(G_X, G_Y, D_{X}, D_{Y}\right)

\end{aligned}\]

<p><img src="https://s1.ax1x.com/2022/04/14/LQUE6O.png" alt="gan-2-11" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-6">*reference-6</a></span></p>

<ul>
  <li>The problem of CycleGAN: a master of steganography (信息隐藏), which means some information is hidden through the first generator in a way that is invisible to human eyes (e.g. black spot becomes inconspicuous) but restored after the second generator.
    <ul>
      <li>It is easier for the generator to get a high score from the discriminator by hiding some information.</li>
      <li>In this scene, cycle consistency is not promised because domain transformation is achieved by retaining some invisible details in the output image from the first generator.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUVXD.png" alt="gan-2-12" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-7">*reference-7</a></span></p>

<h4 id="dual-gan">Dual GAN</h4>

<p><strong>TODO</strong></p>

<h4 id="disco-gan">Disco GAN</h4>

<p><strong>TODO</strong></p>

<h4 id="stargan">StarGAN</h4>

<ul>
  <li>For previous work, multiple models should be built independently for each pair of domains.</li>
  <li>StarGAN extends the scalability of cycleGAN in handling more than two domains by using only one generator.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUene.png" alt="gan-2-13" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-9">*reference-9</a></span></p>

<h3 id="projection-to-a-common-space">Projection to a common space</h3>

<ul>
  <li>Instead of mapping the image of domain $X$ directly to domain $Y$, this firstly encode the image into the latent space which will be then projected to another domain by decoder.</li>
</ul>

<h4 id="common-problem-and-workarounds">Common problem and workarounds</h4>

<ul>
  <li>
    <p>Reconstruction error is built to guide each branch of network can reconstruct images in each domain after the encoding.</p>
  </li>
  <li>
    <p>Discriminator optimize the generation quality and solve obscure images produced by autoencoder (learn by averaging)</p>
  </li>
  <li>
    <p>After separately training, each row of the network has ability to reconstruct image in each domain.</p>
    <ul>
      <li>But images with <strong>the same attribute in different domain</strong> may not project to the same position in the latent space.</li>
      <li>The <strong>distribution</strong> of the latent representations from different domain are different.</li>
      <li>For example, black hair in the domain $A$ may be projected to <code class="language-plaintext highlighter-rouge">[0, 0, 1]</code> by $EN_X$, but black hair in the domain $B$ may be projected to <code class="language-plaintext highlighter-rouge">[1, 0, 0]</code> by $EN_Y$.</li>
      <li>We can not get output with desirable features by controlling the latent space vector in inference stage.</li>
      <li>It doesn’t make sense to train the latent space in this way!</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUKAA.png" alt="gan-2-14" style="margin:auto;display:block;" /></p>

<h4 id="couple-gan--unit">Couple GAN &amp;&amp; UNIT</h4>

<p><strong>TODO</strong> Couple GAN, UNIT</p>

<ul>
  <li>Couple GAN and UNIT share the parameters of last several layers of encoders and firsst several layers of decoders, which ensures that different encoders/decoders has exact same mapping function to the latent space.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUn7d.png" alt="gan-2-15" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-10">*reference-10, *reference-11</a></span></p>

<h4 id="fader-networks">Fader Networks</h4>

<ul>
  <li>Comparing to sharing the parameters of the network, <strong>Domain Discriminator</strong> is built to trained adversarially to disentange the latent representation directly.
    <ul>
      <li>$EN_X$ and $EN_Y$ are trained to fool the domain discriminator.</li>
      <li>Domain Discriminator distinguishes the latent code from $EN_X$ or $EN_Y$, forcing the output of $EN_X$ and $EN_Y$ have <strong>the same distribution</strong>.</li>
    </ul>
  </li>
  <li>Well-trained Fader Network: $EN_X$ and $EN_Y$ map the images from different domain to the same distribution as latent representations.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUQht.png" alt="gan-2-16" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-12">*reference-12</a></span></p>

<h4 id="combogan">ComboGAN</h4>

<ul>
  <li>In the same way that <a href="#cyclegan">CycleGAN</a> uses unsupervised learning and cycle consistency loss, ComboGAN adds latent vectors to control generation features</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQUMtI.png" alt="gan-2-17" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-13">*reference-13</a></span></p>

<p><strong>TODO</strong> ComboGAN</p>

<h4 id="xgan--dtn">XGAN &amp;&amp; DTN</h4>

<ul>
  <li>Comparing to ComboGAN do cycle-consistency loss in pixel wise, XGAN and DTN guide the encoders mapping to the same latent space by the <strong>semantic-consistency loss</strong> on the latent representation itself.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/14/LQU19P.png" alt="gan-2-18" style="margin:auto;display:block;" /></p>

<p><span class="ref"><a href="#reference-14">*reference-14</a></span></p>

<p><strong>TODO</strong> XGAN
<strong>TODO</strong> DTN: unsupervised</p>

<p><strong>TODO</strong> Bram matrix</p>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php">Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee</a><a name="reference-1"></a></li>
  <li><a href="https://arxiv.org/abs/1612.03242">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a><a name="reference-2"></a></li>
  <li><a href="https://arxiv.org/abs/1611.07004">Image-to-Image Translation with Conditional Adversarial Networks</a><a name="reference-3"></a></li>
  <li><a href="https://arxiv.org/abs/1703.10593">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a><a name="reference-4"></a></li>
  <li><a href="https://arxiv.org/abs/1611.02200">Unsupervised Cross-Domain Image Generation</a><a name="reference-5"></a></li>
  <li><a href="https://arxiv.org/abs/1703.10593">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a><a name="reference-6"></a></li>
  <li><a href="https://arxiv.org/abs/1703.05192">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</a><a name="reference-7"></a></li>
  <li><a href="https://arxiv.org/abs/1704.02510">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</a><a name="reference-8"></a></li>
  <li><a href="https://arxiv.org/abs/1711.09020">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a><a name="reference-9"></a></li>
  <li><a href="https://arxiv.org/abs/1606.07536">Coupled Generative Adversarial Networks</a><a name="reference-10"></a></li>
  <li><a href="https://arxiv.org/abs/1703.00848">Unsupervised Image-to-Image Translation Networks</a><a name="reference-11"></a></li>
  <li><a href="https://arxiv.org/abs/1706.00409">Fader Networks: Manipulating Images by Sliding Attributes</a><a name="reference-12"></a></li>
  <li><a href="https://arxiv.org/abs/1712.06909">ComboGAN: Unrestrained Scalability for Image Domain Translation</a><a name="reference-13"></a></li>
  <li><a href="https://arxiv.org/abs/1711.05139">XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings</a><a name="reference-14"></a></li>
</ol>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer vision" /><category term="generative model" /><category term="notes" /><summary type="html"><![CDATA[Conditional Generative Model]]></summary></entry><entry><title type="html">Basic Theory of Generative Model</title><link href="https://harrypotterrrr.github.io//2022/01/20/gan-1.html" rel="alternate" type="text/html" title="Basic Theory of Generative Model" /><published>2022-01-20T00:00:00-05:00</published><updated>2022-01-20T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2022/01/20/gan-1</id><content type="html" xml:base="https://harrypotterrrr.github.io//2022/01/20/gan-1.html"><![CDATA[<p>Deep Generative Model</p>

<!--more-->

<h2 id="basic-idea-of-gan">Basic Idea of GAN</h2>

<ul>
  <li>
    <p>Each dimension of input vector represents some characteristic, which means that change the input of the noise, some of the characteristic of the output image will change.</p>
  </li>
  <li>
    <p>The outout of discriminator is commonly a scalar. The larger value means realistic, while smaller means fake.</p>
  </li>
</ul>

<h3 id="algorithm">Algorithm</h3>

<p>Algorithm Initialize $\theta_{d}$ for $\mathrm{D}$, and $\theta_{g}$ for $\mathrm{G}$</p>
<ul>
  <li>In each training iteration:
    <ol>
      <li>Sample m examples $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database</li>
      <li>Sample m noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from a distribution</li>
      <li>Obtaining generated data $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}, \tilde{x}^{i}=G\left(z^{i}\right)$</li>
      <li>Update discriminator paramters $\theta_{d}$ to maximize<br />
 \(\begin{aligned}
&amp;\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D\left(x^{i}\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(\tilde{x}^{i}\right)\right) \\
&amp;\theta_{d} \leftarrow \theta_{d}+\eta \nabla \tilde{V}\left(\theta_{d}\right)
\end{aligned}\)</li>
      <li>Sample m noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from a distribution</li>
      <li>Update generator parameters $\theta_{g}$ to maximize<br />
 \(\begin{aligned}
&amp;\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(z^{i}\right)\right)\right) \\
&amp;\theta_{g} \leftarrow \theta_{g}-\eta \nabla \tilde{V}\left(\theta_{g}\right)
\end{aligned}\)</li>
    </ol>
  </li>
  <li>Takeaway:
    <ul>
      <li>Fix generator G, update discriminator D. Discriminator learns to assign high scores to real objects and low scores to generated result. Gradient <strong>descent</strong> of the final score.</li>
      <li>Fix discriminator D, and update generator G. Gnerator learns to fool the discriminator. Gradient <strong>ascent</strong> of the final score.</li>
    </ul>
  </li>
</ul>

<h3 id="generator">Generator</h3>

<ul>
  <li>Generator can be designed as <strong>autoencoder</strong>
    <ul>
      <li>Encoder part of the generator maps the original images to the latent space as the compact representation of the input object.</li>
      <li>Decoder part of the generator reconstructs the original object from code to generated result as closely as possible.</li>
      <li>After the training, we can <strong>discard</strong> the encoder part, replacing it with the input code as whatever we want.</li>
      <li>Change the input vector of the input noise, the generated result will <strong>vary smoothly</strong>, but <strong>may not make sense</strong> e.g. linear interpolation of the input noise</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/12/Le3aa4.png" alt="gan-1-1" style="margin:auto;display:block;" /></p>

<p><strong>TODO</strong> VAE</p>

<h3 id="comparing-to-vae">Comparing to VAE</h3>

<p><strong>TODO</strong> TODO result in what?</p>

<ul>
  <li>VAE: The relation between components are not-correlated since neurons in the same layer of the network will not influence each other and they are independent</li>
  <li>Thus VAE can not discriminate naunced between features, which cause the result more vague than GAN</li>
  <li>To catch the correpondence among pixels, deep structure needed.</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/12/LeoGNT.png" alt="gan-1-2" style="margin:auto;display:block;" /></p>

<h2 id="discriminator">Discriminator</h2>

<ul>
  <li>Use discriminator to generate?
    <ul>
      <li>Inference: Generate object $\widetilde{x}$ that<br />
 \(\widetilde{x}=\arg \max _{x \in X} D(x)\)</li>
      <li>Enumerate all possible $x$, which is not always feasible, because the above equations can only be solved when it is linear. However linearity constrain the ability of the network.</li>
    </ul>
  </li>
  <li>Algorithm:
    <ul>
      <li>Given a set of positive examples and negative example generated randomly</li>
      <li>In each iteration
        <ul>
          <li>Learn a discriminator D that can discriminate positive and negative examples</li>
          <li>Find relatively good examples from negative examples by $\widetilde{x}=\arg \max _{x \in X} D(x)$</li>
          <li>Replace the negative examples with relatively good examples</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/12/Leo8EV.png" alt="gan-1-3" style="margin:auto;display:block;" /></p>

<ul>
  <li>From the above figure, we noticed that discriminator can only optimize the distribution corresponding to the generated distribution, which is one of the reason why GAN is difficult to optimize.</li>
</ul>

<p><strong>TODO</strong> structured learning</p>

<h3 id="gan">GAN</h3>

<ul>
  <li>From Discriminator’s point of view
    <ul>
      <li>Use Generator to generate negative samples, replacing infeasible sovled <code class="language-plaintext highlighter-rouge">argmax</code> function</li>
    </ul>
  </li>
  <li>From Generator’s point of view
    <ul>
      <li>Still generate the object <strong>without correspondence</strong> (pixel-by-pixel)</li>
      <li>But it is led by the instruction of the discriminator with <strong>global view</strong></li>
    </ul>
  </li>
</ul>

<h2 id="basic-theory">Basic Theory</h2>

<h3 id="generation">Generation</h3>

<ul>
  <li>The realistic images from dataset can be regarded as the <strong>low-dimensional manifold</strong> in <strong>high-dimensional space</strong>. Sampling from the dataset is the process of sampling from the manifold in the space.</li>
</ul>

<p class="success"><strong>Info</strong>: Manifold, in short, generalizes the triangle, circle, squares, etc. to higher dimensions. If we take a closer look at the N-dimensional manifold (N-manifold), it looks like N-1 dimensional manifold.</p>

<ul>
  <li>
    <p>The ensemble of images from the sampling constitute the data distribution $P_{data}(x)$, which only occupies the tiny part of the whole high-dimensional space.</p>
  </li>
  <li>
    <p>The point outside of the manifold means there will be <strong>low probablity</strong> of the true sampling.</p>
  </li>
  <li>
    <p>Building a generator is the process of building a mapping from one known distribution to the data distribution in high level.</p>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/17/LNHt9f.png" alt="gan-1-4" style="margin:auto;display:block;width:80%;" /></p>

<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>

<ul>
  <li>Target
    <ul>
      <li>Given a data distribution $P_{data}(x)$</li>
      <li>We target at a distribution $P_{G}(x;\theta)$ parameterized by $\theta$
        <ul>
          <li>To find $\theta$ such that $P_{G}(x;\theta)$ closed to $P_{data}(x)$</li>
          <li>e.g. if we assume $P_{G}(x;\theta)$ is Guassian Mixture Model, $\theta$ are means and variances of the Gaussians</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Step
    <ul>
      <li>Derive $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ by sampling from $P_{data}(x)$</li>
      <li>We can compute corresponding $P_{G}(x^{i};\theta)$</li>
      <li>Then find $\theta^{\ast}$ maximizing the likelihood</li>
    </ul>

\[L=\prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right)\]
  </li>
  <li><strong>Maximum Likelihood Estimation</strong> = <strong>Minimize KL Divergence</strong>
    <ul>
      <li><strong>KL-divergence</strong> measures the difference between two distribution $P_{\text {data}}$ and $P_{\text {G}}$<br />
 \(\begin{aligned}
 KL\left(P_{\text {data}} \| P_{G}\right)
 &amp;= P_{\text {data}} \log P_{data}- P_{data} \log P_{\text {G}} \\[8px]
 &amp;= P_{\text {data}} \log \frac{P_{data}}{P_{\text {G}}}
 \end{aligned}\)</li>
      <li>$x \sim data$ means $x$ is sampled from the distribution of data.</li>
      <li>$P_{data}(x)$ means the probablity of the distribution <code class="language-plaintext highlighter-rouge">data</code> at $x$ sampling point.</li>
      <li>$P_{G}(x;\theta)$ means the probablity of the distribution <code class="language-plaintext highlighter-rouge">G</code> at $x$ sampling point with parameter $\theta$.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
\theta^{*} 
&amp;=\arg \max _{\theta} \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right) \\[12px]
&amp;=\arg \max _{\theta} \log \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right) \\[12px]
&amp;=\arg \max _{\theta} \sum_{i=1}^{m} \log P_{G}\left(x^{i} ; \theta\right) \quad \color{red}{\{ x^{1}, x^{2}, \ldots, x^{m} \} \text{ from } P_{data}(x) } \\[15px]
&amp; \approx \arg \max _{\theta} mE_{x \sim P_{\text {data}}}\left[\log P_{G}(x ; \theta)\right] \quad \color{red}{\text{argmax ignore } m \text{ multiplier}} \\[14px]
&amp; = \arg \max _{\theta} E_{x \sim P_{\text {data}}}\left[\log P_{G}(x ; \theta)\right] \\[12px]
&amp;=\arg \max _{\theta} \int_{x} P_{\text {data}}(x) \log P_{G}(x ; \theta) d x \\[12px]
&amp;=\arg \max _{\theta} \int_{x} P_{\text {data}}(x) \log P_{G}(x ; \theta) d x - \int_{x} P_{d a t a}(x) \log P_{\text {data}}(x) d x \\
&amp;\color{white}{= \arg \max _{\theta} \int_{x} P_{\text {data}}(x) \log P_{G}(x ; \theta) d x} \color{red}{\scriptsize {\text{add an item without } \theta \text{ and not influence argmax}}}\\[-15px]
&amp;=\arg \min_{\theta} K L\left(P_{\text {data}} \| P_{G}\right)
\end{aligned}\]

<p><strong>TODO</strong>: Expectation Maximum</p>

<ul>
  <li>What if $P_G$ is not Gaussian Mixture Model but more complex and general model?
    <ul>
      <li>The mean and variance of the more complex model is difficult to get, since the <strong>explicit</strong> expression of the corresponding <strong>probability density function</strong> is unknown.</li>
      <li>Thus parameters $\theta$ are <strong>implicit</strong> and impossible to found.</li>
    </ul>
  </li>
</ul>

<h3 id="drawback-of-gmm">Drawback of GMM</h3>

<ul>
  <li>In the past, Generator is built as Guassian Mixture model (GMM) with explicit parameters $\theta$.
    <ul>
      <li>As mentioned <a href="#generation">before</a>, GMM <strong>fails to represent the low-dimensional manifold</strong> which images lie along in high-dimensional space.</li>
      <li>Therefore, no matter what <code class="language-plaintext highlighter-rouge">mean</code> and <code class="language-plaintext highlighter-rouge">variance</code> of GMM looks like, it fails to resemble the target distribution.</li>
      <li>As a result, the output of GMM is obscure and fuzzy.</li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/17/LNHGNt.png" alt="gan-1-5" style="margin:auto;display:block;" /></p>

<h3 id="generator-1">Generator</h3>

<ul>
  <li>Generator <code class="language-plaintext highlighter-rouge">G</code> is a network, which defines a probability distirbution $P_G$, mapping the input distribution to the target.
    <ul>
      <li>The network has an ability to approximate any functions.</li>
      <li>The input distribution is not decisive, normal distribution, uniform distribution, etc.</li>
      <li>Even the simplest distribution of input $z$ can be mapped to the complex one.</li>
    </ul>
  </li>
</ul>

\[G^{*}=\arg \min _G \operatorname{Div}\left(P_{G}, P_{\text {data}}\right)\]

<ul>
  <li>The explicit formulations of $P_G$ and $P_{data}$ are still unknown, so it is impossible to do gradient descent on $G$ to minimize divergence.
    <ul>
      <li>But we can <strong>sample</strong> data from two distribution $P_G$ and $P_{data}$.
        <ul>
          <li>Sampling from $P_{data}$: randomly sample from the database.</li>
          <li>Sampling from $P_G$: randomly sample from normal, fed into the network and get outputs.</li>
        </ul>
      </li>
      <li>How to compute the divergence between samplings from two distributions? <strong>Discriminator</strong></li>
    </ul>
  </li>
</ul>

<p class="success"><strong>Info</strong>: <code class="language-plaintext highlighter-rouge">Div</code> is the divergence between two distribution, e.g. KL-divergence, JS-divergence.</p>

<p><img src="https://s1.ax1x.com/2022/04/17/LNHN38.png" alt="gan-1-6" style="margin:auto;display:block;" /></p>

<h3 id="discriminator-1">Discriminator</h3>

<ul>
  <li>
    <p>Recall the objective function for generator <code class="language-plaintext highlighter-rouge">G</code>:<br />
 \(G^{*}=\arg \min _G \operatorname{Div}\left(P_{G}, P_{\text {data}}\right)\)</p>
  </li>
  <li>The objective function for discriminator <code class="language-plaintext highlighter-rouge">D</code>:<br />
 \(D^{*}=\arg \max _D \operatorname{V}(D, G) \\
 V(G, D)=E_{x \sim P_{\text {data}}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))]\)</li>
  <li>This is exactly same as training a <strong>Binary Classifier</strong>
    <ul>
      <li>$P_{data}$ is binary class <code class="language-plaintext highlighter-rouge">1</code></li>
      <li>$P_G$ is binary class <code class="language-plaintext highlighter-rouge">0</code></li>
      <li><code class="language-plaintext highlighter-rouge">D</code> is a binary classifier</li>
      <li>Binary classifier is to <em>maximize</em> <strong>Cross Entropy</strong> rather than <strong>Mean Square Error</strong>, since MSE-based objective is non-convex optimization problem</li>
    </ul>
  </li>
  <li>The maximum objective is related to <strong>JS divergence</strong>.</li>
</ul>

<p><span class="ref"><a href="#reference-2">*reference-2</a></span></p>

<ul>
  <li>Given <code class="language-plaintext highlighter-rouge">G</code>, the optimal $D^*$ maximizing:<br />
 \(\begin{aligned}
 D^{*}&amp;=\arg \max _D \operatorname{V}(D, G) \\[10px]
 V(G, D) &amp;=E_{x \sim P_{\text {data}}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))] \\[10px]
 &amp;=\int_{x} P_{d a t a}(x) \log D(x) d x+\int_{x} P_{G}(x) \log (1-D(x)) d x \\[10px]
 &amp;=\int_{x}\left[P_{d a t a}(x) \log D(x)+P_{G}(x) \log (1-D(x))\right] d x
 \end{aligned}\)</li>
  <li>Assume that <code class="language-plaintext highlighter-rouge">D(x)</code> can represent <strong>any function</strong>. <a name="d-assumption"></a>
    <ul>
      <li>Consider the integral as the sum of each <strong>piecewise function</strong>(分段函数) for $x_1, x_2, …$, since we have assumed <code class="language-plaintext highlighter-rouge">D(x)</code> can represent any function, and neural network indeed can be regarded as piecewise function mathematically.</li>
      <li>When each piecewise function reaches the largest individually, the final integral must also be the largest.</li>
    </ul>
  </li>
  <li>Discretize the above equation, then the objective will be:<br />
 \(\begin{cases}
 P_{d a t a}(x) \log D(x)+P_{G}(x) \log (1-D(x)) \\[10px]
 a = P_{data},\ b = P_{G}
 \end{cases}\)
    <ul>
      <li>Solve for the extremum point:<br />
\(\begin{aligned}
\mathrm{f}(D) &amp;=\operatorname{alog}(D)+b \log (1-D) \\[8px]
\frac{d \mathrm{f}(D)}{d D} &amp;=a \times \frac{1}{D}+b \times \frac{1}{1-D} \times  (-1) =0 \\[8px]
a \times\left(1-D^{*}\right) &amp;=b \times D^{*} \\[5px]
0&lt; D^{*} &amp;=\frac{a}{a+b} =\frac{P_{\text {data}}(x)}{P_{\text {data}}(x)+P_{G}(x)}&lt;1  \\[8px]
0&lt; D^{*} &amp;&lt;1 
\end{aligned}\)</li>
    </ul>
  </li>
  <li>Substitue the optimal $D^* = \frac{P_{\text {data}}(x)}{P_{\text {data}}(x)+P_{G}(x)}$:</li>
</ul>

<p>\(\begin{aligned}
V &amp;=E_{x \sim P_{\text {data}}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))] \\[15px]
&amp;=E_{x \sim P_{\text {data}}}\left[\log \frac{P_{\text {data}}(x)}{P_{\text {data}}(x)+P_{G}(x)}\right]+E_{x \sim P_{G}}\left[\log \frac{P_{G}(x)}{P_{\text {data}}(x)+P_{G}(x)}\right] \\[15px]
&amp;=\int_{x} P_{\text {data}}(x) \log \frac{P_{\text {data}}(x)}{P_{\text {data}}(x)+P_{G}(x)} d x+\int_{x} P_{G}(x) \log \frac{P_{G}(x)}{P_{\text {data}}(x)+P_{G}(x)} d x \\[15px]
&amp;=\int_{x} P_{\text {data}}(x) \log \frac{\frac{1}{2}P_{\text {data}}(x)}{\frac{1}{2}[P_{\text {data}}(x)+P_{G}(x)]} d x+\int_{x} P_{G}(x) \log \frac{\frac{1}{2}P_{G}(x)}{\frac{1}{2}[P_{\text {data}}(x)+P_{G}(x)]} d x \\[15px]
&amp;=-2log2 + \int_{x} P_{\text {data}}(x) \log \frac{P_{\text {data}}(x)}{\frac{1}{2}[P_{\text {data}}(x)+P_{G}(x)]} d x+\int_{x} P_{G}(x) \log \frac{P_{G}(x)}{\frac{1}{2}[P_{\text {data}}(x)+P_{G}(x)]} d x \\[15px]
&amp;=-2log2 + K L\left(P_{\text {data}} \| \frac{P_{\text {data}}(x)+P_{G}(x)}{2}\right) + K L\left(P_{\text {G}} \| \frac{P_{\text {data}}(x)+P_{G}(x)}{2}\right) \\[15px]
&amp;=-2log2 + \color{red}{2JS\left(P_{\text {data}} \| P_{\text {G}}\right)}
\end{aligned}\)</p>
<ul>
  <li>JS divergence is symmetric, while KL divergence is asymmetric.
   \(\begin{aligned}
   JS\left(P_{\text {data}} \| P_{G}\right)
   &amp;= \frac{1}{2}[KL(P_{\text {data}} \| M) + KL(P_{\text {G}} \| M)] \\
   M &amp;= \frac{1}{2}(P_{data} + P_{G})
   \end{aligned}\)</li>
</ul>

<p><strong>TODO</strong>: inverse KL divergence</p>

<h3 id="gan-1">GAN</h3>

<ul>
  <li>The objective of Generator <code class="language-plaintext highlighter-rouge">G</code> is to minimize the divergence between $P_G$ and $P_{data}$</li>
  <li>The divergence is measured by Discriminator <code class="language-plaintext highlighter-rouge">D</code>, which maximizes the <strong>cross entropy</strong> as a <strong>binary classifie</strong>r.</li>
  <li>The maximum objective is related to JS divergence.</li>
</ul>

\[\begin{aligned}
 D^{*}&amp;=\operatorname{Div}\left(P_{G}, P_{\text {data}}\right)=\arg \max _D \operatorname{V}(D, G) \\
 G^{*}&amp;=\arg \min _G \operatorname{Div}\left(P_{G}, P_{\text {data}}\right) \\
&amp;=\arg \min _G \max _D \operatorname{V}(D, G) 
\end{aligned}\]

<ul>
  <li>For each fixed generator, we can find a discriminator with the <strong>maximum</strong> $V(D, G_i)$</li>
  <li>For each $V(D, G_i)$, we can get a <strong>minimum</strong> generator</li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/17/LNHJ4P.png" alt="gan-1-7" style="margin:auto;display:block;width:70%" /></p>

<ul>
  <li>In each iteration
    <ul>
      <li>Fix generator <code class="language-plaintext highlighter-rouge">G</code>, update discriminator $D$.</li>
      <li>Fix discriminator <code class="language-plaintext highlighter-rouge">D</code>, update generator $G$.</li>
    </ul>
  </li>
</ul>

<h4 id="algorithm-1">Algorithm</h4>

<ul>
  <li>To find the best <code class="language-plaintext highlighter-rouge">G</code> minimizing the loss function<br />
 \(G^{*}=\arg \min _G \max _D \operatorname{V}(D, G) = \arg\min_G L(G)\\
 \theta_{G} \leftarrow \theta_{G}-\eta \partial L(G) / \partial \theta_{G}\)
    <ul>
      <li>Let the maximum value $\max_D V(G,D)$ as $L(G)$</li>
      <li>$\theta_G$ defines <code class="language-plaintext highlighter-rouge">G</code></li>
      <li>The gradient of the max function is the gradient of max value of each piecewise function.</li>
    </ul>
  </li>
  <li>Given $G_0$</li>
  <li>Find $D^*_0$ maximizing $V(G_0, D)$ , using Gradient Ascent
    <ul>
      <li>$V(G_0, D_{0}^{*} )$ is the JS divergence between $P_data(x)$ and $P_{G_0}(x)$</li>
    </ul>
  </li>
  <li>Obtain $G_1$<br />
 \(\theta_{G} \leftarrow \theta_{G}-\eta \partial V\left(G, D_{0}^{*}\right) / \partial \theta_{G}\)</li>
  <li>Give $G_1$</li>
  <li>Find $D^*_0$ maximizing $V(G_0, D)$ , using Gradient Ascent
    <ul>
      <li>$V(G_1, D_1^*)$ is the JS divergence between $P_data(x)$ and $P_{G_0}(x)$</li>
    </ul>
  </li>
  <li>Obtain $G_2$
 \(\theta_{G} \leftarrow \theta_{G}-\eta \partial V\left(G, D_{1}^{*}\right) / \partial \theta_{G}\)</li>
</ul>

<p class="error"><strong>Critical</strong>: Actually, after we get the optimal $D_i^*$ by maximizing the JS-divergence, given the fixed $G_i$, we do gradient descent optimizing $G_i$ a little bit per step to finally get $G_{i+1}$. However, in the process of the gradient descent of $G_i$ to $G_{i+1}$, the divergence function $V(G_i, D_i)$ may change a lot, which cause the maximum objective value of <code class="language-plaintext highlighter-rouge">JS-divergence</code> shift to somewhere else.</p>

<p><img src="https://s1.ax1x.com/2022/04/17/LNH8AI.png" alt="gan-1-8" style="margin:auto;display:block;" /></p>

<ul>
  <li>Therefore, the important thing is that we don’t update $G$ too much for every steps, and $D$ is well-trained comparing to current $G$, so that the function of $V(G_i, D_i)$ won’t change a lot and $D_i^* \approx D_{i+1}^*$.</li>
</ul>

<h4 id="in-practical">In practical</h4>

<ul>
  <li>Actually, We can not get the expectation value of $P_{data}$ or $P_G$
    <ul>
      <li>In other words, we can not calculate the maximum value of the below equation:<br />
 \(V =E_{x \sim P_{\text {data }}}[\log D(x)]+E_{x \sim P_{G}}[\log (1-D(x))]\)</li>
    </ul>
  </li>
  <li>But we can do sampling from $P_{data}$ or $P_G$ to <strong>approximate</strong> the expectation.</li>
  <li>Given $G$, to compute $\max_D V(G,D)$
    <ul>
      <li>Sample ${\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}}$ from $P_{data}(x)$, sample ${x^{1}, x^{2}, \ldots, x^{m}}$ from generator $P_G(x)$</li>
      <li>To maximize:<br />
 \(\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D(x^{i})+\frac{1}{m} \sum_{i=1}^{m} \log (1-D(\tilde{x}^{i}))\)</li>
    </ul>
  </li>
  <li>Thus, $D$ is a binary classifier with sigmoid output, which can be designed to be very deep.
    <ul>
      <li>${\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}}$ from $P_{data}(x)$ is <code class="language-plaintext highlighter-rouge">Positive examples</code>.</li>
      <li>${x^{1}, x^{2}, \ldots, x^{m}}$ from generator $P_G(x)$ is <code class="language-plaintext highlighter-rouge">Negative examples</code>.</li>
      <li>This is equivalent to <strong>minimize Cross-entropy</strong>.</li>
    </ul>
  </li>
</ul>

<h2 id="total-algorithm">Total Algorithm</h2>

<h3 id="conclusion">Conclusion</h3>

<ul>
  <li>In each training iteration:
    <ul>
      <li><strong>Learning D</strong>: find <strong>lower bound</strong> of $\max_D V(G,D)$ and repeat <code class="language-plaintext highlighter-rouge">k</code> times for fully trained.
        <ul>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> examples $\{ x^{1}, x^{2}, \ldots, x^{m} \}$ from database distribution $P_{data}(x)$.</li>
          <li>Sample <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
          <li>Obtaining generated data $\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\}, \tilde{x}^{i}=G\left(z^{i}\right), \tilde{x}^i = G(z^i)$.</li>
          <li>Update discriminator paramters $\theta_{d}$ to maximize.<br />
\(\begin{aligned}
&amp;\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D\left(x^{i}\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(\tilde{x}^{i}\right)\right) \\
&amp;\theta_{d} \leftarrow \theta_{d}\color{red}{+}\eta \nabla \tilde{V}\left(\theta_{d}\right)
\end{aligned}\)</li>
        </ul>
      </li>
      <li><strong>Learning G</strong>: find the minimum of $\min_G V(G,D)$ <strong>only once</strong>.</li>
      <li>Sample <strong>another</strong> <code class="language-plaintext highlighter-rouge">m</code> noise samples $\{z^{1}, z^{2}, \ldots, z^{m}\}$ from the prior $P_{prior}(z)$.</li>
      <li>Update generator parameters $\theta_{g}$ to maximize<br />
 \(\begin{aligned}
 &amp;\tilde{V}=\underset{\text{irrelevant to } G}{\boxed{\frac{1}{m} \sum_{i=1}^{m}  \log D\left(x^{i}\right) } } +\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(z^{i}\right)\right) \right) \\
 &amp;\theta_{g} \leftarrow \theta_{g}\color{red}{-}\eta \nabla \tilde{V}\left(\theta_{g}\right)
 \end{aligned}\)</li>
    </ul>
  </li>
</ul>

<p class="warning"><strong>Note that</strong> it is impossible for $D$ to find the maximum value of $\max_D V(G,D)$, which means that the JS-divergence can never be reached, because:<br /> <strong>1</strong>. The learning ability of $D$ is limited due to the iteration times and learning rate, the $D$ can not convergence in several update steps.<br /> <strong>2</strong>. Even with the infinite training step and the convergence achieved, it is more likely for $D$ to fall in local minima.<br /> <strong>3</strong>. Even without trapped in local minima, the capcity of the generatlization of $D$ is limited, which contradict the <a href="#d-assumption">assumption</a> that $D$ can represent all functions.</p>

<h3 id="in-real-implementation">In Real Implementation</h3>

<ul>
  <li>The training of $G$ would be slow because of the initial output of $D(x)$ is closed to $0$ and the corresponding gradient to $G$ is very small.
 \(V =\underset{\text{irrelevant to } G}{\boxed{E_{x \sim P_{\text {data } }}[\log D(x)]}}+E_{x \sim P_{G}}[\log (1-D(x))]\)</li>
  <li>Two workarounds
    <ul>
      <li>Replace the target with the following, greatly increase the gradient of the very beginning stage, <em>Minimax GAN (MMGAN)</em> 
\(V = E_{x \sim P_{G}}[- \log (D(x))]\)</li>
      <li>Replace the negative label of $G$ with the positive label, <em>Non-saturating GAN (NSGAN)</em></li>
    </ul>
  </li>
</ul>

<p><img src="https://s1.ax1x.com/2022/04/17/LNHBHs.png" alt="gan-1-9" style="margin:auto;display:block;width:30%;" /></p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php">Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee</a></li>
  <li><a href="https://blog.csdn.net/taoqick/article/details/102621605">分类用交叉熵而不用MSE</a><a name="reference-2"></a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer vision" /><category term="generative model" /><category term="notes" /><summary type="html"><![CDATA[Deep Generative Model]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅹ</title><link href="https://harrypotterrrr.github.io//2020/12/20/cg-10.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅹ" /><published>2020-12-20T00:00:00-05:00</published><updated>2020-12-20T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2020/12/20/cg-10</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/12/20/cg-10.html"><![CDATA[<p>Material, Microfacet Theory, Isotropic Material, Anisotropic Material</p>

<!--more-->

<h2 id="material">Material</h2>

<ul>
  <li>Recall that BRDF is used to calculate how much of the light ray will be reflected in a particular outgoing direction by the surface, which specify reflection ability and properties of the surface
    <ul>
      <li>Material == BRDF</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/11/y0TtQP.jpg" alt="cg-10-1" /></p>

<h3 id="diffuse--lambertian-material">Diffuse / Lambertian Material</h3>

<ul>
  <li>Assume a senario with a reflected point
    <ul>
      <li>Light coming in distributed uniformly in the hemisphere</li>
      <li>The point absorbs nothing energy and reflects all the energy out</li>
      <li>Light is equally reflected in each output direction</li>
    </ul>
  </li>
</ul>

\[\begin{aligned} 
L_{o}\left(\omega_{o}\right) &amp;=\int_{H^{2}} f_{r} L_{i}\left(\omega_{i}\right) \cos \theta_{i} d \omega_{i} \\ 
&amp;=f_{r} L_{i} \int_{H^{2}} \cos \theta_{i} d \omega_{i} \\ 
&amp;=f_{r} L_{i} \int_{H^{2}} \cos \theta \sin \theta d \theta d \varphi \\ 
&amp;=f_{r} L_{i} \int_{0}^{\frac{\pi}{2}} \cos \theta \sin \theta d \theta \int_{0}^{2 \pi} d \theta \\ 
&amp;=f_{r} L_{i} \cdot \frac{1}{4} \cos 2 \theta \bigg|_{\frac{\pi}{2}} ^{0} \cdot \varphi \bigg|_{0} ^{2 \pi} \\ 
&amp;=\pi f_{r} L_{i} 
\end{aligned}\]

<p><img src="https://z3.ax1x.com/2021/02/11/y0T8JA.jpg" alt="cg-10-2" style="width:40%;margin:auto;display:block;" /></p>

<ul>
  <li>On account of energy conservation, the energy absorbed and the enrgy emitted from the point is the same
    <ul>
      <li>incident radiance equals to exitant radiance</li>
    </ul>

\[\left\{\begin{array}{l} L_o = L_i \\ L_{o}\left(\omega_{o}\right) = \pi f_{r} L_{i}\end{array}\right.\]

\[f_r = \frac{1}{\pi}\]
  </li>
  <li>Denote reflection ratio $\rho$ (<strong>albedo</strong>) $0 \lt \rho \lt 1$
    <ul>
      <li>Three dimensional vector to specify different ability of reflection regrading three colors</li>
    </ul>
  </li>
</ul>

\[f_r = \frac{\rho}{\pi} \in (0, \frac{1}{\pi})\]

<h3 id="material-category">Material Category</h3>

<h4 id="diffuse-material">Diffuse Material</h4>

<p><img src="https://z3.ax1x.com/2021/02/11/y0T3id.jpg" alt="cg-10-3" /></p>

<h4 id="glossy-material">Glossy Material</h4>

<p><img src="https://z3.ax1x.com/2021/02/11/y0TJzt.jpg" alt="cg-10-4" /></p>

<h4 id="ideal-reflective--refractive-material">Ideal Reflective / Refractive Material</h4>

<p><img src="https://z3.ax1x.com/2021/02/11/y0TGRI.jpg" alt="cg-10-5" /></p>

<h3 id="specular-reflection">Specular Reflection</h3>
<ul>
  <li>Calculate <a href="/2020/12/03/cg-9#Angles-and-Solid-Angles">zenith angle and azimuth angle</a></li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/11/y0TNsf.jpg" alt="cg-10-6" style="width:70%;margin:auto;display:block;" /></p>

\[\omega_{o}+\omega_{i}=2 \cos \theta \overrightarrow{\mathrm{n}}=2\left(\omega_{i} \cdot \overrightarrow{\mathrm{n}}\right) \overrightarrow{\mathrm{n}}\]

\[\omega_{o}=-\omega_{i}+2\left(\omega_{i} \cdot \overrightarrow{\mathrm{n}}\right) \overrightarrow{\mathrm{n}}\]

<p class="info"><strong>Tip</strong>: The difference between Blinn-Phong model and Phong model is the half vector, which is computed by adding incident vector and the vector of viewing direction. With dot product, Computing reflected is less efficient than half vector.</p>

<p class="success"><strong>Info</strong>: BRDF distributes as <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac δ function(distribution)</a>. The total area under BRDF curve is the <strong>albedo</strong> specifying what fraction of incident light is reflected in total, which is as opposed to being abosorbed or transmitted. For the perfect reflector, the area under the curbe sums to 1, because it reflects all of the incident light but in one direction. In this case, the δ function, with the area of 1, is an infinite thin and infinite tall spike at $x = 0$, which is ideal reflective material.</p>

<p class="warning"><strong>Note</strong>: Obviously, a real material can not be perfect specular reflector. First, the area under the curve is less than 1 due to some of the light absorbed. More importantly, the reflection peak can not be infinite thin so that the reflection will be blurred ever so slightly. This implicates that the peak will not be infinitely high. The wider the peak, the less hight it has to be to maintain the area of 1.</p>

<h3 id="specular-refraction">Specular Refraction</h3>

<ul>
  <li>In addition to reflecting off surface, light may be transmitted through surface.</li>
  <li>Light refracts when it enters a new medium.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/11/y0TUL8.jpg" alt="cg-10-7" /></p>

<ul>
  <li>Though the scattering of the light is due to the discrepancy of the wave length of light, we still consider it in geometric way as different refractive index in CG.</li>
  <li>The phenomenon of <a href="https://en.wikipedia.org/wiki/Caustic_(optics)">caustics</a> as the right image shows is caused by light rays refracting off the uneven and curved surface of the sea and converging on the seafloor. It is tricky to deal with such thing in practical CG implementation.</li>
</ul>

<h4 id="snells-law">Snell’s Law</h4>

<ul>
  <li><strong>Snell’s Law</strong>: Transmitted angle depends on
    <ul>
      <li>index of refraction (IOR) for incident ray</li>
      <li>index of refraction (IOR) for exiting ray</li>
    </ul>
  </li>
  <li>calculate azimuth angle in the same as reflection</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/yrab7T.jpg" alt="cg-10-8" style="width:60%;margin:auto;display:block;" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Medium</th>
      <th style="text-align: center">$\eta ^*$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Vaccum</td>
      <td style="text-align: center">1.0</td>
    </tr>
    <tr>
      <td style="text-align: center">Air (sea level)</td>
      <td style="text-align: center">1.00029</td>
    </tr>
    <tr>
      <td style="text-align: center">Water(20°C)</td>
      <td style="text-align: center">1.333</td>
    </tr>
    <tr>
      <td style="text-align: center">Glass</td>
      <td style="text-align: center">1.5-1.6</td>
    </tr>
    <tr>
      <td style="text-align: center">Diamond</td>
      <td style="text-align: center">2.42</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>index of refraction is wavelength dependent</p>
  </li>
  <li>
    <p>Total internal reflection</p>
  </li>
</ul>

\[\begin{aligned} \eta_{i} \sin \theta_{i} &amp;=\eta_{t} \sin \theta_{t} \\ \cos \theta_{t} &amp;=\sqrt{1-\sin ^{2} \theta_{t}} \\ &amp;=\sqrt{1-\left(\frac{\eta_{i}}{\eta_{t}}\right)^{2} \sin ^{2} \theta_{i}} \\ &amp;=\sqrt{1-\left(\frac{\eta_{i}}{\eta_{t}}\right)^{2}\left(1-\cos ^{2} \theta_{i}\right)}\end{aligned}\]

\[1-\left(\frac{\eta_{i}}{\eta_{t}}\right)^{2}\left(1-\cos ^{2} \theta_{i}\right)&lt;0\]

<ul>
  <li>When light is moving from a more optically dense medium (e.g. water) to a less optically dense medium (e.g. air), which means $\eta_i / \eta_t \gt 1$</li>
  <li>
    <p>Light incident on boundary from large enough angle <strong>may</strong> not exit medium</p>
  </li>
  <li><strong>Snell’s Window / Circle</strong>: underwater viewer sees everthing above the surface through a cone of light.
    <ul>
      <li>The area outside Snell’s window will be dark or show a reflection of underwater objects by total internal reflection</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/yraONF.jpg" alt="cg-10-9" /></p>

<p class="info"><strong>Tip</strong>: The sphere is symmetric object, which means that the light ray refracted into the sphere will <strong>always</strong> be refracted out.</p>

<p class="success"><strong>info</strong>: <strong>BTDF</strong> (bidirectional transmittance distribution function) is similar to <strong>BRDF</strong> (bidirectional reflectance distribution function) but for the opposite side of the surface, especially used to evaluate how outgoing refractive light distributes at incident point. Further, <strong>BSDF</strong> (bidirectional scattering distribution function) is the generalization of <strong>BRDF</strong> and <strong>BTDF</strong>. Moreover, <strong>BSSRDF</strong> (bidirectional scattering-surface reflectance distribution function) describes the relationship between outgoing radiance and the incident flux, including the phenonmona like <a href="https://en.wikipedia.org/wiki/Subsurface_scattering">subsurface scattering</a>.</p>

<h4 id="fresnel-reflection-term">Fresnel Reflection Term</h4>

<ul>
  <li>
    <p><strong>Fresnel Reflection</strong>: predict the reflectance of smooth surfaces, and depends solely on the refractive index and the angle of incidence.</p>
  </li>
  <li>
    <p>The inverted reflection in water gradually evanish with the increasing incident angle between the normal of horizontal plane and incident light.</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/yraXh4.jpg" alt="cg-10-10" /></p>

<ul>
  <li>Fresnel Term of Dielectric material ($\eta = 1.5$, <strong>i.e.</strong> most glasses and plastics)
    <ul>
      <li>Reflectance increases greatly as the grazing angle increases</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/yraHBV.jpg" alt="cg-10-12" /></p>

<ul>
  <li>Fresnel Term of Conductor (<strong>i.e.</strong> metals)
    <ul>
      <li>The effect of Fresnel reflectance is subtle</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/yraLAU.jpg" alt="cg-10-11" /></p>

<p class="info"><strong>Tip</strong>: Index of refraction of the conductor is actually the complex number, which contains two number $n,\ k$.</p>

<ul>
  <li>This is the reason why people use polished metal (mercury) as mirror instead of polished glass.</li>
</ul>

<p class="success"><strong>Info</strong>: Polarization is defined relative to the plane of incidence, i.e. the plane that contains the incoming and reflected rays as well as the normal to the sample surface. Perpendicular (<strong>s-</strong>)polarization is the polarization where the electric field is perpendicular to the plane of incidence. Parallel (<strong>p-</strong>)polarization is the polarization where the electric field is parallel to the plane of incidence.</p>

<h4 id="fresnel-equation">Fresnel Equation</h4>

<ul>
  <li>Consider the light as a wave</li>
  <li>Accurate and need to consider polarization</li>
</ul>

\[R_{\mathrm{s}}=\left|\frac{n_{1} \cos \theta_{\mathrm{i}}-n_{2} \cos \theta_{\mathrm{t}}}{n_{1} \cos \theta_{\mathrm{i}}+n_{2} \cos \theta_{\mathrm{t}}}\right|^{2}=\left|\frac{n_{1} \cos \theta_{\mathrm{i}}-n_{2} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}}{n_{1} \cos \theta_{\mathrm{i}}+n_{2} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}}\right|^{2}\]

\[R_{\mathrm{p}}=\left|\frac{n_{1} \cos \theta_{\mathrm{t}}-n_{2} \cos \theta_{\mathrm{i}}}{n_{1} \cos \theta_{\mathrm{t}}+n_{2} \cos \theta_{\mathrm{i}}}\right|^{2}=\left|\frac{n_{1} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}-n_{2} \cos \theta_{\mathrm{i}}}{n_{1} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}+n_{2} \cos \theta_{\mathrm{i}}}\right|\]

<ul>
  <li>Effective reflectivity (<strong>unpolarized</strong>) is just the average of two reflectivities</li>
</ul>

\[R_{\mathrm{eff}} =\frac{1}{2}\left(R_{\mathrm{s}}+R_{\mathrm{p}}\right)\]

<h4 id="schlicks-approximation">Schlick’s approximation</h4>

<ul>
  <li>$\theta$ : angle between the incident light and the normal</li>
  <li>$n_1,\ n_2$ : indices of refraction of two media at the intrerface</li>
  <li>$R_0$ : reflection coefficient for light incoming parallel to the normal (the value of Fresnel term when $\theta = 0$)</li>
</ul>

\[R(\theta) =R_{0}+\left(1-R_{0}\right)(1-\cos \theta)^{5}\]

\[R_{0} =\left(\frac{n_{1}-n_{2}}{n_{1}+n_{2}}\right)^{2}\]

<h2 id="microfacet-material">Microfacet Material</h2>

<h3 id="microfacet-theory">Microfacet Theory</h3>

<ul>
  <li><strong>Microfacet theory</strong>: Rough surfaces can be modeled as a collection of small microfacets
    <ul>
      <li>Macroscale: flat &amp; rough material, describe underlying smooth surface</li>
      <li>Microscale: bumpy &amp; specular property, describe high variation of microfacet surfaces</li>
    </ul>
  </li>
  <li>Individual elements of surface at like <strong>mirror</strong>
    <ul>
      <li>Known as Microfacets</li>
      <li>Each microfacet has its own normal</li>
    </ul>
  </li>
  <li>Intuitively, surface is regarded as <strong>material</strong> see from a distance, <strong>geometry</strong> see from closeup
    <ul>
      <li>As the camera zooms out away from the surface, geometry will gradually fades into material.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysEcs1.jpg" alt="cg-10-13" /></p>

<h3 id="microfacet-brdf">Microfacet BRDF</h3>

<ul>
  <li>
    <p>Microfacet BRDF: <strong>distribution</strong> of microfacets’ normals</p>

    <ul>
      <li>Concentrated &lt;==&gt; smooth &lt;==&gt; glossy</li>
    </ul>

    <p><img src="https://z3.ax1x.com/2021/02/13/ysAq5F.jpg" alt="cg-10-14" style="width:80%;" /></p>

    <ul>
      <li>Spread &lt;==&gt; rough  &lt;==&gt; diffuse</li>
    </ul>

    <p><img src="https://z3.ax1x.com/2021/02/13/ysAbUU.jpg" alt="cg-10-15" style="width:80%;" /></p>
  </li>
</ul>

\[f_{microfacet} (i, o) = \frac{F(i, h)G(i,o,h)D(h)}{4(n,i)(n,o)}\]

<ul>
  <li>$f_{microfacet}$ : microfacet BRDF</li>
  <li>$F(i, h)$ : Fresnel reflectance term, the amount of energy reflected in terms of incident angle</li>
  <li>$G(i,o,h)$ : geometry term of shadowing masking between microfacet</li>
  <li>$D(h)$ : normal distribution term describing how microfacet normals is distributed around the given direction $h$</li>
  <li>$i$ : incident light direction</li>
  <li>$o$ : view direction</li>
  <li>$n$ : surface normal</li>
  <li>$h$ : half vector between $i$ and $o$</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysAoD0.jpg" alt="cg-10-16" style="width:80%;" /></p>

<ul>
  <li>More details discussed in paper <a href="https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html">Microfacet Models for Refraction through Rough Surfaces</a></li>
</ul>

<p class="success"><strong>Info</strong>: Three Important Geometric Effects to Consider with Microfacet Reflection Models. <strong>(a)</strong> Masking: the microfacet of interest isn’t visible to the viewer due to occlusion by another microfacet. <strong>(b)</strong> Shadowing: analogously, light doesn’t reach the microfacet. <strong>(c)</strong> Interreflection: light bounces among the microfacets before reaching the viewer. Shadow-masking effect is obvious when the angle between incident light or viewing direction and normal is aounrd 90°, which we called <strong>grazing angle</strong> (掠射角).<br />
<img src="https://z3.ax1x.com/2021/02/13/ysEPUO.jpg" alt="cg-10-17" /></p>

<ul>
  <li>Microfacet model is widely used in <strong>PBR</strong> (Physically Based Rendering)</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysAHET.jpg" alt="cg-10-18" /></p>

<h3 id="isotropic--anisotropic-material">Isotropic / Anisotropic Material</h3>

<ul>
  <li>One way to classifying material: directionality of underlying surface</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysATbV.jpg" alt="cg-10-19" style="width:70%;" /></p>

<ul>
  <li><strong>Isotropic Material</strong>: have identical properties in all direction.
    <ul>
      <li>Rotate around normal without changing reflections.</li>
      <li>BRDF only depends on <strong>relative</strong> location (azimuth angle) of the incident and exitent light ray</li>
      <li>e.g. Most materials</li>
    </ul>
  </li>
</ul>

\[f_{r}\left(\theta_{i}, \phi_{i} ; \theta_{r}, \phi_{r}\right) = f_{r}\left(\theta_{i}, \phi_{i} - \phi ; \theta_{r}, \phi_{r} - \phi \right) = f_{r}\left(\theta_{i}, \theta_{r}, \phi_{r} - \phi_{i} \right)\]

<p><img src="https://z3.ax1x.com/2021/02/13/ysAOC4.jpg" alt="cg-10-20" style="width:60%;margin:auto;display:block;" /></p>

<ul>
  <li><strong>Anisotropic Material</strong>: change properties with direction.
    <ul>
      <li>Rotate on zaimuth angle $\Phi$ and change BRDF</li>
      <li>e.g. Brushed metal</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysAX8J.jpg" alt="cg-10-21" /></p>

<ul>
  <li>Some other anisotropic material
    <ul>
      <li>Nylon: istropic along colum and row but not in the diagonal direction</li>
      <li>Velvet: self-defined microfacet material</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysENq0.gif" alt="cg-10-22" /></p>

<h2 id="properties-of-brdf">Properties of BRDF</h2>

<ul>
  <li>Non-negativity: BRDF represents the distribution of energy</li>
</ul>

\[f_{r}\left(\omega_{i} \rightarrow \omega_{r}\right) \geq 0\]

<ul>
  <li>Linearity: In Blinn-Phong model, shading is the sum of ambient, diffuse, specular terms, while BRDF takes three parts as a whole.
    <ul>
      <li>More generally, in many cases, material often require multiple ray tracing with different BRDF to achieve reflective properties.</li>
      <li>The total radiance of a point on a surface can be simply expressed as the sum of the radiance of multiple BRDFS.</li>
    </ul>
  </li>
</ul>

\[L_{r}\left(\mathrm{p}, \omega_{r}\right)= \sum_{BRDF} \int_{H^{2}} f_{r}\left(\mathrm{p}, \omega_{i} \rightarrow \omega_{r}\right) L_{i}\left(\mathrm{p}, \omega_{i}\right) \cos \theta_{i} \mathrm{~d} \omega_{i}\]

<p><img src="https://z3.ax1x.com/2021/02/13/ysAj29.jpg" alt="cg-10-23" style="width:70%;margin:auto;display:block;" /></p>

<ul>
  <li>Reciprocity principle (可逆性): similar to the reciprocity of light ray</li>
</ul>

\[f_{r}\left(\omega_{r} \rightarrow \omega_{i}\right)=f_{r}\left(\omega_{i} \rightarrow \omega_{r}\right)\]

<ul>
  <li>Energy conservation: otherwise, the energy will not converge but explode in path tracing.</li>
</ul>

\[\forall \omega_{r} \int_{H^{2}} f_{r}\left(\omega_{i} \rightarrow \omega_{r}\right) \cos \theta_{i} \mathrm{~d} \omega_{i} \leq 1\]

<ul>
  <li>Isotropic or anisotropic
    <ul>
      <li>If isotropic:</li>
    </ul>

\[f_{r}\left(\theta_{i}, \phi_{i} ; \theta_{r}, \phi_{r}\right)=f_{r}\left(\theta_{i}, \theta_{r}, \phi_{r}-\phi_{i}\right)\]

    <ul>
      <li>Then, from reciprocity:</li>
    </ul>

\[f_{r}\left(\theta_{i}, \theta_{r}, \phi_{r}-\phi_{i}\right)=f_{r}\left(\theta_{r}, \theta_{i}, \phi_{i}-\phi_{r}\right)=f_{r}\left(\theta_{i}, \theta_{r},\left|\phi_{r}-\phi_{i}\right|\right)\]
  </li>
</ul>

<h2 id="measuring-brdf">Measuring BRDF</h2>

<h3 id="motivation-of-measuring-brdf">Motivation of Measuring BRDF</h3>
<ul>
  <li>Avoid need to develop / derive models:the error of the theory model is sometimes very large</li>
  <li>Accurately render with real-world materials: automatically includes all of the scattering effects present and useful for product design, special effects, …</li>
</ul>

<h3 id="general-approach">General Approach</h3>

<ul>
  <li>Image-based BRDF measurement: goniorreflectometer</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysESDx.jpg" alt="cg-10-24" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">outgoing</span> <span class="n">direction</span> <span class="n">w_o</span><span class="p">:</span>
    <span class="n">move</span> <span class="n">light</span> <span class="n">to</span> <span class="n">illuminate</span> <span class="n">surface</span> <span class="k">with</span> <span class="n">a</span> <span class="n">thin</span> <span class="n">beam</span> <span class="k">from</span> <span class="n">w_o</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">incoming</span> <span class="n">direction</span> <span class="n">w_i</span><span class="p">:</span>
        <span class="n">move</span> <span class="n">sensor</span> <span class="n">to</span> <span class="n">be</span> <span class="n">at</span> <span class="n">direction</span> <span class="n">w_i</span> <span class="k">from</span> <span class="n">surface</span>
        <span class="n">measure</span> <span class="n">incident</span> <span class="n">radiance</span>
</code></pre></div></div>

<ul>
  <li>Improving efficiency
    <ul>
      <li>Isotropic surfaces reduce dimensionality from 4D to 3D
        <ul>
          <li><strong>4D to 3D</strong>: $f_{r}\left(\theta_{i}, \phi_{i} ; \theta_{r}, \phi_{r}\right) \implies f_{r}\left(\theta_{i}, \theta_{r}, \phi_{r}-\phi_{i}\right)$</li>
          <li><strong>Curse of dimensionality</strong>: with an increase dimension, the size of data surges</li>
        </ul>
      </li>
      <li>Reciprocity reduces number of measurements by half</li>
      <li>Clever optical systems …</li>
    </ul>
  </li>
</ul>

<h3 id="challenges-in-measuring-brdf">Challenges in Measuring BRDF</h3>

<ul>
  <li>Accurate measurements at grazing angles
    <ul>
      <li>Important due to Fresnel effects</li>
    </ul>
  </li>
  <li>Measuring with dense enough sampling to capture high frequency specularities</li>
  <li>Retro-reflection</li>
  <li>Spatially-varying reflectance, …</li>
</ul>

<h3 id="representing-measured-brdf">Representing Measured BRDF</h3>

<ul>
  <li>Desirable qualities
    <ul>
      <li>Compact representation</li>
      <li>Accurate representation of measured data</li>
      <li>Efficient evaluation for arbitrary pairs of diretions</li>
      <li>Good distributions available for importance sampling</li>
    </ul>
  </li>
  <li>Tabular Representation
    <ul>
      <li>Store regularly-spaced samples in $ ( \theta_i, \theta_o, \big| \phi_i - \phi_o \big| ) $
        <ul>
          <li>Better: reparameterize angles to better match specularities</li>
        </ul>
      </li>
      <li>Generally need to resample measured values to table</li>
      <li>Very high storage requirements</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/13/ysAzK1.jpg" alt="cg-10-25" style="width:60%;" /></p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://computergraphics.stackexchange.com/questions/4767">BRDF of specular reflection is infinite</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Fresnel_equations">Fresnel equations</a></li>
  <li><a href="https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html">Microfacet Models for Refraction through Rough Surfaces</a></li>
  <li><a href="http://www.pbr-book.org/3ed-2018/Reflection_Models/Microfacet_Models.html">Microfacet Models</a></li>
  <li><a href="http://simonstechblog.blogspot.com/2011/12/microfacet-brdf.html">Microfacet BRDF</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/62904454">深入BRDF和PBR</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Material, Microfacet Theory, Isotropic Material, Anisotropic Material]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅸ</title><link href="https://harrypotterrrr.github.io//2020/12/03/cg-9.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅸ" /><published>2020-12-03T00:00:00-05:00</published><updated>2020-12-03T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2020/12/03/cg-9</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/12/03/cg-9.html"><![CDATA[<p>Advanced Ray Tracing, Radiometry, BRDF, Path Tracing</p>

<!--more-->

<h2 id="radiometry">Radiometry</h2>

<ul>
  <li>Motivation: Blinn-Phong is experienced model and light intensity corresponding to the amount of shading is not explained well, the output is not realistic but plastic. Whitted-style ray tracing sometimes doesn’t give correct result, especially involving how much light energy is refracted and reflected, and how much the rest is absorbed by the material, etc.
    <ul>
      <li>All the answers can be found in radiometry!</li>
    </ul>
  </li>
  <li>Measurement system and units for illumination</li>
  <li>Accurately measure the spatial properties of light (not temperal dimension, we still suppose light travel in straight line with out wave property)
    <ul>
      <li>New terms: Radiant flux, intensity, irradiance, radiance (辐射通量, 光强, 照度, 亮度)</li>
    </ul>
  </li>
  <li>Perform lighting calculations in a physically correct manner</li>
</ul>

<h3 id="radiant-energy-and-flux">Radiant Energy and Flux</h3>

<ul>
  <li><strong>Radiant energy</strong>: the energy of electromagnetic radiation. It is measured in units of joules, and denoted by the symbol:</li>
</ul>

\[Q[J=Joule]\]

<ul>
  <li><strong>Radiant flux</strong> (<strong>radiant power</strong>, <strong>luminous flux</strong>): the energy emitted, reflected, transmitted or received, <strong>per unit time</strong>.
    <ul>
      <li><strong>Flux</strong>: photons flowing through a sensor in unit time</li>
    </ul>
  </li>
</ul>

\[\Phi \equiv \frac{\mathrm{d} Q}{\mathrm{~d} t}\]

\[[\mathrm{~W}=\mathrm{Watt}][\mathrm{lm}=\operatorname{lumen}]^{\star}\]

<p><img src="https://z3.ax1x.com/2021/01/19/sgJQAI.png" alt="cg-9-1" style="margin:auto;display:block;" /></p>

<h3 id="radiant-intensity">Radiant Intensity</h3>

<ul>
  <li><strong>Radiant Intensity</strong>: The radiant (luminous) intensity is the power per unit solid angle emitted by a point light source.</li>
</ul>

\[I(\omega) \equiv \frac{\mathrm{d} \Phi}{\mathrm{d} \omega}\]

\[\left[\frac{\mathrm{W}}{\mathrm{sr}}\right]\left[\frac{\mathrm{lm}}{\mathrm{sr}}=\mathrm{cd}=\mathrm{candela}\right]\]

<ul>
  <li>The <strong>candela</strong> is one of the seven SI base units</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/sg8uxe.jpg" alt="cg-9-2" width="80%" /></p>

<h3 id="angles-and-solid-angles">Angles and Solid Angles</h3>

<ul>
  <li><strong>Angle</strong>: ratio of subtended arc length on circle to radius
    <ul>
      <li>$\theta = l / r$</li>
      <li>Circle has $2\pi$ radians</li>
    </ul>
  </li>
  <li><strong>Solid angle</strong>: ratio of subtended area on sphere to radius squared
    <ul>
      <li>$\omega = A / r^2$</li>
      <li>Sphere has $4\pi$ steradians</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/sg8V56.jpg" alt="cg-9-3" width="80%" /></p>

<ul>
  <li>Differential Solid Angles
    <ul>
      <li>Direction vector $\omega$ to denote direction vector of unit length</li>
      <li>$\theta$ : elevation, zenith angle (仰角)</li>
      <li>$\phi$ : azimuth, azimuth angle (方位角)</li>
    </ul>

\[\mathrm{d} A =(r \mathrm{~d} \theta)(r \sin \theta \mathrm{d} \phi) =r^{2} \sin \theta \mathrm{d} \theta \mathrm{d} \phi\]

\[\mathrm{d} \omega =\frac{\mathrm{d} A}{r^{2}}=\sin \theta \mathrm{d} \theta \mathrm{d} \phi\]

    <ul>
      <li>Sphere: $S^{2}$</li>
    </ul>
  </li>
</ul>

\[\Omega =\int_{S^{2}} \mathrm{~d} \omega = \int_{0}^{\pi} \sin \theta \mathrm{d} \theta \int_{0}^{2 \pi} \mathrm{d} \phi  =4 \pi\]

<p><img src="https://z3.ax1x.com/2021/01/19/sg8ePK.jpg" alt="cg-9-4" width="80%" /></p>

<ul>
  <li>Isotropic Point Source</li>
</ul>

\[\Phi = \int_{S^2}Idw = 4\pi I\]

\[I=\frac{\Phi}{4\pi}\]

<p><img src="https://z3.ax1x.com/2021/01/19/sgqUs0.jpg" alt="cg-9-5" width="80%" /></p>

<ul>
  <li>Modern LED Light
    <ul>
      <li>Output: 815 lumens (11W LED replacement paifor 60W incandescent)</li>
      <li>Radiant intensity (assume isotropic) = 815 lumens / 4π sr = 65 candelas</li>
    </ul>
  </li>
</ul>

<h3 id="irradiance">Irradiance</h3>

<ul>
  <li><strong>Irradiance</strong>: the power per (perpendicular / projected) unit area incident on a surface point.</li>
</ul>

\[E(\mathbf{x}) \equiv \frac{\mathrm{d} \Phi(\mathbf{x})}{\mathrm{d} A}\]

\[\left[\frac{\mathrm{W}}{\mathrm{m}^{2}}\right]\left[\frac{\mathrm{lm}}{\mathrm{m}^{2}}=\operatorname{lux}\right]\]

<ul>
  <li>The <strong>lux</strong> is SI derived unit of illuminance, measuring luminous flux per unit area.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/sg8uxe.jpg" alt="cg-9-2" width="80%" /></p>

<ul>
  <li><strong>Irradiance</strong> at surface is proportional to <strong>cosine</strong> of angle between light direction and surface normal
    <ul>
      <li>Recall <a href="/2020/09/02/cg-5#diffuse-reflection">Lambert’s Cosine Law</a></li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2AFXT.jpg" alt="cg-9-6" /></p>

<p class="info"><strong>Tip</strong>: Analogous <strong>Irradiance</strong> to <strong>Electric field intensity</strong>, <strong>Radiant flux</strong> to <strong>Magnetic Flux</strong>.</p>

<ul>
  <li>The reason why we feel cold and hot in different seasons
    <ul>
      <li>Earth’s axis of rotation: ~23.5° off axis</li>
      <li>Give rise to the different Solar elevation angle</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2AP10.jpg" alt="cg-9-7" /></p>

<ul>
  <li><a href="/2020/09/02/cg-5#light-falloff">Correction</a> of Irradiance Falloff
    <ul>
      <li>Assume light is emitting power $\Phi$ in a uniform angular distribution</li>
      <li>Compare irradiance at surface of two spheres</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2AicV.jpg" alt="cg-9-8" /></p>

<h3 id="radiance">Radiance</h3>

<ul>
  <li><strong>Radiance</strong> (<strong>luminance</strong>): the power emitted, reflected, transmitted or received by a surface, <strong>per unit solid angle</strong>, <strong>per projected unit area</strong>.
    <ul>
      <li>Radiance is the fundamental field quantity that describes the distribution of light in an environment</li>
      <li>Radiance is the quantity associated with a ray</li>
      <li>Rendering is all about computing radiance</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2ACpq.jpg" alt="cg-9-9" style="width:70%;margin:auto;display:block;" /></p>

\[L(\mathrm{p}, \omega) \equiv \frac{\mathrm{d}^{2} \Phi(\mathrm{p}, \omega)}{\mathrm{d} \omega \mathrm{d} A \cos \theta}\]

\[\left[\frac{\mathrm{W}}{\mathrm{sr} \mathrm{m}^{2}}\right]\left[\frac{\mathrm{cd}}{\mathrm{m}^{2}}=\frac{\mathrm{lm}}{\mathrm{sr} \mathrm{m}^{2}}=\mathrm{nit}\right]\]

<ul>
  <li>$\cos\theta$ accounts for projected surface area</li>
  <li>The <strong>nit</strong>, the candela per square metre, is the derived SI unit of luminance.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/sg8uxe.jpg" alt="cg-9-2" width="80%" /></p>

<ul>
  <li>Recall
    <ul>
      <li><strong>Irradiance</strong>: power per projected unit area</li>
      <li><strong>Intensity</strong>: power per solid angle</li>
      <li><strong>Radiance</strong>: power per solid angle, per projected unit area</li>
    </ul>
  </li>
  <li>Thus
    <ul>
      <li><strong>Radiance</strong>: Irradiance per solid angle</li>
      <li><strong>Radiance</strong>: Intensity per projected unit area</li>
    </ul>
  </li>
</ul>

<h4 id="incident-radiance">Incident Radiance</h4>

<ul>
  <li><strong>Incident Radiance</strong>: the irradiance per unit solid angle arriving at the surface
    <ul>
      <li>It is the light arriving at the surface along a given ray (point on surface and incident direction).</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2Aphn.jpg" alt="cg-9-10" style="width:70%;margin:auto;display:block;" /></p>

\[L(\mathrm{p}, \omega)=\frac{\mathrm{d} E(\mathrm{p})}{\mathrm{d} \omega \cos \theta}\]

<h4 id="exiting-radiance">Exiting Radiance</h4>

<ul>
  <li><strong>Exiting Radiance</strong>: the intensity per unit projected area leaving the surface
    <ul>
      <li>For an area light, it is the light emitted along a given ray (point on surface and exit direction).</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2ACpq.jpg" alt="cg-9-9" style="width:70%;margin:auto;display:block;" /></p>

\[L(\mathrm{p}, \omega)=\frac{\mathrm{d} I(\mathrm{p}, \omega)}{\mathrm{d} A \cos \theta}\]

<h4 id="irradiance-vs-radiance">Irradiance vs. Radiance</h4>

<ul>
  <li>Irradiance: total power received by area $dA$</li>
  <li>Radiance: power received by area $dA$ from direction $d\omega$</li>
</ul>

\[d E(\mathrm{p}, \omega) =L_{i}(\mathrm{p}, \omega) \cos \theta \mathrm{d} \omega\]

\[E(\mathrm{p}) =\int_{H^{2}} L_{i}(\mathrm{p}, \omega) \cos \theta \mathrm{d} \omega\]

<ul>
  <li>Unit Hemisphere $H^2$</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/19/s2AAnU.jpg" alt="cg-9-11" style="width:55%;margin:auto;display:block;" /></p>

<h2 id="bidirectional-reflectance-distribution-function">Bidirectional Reflectance Distribution Function</h2>

<ul>
  <li>Radiance from direction $\omega_i$ turns into the power $E$ that $dA$ receives</li>
  <li>The npower $E$ will become the radiance to any other direction $\omega_o$</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/29/yihyJx.jpg" alt="cg-9-12" style="width:55%" /></p>

<ul>
  <li>
    <p>Differential irradiance incoming: $d E\left(\omega_{i}\right)=L\left(\omega_{i}\right) \cos \theta_{i} d \omega_{i}$</p>
  </li>
  <li>
    <p>Differential radiance exiting (due to $dE(\omega_i)$): $dL_r(\omega_r)$</p>
  </li>
  <li>
    <p>BRDF can be understanded in the way of energy distribution after the point on the surface absorbs the energy from directions all around and then reflects.</p>
    <ul>
      <li>For the specular reflection, BRDF specifies the reflected energy is mostly on the reflection direction</li>
      <li>For the diffuse reflection, BRDF specifies the reflected energy will distributed amoung all directions around.</li>
    </ul>
  </li>
</ul>

<h3 id="brdf">BRDF</h3>

<ul>
  <li><strong>Bidirectional Reflectance Distribution Function</strong> (BRDF): represents how much light is reflected into each outgoing direction $\omega_r$ from each incoming direction</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/29/yihsF1.jpg" alt="cg-9-13" style="width:65%;margin:auto;display:block;" /></p>

\[f_{r}\left(\omega_{i} \rightarrow \omega_{r}\right)=\frac{\mathrm{d} L_{r}\left(\omega_{r}\right)}{\mathrm{d} E_{i}\left(\omega_{i}\right)}=\frac{\mathrm{d} L_{r}\left(\omega_{r}\right)}{L_{i}\left(\omega_{i}\right) \cos \theta_{i} \mathrm{~d} \omega_{i}}\left[\frac{1}{\mathrm{sr}}\right]\]

<ul>
  <li><strong>Conclusion</strong>: The BRDF, the ratio of exitant radiance to incident irradiance, is to characterize the directional properties of how a surface reflects light, which is independent of the strength of the light.
    <ul>
      <li>In fact, we use <em>cosine irradiance collector</em> to measure incident irradiance, <em>conical baffler</em> to measure received radiance, and calculate the BRDF corresponding to the surface/texture by dividing the two. <a href="https://www.zhihu.com/question/28476602/answer/41003204">More info</a>, and <a href="/2020/12/20/cg-10.html#Measuring-BRDF">later notes</a></li>
      <li>In a word, BRDF describes how the light interacts with the object. Accordingly, BRDF describes the property of the texture.</li>
    </ul>
  </li>
  <li>The quantity is between zero and one for reasons of energy conservation.</li>
</ul>

<h3 id="reflection-equation">Reflection Equation</h3>

<ul>
  <li>Consider BRDF only takes one direction of incident light, thus integral light (energy) from all directions of hemisphere will get the entire light source directing to the reflection point.
    <ul>
      <li>Understand in discrete way: traverse solid angles on the hemisphere, each incident solid angle will make contribution to the radiance of one specified direction. Sum these up will get the entire contribution, that is received energy.</li>
    </ul>
  </li>
</ul>

\[L_{r}\left(\mathrm{p}, \omega_{r}\right)=\int_{H^{2}} f_{r}\left(\mathrm{p}, \omega_{i} \rightarrow \omega_{r}\right) L_{i}\left(\mathrm{p}, \omega_{i}\right) \cos \theta_{i} \mathrm{~d} \omega_{i}\]

<p><img src="https://z3.ax1x.com/2021/01/29/yihDoR.jpg" alt="cg-9-14" style="width:70%;margin:auto;display:block;" /></p>

<ul>
  <li><strong>Challenege</strong>: recursion
    <ul>
      <li><strong>Reflected radiance</strong> $L_{r}\left(\mathrm{p}, \omega_{r}\right)$ on the left hand depends on <strong>incoming radiance</strong> $L_{i}\left(\mathrm{p}, \omega_{i}\right)$ on the right hand.</li>
      <li>But <strong>incoming radiance</strong> depends on reflected radiance at another point in the scene.
        <ul>
          <li>Incoming radiance is not limited to the light source, but reflected light etc.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="rendering-equation">Rendering Equation</h3>

<ul>
  <li>Rewrite the reflection equation by adding an <strong>Emission term</strong> $L_{e}\left(p, \omega_{o}\right)$ to make it general:</li>
</ul>

\[L_{o}\left(p, \omega_{o}\right)=L_{e}\left(p, \omega_{o}\right)+\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i}\]

<p class="error"><strong>Caveat</strong>: we assume that all directions are pointing <strong>outwards</strong>, and field of integration is hemisphere, which is the reason for $\cos\theta$ replacing $max(\cos\theta, 0)$.</p>

<ul>
  <li>
    <p>Volume rendering equation is a more general form of rendering equation.</p>
  </li>
  <li>
    <p>The solution of renderings equation will be narrated in the <a href="#path-tracing">following part</a>.</p>
  </li>
  <li>
    <p>Understanding of the rendering equation</p>
    <ul>
      <li>for the single light source</li>
    </ul>

\[\underset{\\ \quad \\ Reflected\ Light \\ (Output\ image)}{L_{r}\left(x, \omega_{r}\right)}= \underset{\\ \quad \\ Emission}{L_{e}\left(x, \omega_{r}\right)} + \underset{\\ \quad \\ Incident \ Light \\ (from\ light \\ source)}{L_{i}\left(x, \omega_{i}\right)} \underset{\\ \quad \\ BRDF}{f\left(x, \omega_{i}, \omega_{r}\right)} \underset{\\ \quad \\ Cosine\ of \\ Incident\ angle}{\left(\omega_{i}, n\right)}\]

    <ul>
      <li>for the multiple light sources</li>
    </ul>

\[\underset{\\ \quad \\ Reflected\ Light \\ (Output\ image)}{L_{r}\left(x, \omega_{r}\right)} = \underset{\\ \quad \\ Emission}{L_{e}\left(x, \omega_{r}\right)} + \sum \underset{\\ \quad \\ Incident \ Light \\ (from\ light \\ source)}{L_{i}\left(x, \omega_{i}\right)} \underset{\\ \quad \\ BRDF}{f\left(x, \omega_{i}, \omega_{r}\right)} \underset{\\ \quad \\ Cosine\ of \\ Incident\ angle}{\left(\omega_{i}, n\right)}\]

    <ul>
      <li>for the planar light sources</li>
    </ul>

\[\underset{\\ \quad \\ Reflected\ Light \\ (Output\ image)}{L_{r}\left(x, \omega_{r}\right)} = \underset{\\ \quad \\ Emission}{L_{e}\left(x, \omega_{r}\right)} + \int_\omega \underset{\\ \quad \\ Incident \ Light \\ (from\ light \\ source)}{L_{i}\left(x, \omega_{i}\right)} \underset{\\ \quad \\ BRDF}{f\left(x, \omega_{i}, \omega_{r}\right)} \underset{\\ \quad \\ Cosine\ of \\ Incident\ angle}{\cos \theta_i d\omega_i}\]

    <ul>
      <li>for the surfaces with reflected light (interreflection)</li>
    </ul>

\[\underset{\\ \quad \\ Reflected\ Light \\ (Output\ image) \\ \quad \color{red}{Unkown}}{L_{r}\left(x, \omega_{r}\right)} = \underset{\\ \quad \\ Emission \\ \color{blue}{Known}}{L_{e}\left(x, \omega_{r}\right)} + \int_\omega \underset{\\ \quad \\ Reflected\ light \\ from\ others \\ \quad \color{red}{Unkown}}{L_{r}\left(x^\prime, -\omega_i\right)} \underset{\\ \quad \\ BRDF \\ \color{blue}{Known}}{f\left(x, \omega_{i}, \omega_{r}\right)} \underset{\\ \quad \\ Cosine\ of \\ Incident\ angle \\ \color{blue}{Known}}{\cos \theta_i d\omega_i}\]

    <p class="info"><strong>Tip</strong>: $-\omega$ is because the light direction is pointing opposite of the incident direction with regards to $x^\prime$.</p>

    <ul>
      <li>which is the canonical form of <a href="#fredholm-integral-equation">Fredholm Integral Equation</a> of second kind</li>
    </ul>

\[\underset{\\ \quad \\ Radiance \\ \color{red}{Unkown}}{\color{red}{I(u)}} = \underset{\\ \quad \\ Emission}{e(u)} + \int \underset{\\ \quad \\ Radiance\\ from\ others \\ \color{red}{Unkown}}{\color{red}{I(v)}} \underset{\\ \quad \\ \quad Kernel\ of\ equation \\ \quad Light\ Transport\ Operator}{\boxed{K(u, v)dv}}\]

    <p class="info"><strong>Tip</strong>: Use substitution rule for integrals from the current object to the object of reflected light source will achieve $d\omega_i$ to $dv$, which will be detailed in <a href="#solution-of-rendering-equation">solution part</a>.</p>

    <ul>
      <li>which can be discretized to a simple matrix equation or system of simultaneous linear equations, where $L,\ E$ are vectors, $K$ is the light transport matrix (<strong>reflection operator</strong>)</li>
    </ul>

\[\color{red}{L} = E + K\color{red}{L}\]

    <ul>
      <li>rearrange terms and use taylor expassion</li>
    </ul>

\[\begin{aligned}\color{red}{L}&amp;=E+K\color{red}{L} \\ I\color{red}{L}-K\color{red}{L} &amp;= E \\ (I-K)\color{red}{L} &amp;= E \\ \color{red}{L} &amp;= (I-K)^{-1}E \\ \color{red}{L} &amp;= (I + K + K^2 + K^3 + ...)E \\ \color{red}{L} &amp;= E + KE + K^2E + K^3E + ...\end{aligned}\]

    <ul>
      <li>final form of ray tracing is</li>
    </ul>

\[L = \color{orange}{E + KE} + K^2E + K^3E + ...\]

    <ul>
      <li>understanding of the expression
        <ul>
          <li>$K$ : Reflection operator</li>
          <li>$E$ : Emission directly from light sources</li>
          <li>$KE$ : Direct illumination on surfaces</li>
          <li>$K^2E$ : Indirect illumination (One bounce indirect, mirros, refraction)</li>
          <li>$K^3E$ : Two bounce indirect illumination</li>
          <li>$\color{orange}{E + KE}$: <strong>Direct illumination</strong> is all the shading in <em>rasterization</em> can do</li>
        </ul>
      </li>
      <li><strong>Global illumination</strong>: the union of all direct and indirect light</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/06/yJ4T4e.jpg" alt="cg-9-15" /></p>

<ul>
  <li>The more bounce of illumination, the more realistic. When the bounce of illumination goes to infinity, the image finally becomes factual as in the real world.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/08/yUcPRs.gif" alt="cg-9-16" style="margin:auto;display:block;" /></p>

<h2 id="mathematics-foundation">Mathematics Foundation</h2>

<h3 id="fredholm-integral-equation">Fredholm Integral Equation</h3>

<ul>
  <li>
    <p>Equation of the first kind</p>

\[g(t)=\int_{a}^{b} K(t, s) f(s) \mathrm{d} s\]

    <ul>
      <li>Given the continuous kernel function $K$ and the function $g$, to find the function $f$</li>
    </ul>
  </li>
  <li>
    <p>Equation of the second kind</p>

\[\varphi(t)=f(t)+\lambda \int_{a}^{b} K(t, s) \varphi(s) \mathrm{d} s\]

    <ul>
      <li>Given the kernel $K(t,s)$, and the function $f(t)$, the problem is typically to find the function $\varphi(t)$</li>
    </ul>
  </li>
</ul>

<h3 id="linear-operator">Linear Operator</h3>

<ul>
  <li><strong>Operator</strong>: generally a mapping or function acting on elements of a space to produce elements of another space.</li>
  <li><strong>Linear Operator</strong> (kernel): $f$ is a linear</li>
  <li>
    <p>operator if it has two properties</p>

\[f(x + y) = f(x) + f(y)\]

\[f(cx) = cf(x)\]
  </li>
  <li>Thus, $f(ax+by) = af(x)+bf(y)$
    <ul>
      <li>If $M$ is a linear operator, $ab$ is constant, $fg$ is function</li>
    </ul>

\[M \bullet (af + bg) = a(M \bullet f) + b(M\bullet g)\]
  </li>
  <li>
    <p>The most common examples of linear operators $K$ of differentiation and integration</p>

\[(K \bullet f)(u)=\frac{\partial f}{\partial u}(u)\]

\[(K \bullet f)(u)=\int k(u, v) f(v) d v\]
  </li>
  <li>Proof of Fredholm Integral Equation as linear operator
    <ul>
      <li>Want to show</li>
    </ul>

\[(K \bullet f)(u)=\int k(u, v) f(v) d v\]

    <ul>
      <li>Then need to verify the two properties of linear operator</li>
    </ul>

\[\begin{aligned} &amp;M\bullet(af) = a(M\bullet f) \\ &amp; M\bullet(f+g) = (M\bullet f) + (M \bullet g) \end{aligned}\]

    <ul>
      <li>Addition</li>
    </ul>

\[\begin{aligned} (K \bullet (f+g))(u) &amp;= \int k(i, v)(f+g)(v)dv \\ &amp;= \int k(i, v)(f(v) + g(v))dv \\ &amp;= \int k(i, v)f(v)dv + \int k(i, v)g(v)dv \\ &amp;= (K \bullet f) (u) + (K \bullet g) (v) \end{aligned}\]

    <ul>
      <li>Multiplication</li>
    </ul>

\[\begin{aligned} (K \bullet af)(u) &amp;= \int k(i, v)(af)(v)dv \\ &amp;= \int ak(i, v)f(v)dv \\ &amp;= a \int k(i, v)f(v)dv \\ &amp;= a(K \bullet f) (u) \end{aligned}\]
  </li>
  <li>More info refer to <a href="https://en.wikipedia.org/wiki/Functional_analysis">functional analysis</a></li>
</ul>

<h3 id="monte-carlo-intergration">Monte Carlo Intergration</h3>

<ul>
  <li>It is too difficult to solve an integral analytically.
    <ul>
      <li>Thus estimate the integral of a function by averaging random samples of the function’s value.</li>
    </ul>
  </li>
  <li>Different with <a href="https://en.wikipedia.org/wiki/Riemann_integral">Riemann integral</a> which solve an integral analytically, <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo</a> estimates the value of integration</li>
  <li>Define the Monte Carlo estimnator for the definite integral of given function $f(x)$
    <ul>
      <li>Definite integral</li>
    </ul>

\[\int_{a}^{b} f(x) d x\]

    <ul>
      <li>Random variable, where $p(x)$ is <em>probability density function</em> (<em>pdf</em>)</li>
    </ul>

\[X_{i} \sim p(x)\]

    <ul>
      <li>MonteCarlo estimator</li>
    </ul>

\[F_{N}=\frac{1}{N} \sum_{i=1}^{N} \frac{f\left(X_{i}\right)}{p\left(X_{i}\right)}\]
  </li>
  <li>
    <p>Monte Carlo Integration</p>

\[\int f(x) dx = \frac{1}{N} \sum_{i=1}^{N} \frac{f\left(X_{i}\right)}{p\left(X_{i}\right)}\]

    <ul>
      <li>The more samples, the less variance</li>
      <li>Sample on x, integrate on x</li>
      <li>Don’t care about the domain of integration, but use numerical method sampling to estimate the value of integration</li>
    </ul>
  </li>
  <li>When the random variable distributes between $a$ and $b$ uniformly</li>
</ul>

\[X_{i} \sim p(x) = \frac{1}{b-a}\]

<ul>
  <li>Then the Basic Monte Carlo estimator will be</li>
</ul>

\[F_{N}=\frac{b-a}{N} \sum_{i=1}^{N} f\left(X_{i}\right)\]

<p><img src="https://z3.ax1x.com/2021/02/08/yUcSIg.png" alt="cg-9-17" style="margin:auto;display:block;" /></p>

<ul>
  <li>If we want to keep the sampling number but increase the estimation accurarcy, we should sample from a distribution of interest than the uniform distribution, that is <a href="https://en.wikipedia.org/wiki/Importance_sampling">Importance Sampling</a></li>
</ul>

<h2 id="path-tracing">Path Tracing</h2>

<h3 id="whitted-style-problem">Whitted-style problem</h3>

<ul>
  <li>Whitted-style ray tracing
    <ul>
      <li>Always perform specular reflections or refractions</li>
      <li>Stop bouncing at diffuse surfaces</li>
    </ul>
  </li>
  <li>
    <p>But these simplifications are not reasonable</p>

    <ul>
      <li>Whitted-style ray tracing can not deal with the glossy materials where the ray reflects not the same as the way on mirror material</li>
    </ul>

    <p><img src="https://z3.ax1x.com/2021/02/08/yUcCGj.jpg" alt="cg-9-18" /></p>

    <ul>
      <li>Whitted-style ray tracing has no reflection between diffuse material</li>
    </ul>

    <p><img src="https://z3.ax1x.com/2021/02/08/yUc9iQ.jpg" alt="cg-9-19" /></p>
  </li>
</ul>

<p class="info"><strong>Tip</strong>: In the left image of the above figure, the region that are not reached by direct illumination are shadowed in black, whereas the diffuse ambient actually illuminates the region. In the right image, the left side of rectangular is shaded in red, and right side of cubic is shaded in green, This phenomenon in which objects or surfaces are colored by reflection of colored light from nearby surfaces is called <strong>color bleeding</strong>.</p>

<h3 id="solution-of-rendering-equation">Solution of rendering equation</h3>

<ul>
  <li>The rendering equation to be solved</li>
</ul>

\[L_{o}\left(p, \omega_{o}\right)=L_{e}\left(p, \omega_{o}\right)+\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i}\]

<ul>
  <li>It involves two problems
    <ul>
      <li>Solve an integral over the hemisphere</li>
      <li>Deal with recursion of $L_{i}\left(p, \omega_{i}\right)$</li>
    </ul>
  </li>
</ul>

<h4 id="a-sample-monte-carlo-solution">A Sample Monte Carlo Solution</h4>

<ul>
  <li>Suppose we want to render one pixel (point) in the following scene for <strong>direct illumination</strong> only
    <ul>
      <li>No indirect reflection light is considered at this time</li>
    </ul>
  </li>
</ul>

\[L_{o}\left(p, \omega_{o}\right)=\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i}\]

<p><img src="https://z3.ax1x.com/2021/02/09/ydKcwT.jpg" alt="cg-9-20" style="width:50%;margin:auto;display:block;" /></p>

<ul>
  <li>
    <p>Use Monte Carlo method to solve this integration numerically</p>

\[\int_{a}^{b} f(x) \mathrm{d} x \approx \frac{1}{N} \sum_{k=1}^{N} \frac{f\left(X_{k}\right)}{p\left(X_{k}\right)} \quad X_{k} \sim p(x)\]

    <ul>
      <li>$f(x)$: $L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right)$</li>
      <li>$pdf$ (assume uniformly sampling the hemisphere): $pdf\left(\omega_{i}\right)=1 / 2 \pi$</li>
    </ul>
  </li>
  <li>
    <p>Shading algorithm for direct illumination</p>
  </li>
</ul>

\[\begin{aligned} L_{o}\left(p, \omega_{o}\right) &amp;=\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i} \\ &amp; \approx \frac{1}{N} \sum_{i=1}^{N} \frac{L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right)}{pdf\left(\omega_{i}\right)} \end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="n">Randomly</span> <span class="n">choose</span> <span class="o">**</span><span class="n">N</span><span class="o">**</span> <span class="n">directions</span> <span class="n">wi</span><span class="o">~</span><span class="n">pdf</span>
    <span class="n">Lo</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">wi</span><span class="p">:</span>
        <span class="n">Trace</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">the</span> <span class="n">light</span><span class="p">:</span>
            <span class="n">Lo</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">L_i</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Lo</span>
</code></pre></div></div>

<h4 id="introducing-global-illumination">Introducing Global Illumination</h4>

<ul>
  <li>One more step forward: what if a ray hits an object
    <ul>
      <li>Point $Q$ also reflects light to $P$, and the amount of radiance is the amount of radiance of direct illumination at point $Q$</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydlMYF.jpg" alt="cg-9-21" style="width:80%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="n">Randomly</span> <span class="n">choose</span> <span class="o">**</span><span class="n">N</span><span class="o">**</span> <span class="n">directions</span> <span class="n">wi</span><span class="o">~</span><span class="n">pdf</span>
    <span class="n">Lo</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">wi</span><span class="p">:</span>
        <span class="n">Trace</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">the</span> <span class="n">light</span><span class="p">:</span>
            <span class="n">Lo</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">L_i</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
        <span class="c1"># Add the following branch
</span>        <span class="k">elif</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">an</span> <span class="nb">object</span> <span class="n">at</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">Lo</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">shade</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">wi</span><span class="p">)</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
            <span class="c1"># minus wi is due to the reversed direction of the light towards object p
</span>    <span class="k">return</span> <span class="n">Lo</span>
</code></pre></div></div>

<h4 id="problem-1-number-explosion">Problem 1: Number Explosion</h4>

<ul>
  <li>Explosion of <code class="language-plaintext highlighter-rouge">num_rays</code> as <code class="language-plaintext highlighter-rouge">num_bounce</code> goes up</li>
</ul>

\[num\_ray = N^{num\_bounce}\]

<p><img src="https://z3.ax1x.com/2021/02/09/ydKDln.jpg" alt="cg-9-22" style="width:80%;" /></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">num_ray</code> will not explode if an only if $\color{red}{N = 1}$</p>
  </li>
  <li>
    <p><strong>Path Tracing</strong>: only <strong>1 ray</strong> is traced at each shading point</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="c1">#  modify N to One
</span>    <span class="n">Randomly</span> <span class="n">choose</span> <span class="o">**</span><span class="n">One</span><span class="o">**</span> <span class="n">directions</span> <span class="n">wi</span><span class="o">~</span><span class="n">pdf</span>
    <span class="n">Trace</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wi</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">the</span> <span class="n">light</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">L_i</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">an</span> <span class="nb">object</span> <span class="n">at</span> <span class="n">q</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">shade</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">wi</span><span class="p">)</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
</code></pre></div></div>

<p class="success"><strong>info</strong>: <a href="https://en.wikipedia.org/wiki/Distributed_ray_tracing">Distributed Ray Tracing</a>: $ N != 1$</p>

<h4 id="ray-generation">Ray Generation</h4>

<ul>
  <li>If the ray is only sampled once, the shaded point will be noisy!</li>
  <li><strong>BUT</strong> it doesn’t matter, because just trace more <strong>paths</strong> thorugh each pixel and average their radiance.
    <ul>
      <li><strong>The goal</strong> is to render pixels of the image, instead of each points of objects in the scene.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydKBSs.jpg" alt="cg-9-23" style="width:70%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ray_generation</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="n">Uniformly</span> <span class="n">choose</span> <span class="o">**</span><span class="n">N</span><span class="o">**</span> <span class="n">sample</span> <span class="n">positions</span> <span class="n">within</span> <span class="n">the</span> <span class="n">pixel</span>
    <span class="n">pixel_radiance</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">pixel</span><span class="p">:</span>
        <span class="n">shoot</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">cam_pos</span><span class="p">,</span> <span class="n">camera_to_sample</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">the</span> <span class="n">scene</span> <span class="n">at</span> <span class="n">p</span><span class="p">:</span>
            <span class="n">pixel_radiance</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="n">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="o">-</span> <span class="n">camera_to_sample</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pixel_radiance</span>
</code></pre></div></div>

<h4 id="problem-2-infinite-recursion">Problem 2: Infinite Recursion</h4>

<ul>
  <li>Dilemma: the light does not stop bouncing indeed in the real world</li>
  <li>Further, cut <code class="language-plaintext highlighter-rouge">num_bounce</code> is to cut <code class="language-plaintext highlighter-rouge">energy</code> and reduce authenticity</li>
  <li>Solution: Russian Roulette (RR)
    <ul>
      <li>With probability $ 0 \lt P \lt 1$, condition 1</li>
      <li>With probablity $1 - P$, otherwise</li>
    </ul>
  </li>
  <li>Previously, we always shoot a ray at a shading point and get the shading result $L_o$</li>
  <li>Suppose we manually set a probability $P$
    <ul>
      <li>With probability $P$, shoot a ray and return the shading result <strong>divided by</strong> $P$: $L_o / P$</li>
      <li>With probablity $1-P$, don’t shoot a ray and you’ll get $0$</li>
    </ul>
  </li>
  <li>In this way, you can still expect to get $L_o$</li>
</ul>

\[Expect = P * (L_o / P) + (1-P)* 0 = L_o\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="c1"># Add the following branch
</span>    <span class="n">Manually</span> <span class="n">specify</span> <span class="n">a</span> <span class="n">probability</span> <span class="n">p_RR</span>
    <span class="n">Randomly</span> <span class="n">select</span> <span class="n">ksi</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">uniform</span> <span class="n">distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ksi</span> <span class="o">&gt;</span> <span class="n">p_RR</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="n">Randomly</span> <span class="n">choose</span> <span class="o">**</span><span class="n">One</span><span class="o">**</span> <span class="n">directions</span> <span class="n">wi</span><span class="o">~</span><span class="n">pdf</span>
    <span class="n">Trace</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wi</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">the</span> <span class="n">light</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">L_i</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_RR</span>
    <span class="k">elif</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">an</span> <span class="nb">object</span> <span class="n">at</span> <span class="n">q</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">shade</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">wi</span><span class="p">)</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosine</span> <span class="o">/</span> <span class="n">pdf</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_RR</span>
</code></pre></div></div>

<ul>
  <li>Expected value of bounce number, given probability $p$</li>
</ul>

\[\begin{aligned}\sum_{n=1}^{+\infty} p^{n} &amp;=\lim _{n \rightarrow+\infty} \frac{p(1-p^{n})}{1-p} \\ &amp;=\frac{p}{1-p}\ (0 \lt p \lt 1) \end{aligned}\]

\[\begin{aligned} E &amp;=\sum_{n=1}^{+\infty} n p(1-p)^{n-1} \\ &amp;=p \sum_{n=1}^{+\infty} n(1-p)^{n-1} \\ &amp;=-p\left(\sum_{n=1}^{+\infty} (1-p)^{n}\right)^{\prime} \\ &amp;=-p\left(\frac{1-p}{p}\right)^{\prime} \\ &amp;=-p \frac{-p-(1-p)}{p^{2}} \\ &amp;=\frac{1}{p} \\ \end{aligned}\]

<h3 id="sampling-the-light">Sampling the Light</h3>

<ul>
  <li>Path tracing is correct but not efficient actually</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydQ0rq.jpg" alt="cg-9-24" /></p>

<ul>
  <li>The reason of being inefficient is the sampled 1 ray <strong>may not</strong> hit the light, so that a lot of rays are wasted if we uniformly sample the hemisphere at the shading point</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydK6mV.jpg" alt="cg-9-25" style="width:80%;margin:auto;display:block;" /></p>

<ul>
  <li>Sampling on the shading point will lead to waste of rays which fail to hit the light source
    <ul>
      <li>Monte Carlo methods allows any sampling methods</li>
      <li>Hence, change the sampling point to the light</li>
    </ul>
  </li>
  <li>Assume uniformly sampling on the light</li>
</ul>

\[\int pdf \color{red}{dA} = 1\]

\[Hence,\ pdf = \frac{1}{A}\]

<ul>
  <li>The rendering equation integrates on <strong>the solid angle</strong>
    <ul>
      <li>Recall Monte Carlo Integration: Sample on $x$, and integrate on $x$</li>
      <li>Integral variable substitution</li>
    </ul>
  </li>
</ul>

\[L_o = \int L_r f_BRDF \cos\theta \color{red}{d\omega}\]

<ul>
  <li>Make the rendering equation converting converting from an integral of $d\omega$ to $dA$.</li>
  <li>The relationship between $d\omega$ and $dA$
    <ul>
      <li>The projected area towards $x^\primex$ at point $x^\prime$ is $d A \cos \theta^{\prime}$</li>
      <li>Recall that $dS = R^2d\theta$</li>
    </ul>
  </li>
</ul>

\[d \omega=\frac{d A \cos \theta^{\prime}}{\left\|x^{\prime}-x\right\|^{2}}\]

<p><img src="https://z3.ax1x.com/2021/02/09/ydKsO0.jpg" alt="cg-9-26" style="width:50%;margin:auto;display:block;" /></p>

<h3 id="final-form-of-path-tracing">Final Form of Path Tracing</h3>

<ul>
  <li>Rewrite the rendering equation as final form with an integral on $dA$</li>
</ul>

\[\begin{aligned} L_{o}\left(x, \omega_{o}\right) &amp;=\int_{\Omega^{+}} L_{i}\left(x, \omega_{i}\right) f_{r}\left(x, \omega_{i}, \omega_{o}\right) \cos \theta \mathrm{d} \omega_{i} \\ &amp;=\int_{A} L_{i}\left(x, \omega_{i}\right) f_{r}\left(x, \omega_{i}, \omega_{o}\right) \frac{\cos \theta \cos \theta^{\prime}}{\left\|x^{\prime}-x\right\|^{2}} \mathrm{~d} A \end{aligned}\]

<ul>
  <li>Now integration is on the light, so for Monte Carlo integration:
    <ul>
      <li>$f(x)$ : $L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right) \frac{\cos \theta \cos \theta^{\prime}}{\left|x^{\prime}-x\right|^{2}}$</li>
      <li>$pdf$ : $1 / A$</li>
    </ul>
  </li>
  <li>Previously, we assume the light is accidentally shot by uniform hemisphere sampling</li>
  <li>Now we consider the radiance coming from two parts:
    <ul>
      <li>Light source (direct, no need to have RR), if the ray is <strong>not blocked</strong> in the middle</li>
      <li>Other reflectors (indirect, need to have RR)</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydKRkF.jpg" alt="cg-9-27" style="width:70%;margin:auto;display:block;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shade</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wo</span><span class="p">)</span>
    <span class="c1"># Contribution from the light source
</span>    <span class="n">Uniformly</span> <span class="n">sample</span> <span class="n">the</span> <span class="n">light</span> <span class="n">at</span> <span class="n">x</span><span class="s">' (pdf_light = 1 / A)
    L_dir = 0.0
    Shoot a ray from p to x'</span>
    <span class="k">if</span> <span class="n">the</span> <span class="n">ray</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">blocked</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">middle</span><span class="p">:</span>
        <span class="n">L_dir</span> <span class="o">=</span> <span class="n">Li</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosθ</span> <span class="o">*</span> <span class="n">cosθ</span><span class="s">' / |x'</span> <span class="o">-</span> <span class="n">p</span><span class="o">|^</span><span class="mi">2</span> <span class="o">/</span> <span class="n">pdf_light</span>

    <span class="c1"># Contribution from other reflections
</span>    <span class="c1">## Test RR
</span>    <span class="n">Manually</span> <span class="n">specify</span> <span class="n">a</span> <span class="n">probability</span> <span class="n">p_RR</span>
    <span class="n">Randomly</span> <span class="n">select</span> <span class="n">ksi</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">uniform</span> <span class="n">distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ksi</span> <span class="o">&gt;</span> <span class="n">p_RR</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">L_dir</span>
    <span class="c1">## Ray Travce
</span>    <span class="n">L_indir</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">Uniformly</span> <span class="n">sample</span> <span class="n">the</span> <span class="n">hemisphere</span> <span class="n">toward</span> <span class="n">wi</span> <span class="p">(</span><span class="n">pdf_hemi</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="n">pi</span><span class="p">)</span>
    <span class="n">Trace</span> <span class="n">a</span> <span class="n">ray</span> <span class="n">r</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">wi</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ray</span> <span class="n">r</span> <span class="n">hit</span> <span class="n">a</span> <span class="n">non</span><span class="o">-</span><span class="n">emitting</span> <span class="nb">object</span> <span class="n">at</span> <span class="n">q</span><span class="p">:</span>
        <span class="n">L_indir</span> <span class="o">=</span> <span class="n">shade</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">wi</span><span class="p">)</span> <span class="o">*</span> <span class="n">f_r</span> <span class="o">*</span> <span class="n">cosθ</span> <span class="o">/</span> <span class="n">pdf_hemi</span> <span class="o">/</span> <span class="n">p_RR</span>
    <span class="k">return</span> <span class="n">L_dir</span> <span class="o">+</span> <span class="n">L_indir</span>
</code></pre></div></div>

<ul>
  <li>Path tracing is almost 100% correct, a.k.a <strong>Photo-realistic</strong></li>
</ul>

<p><img src="https://z3.ax1x.com/2021/02/09/ydKgTU.jpg" alt="cg-9-28" /></p>

<h2 id="summary--future-work">Summary &amp; Future work</h2>

<h3 id="summary">Summary</h3>

<ul>
  <li>Ray tracing: Previous vs. Modern Concepts
    <ul>
      <li>Previous: Ray tracing == Whitted-style ray tracing</li>
      <li>Modern
        <ul>
          <li>The general solution of light transport, including:</li>
          <li>(Unidirectional or Bidirectional) Path tracing</li>
          <li>Photon mapping</li>
          <li>Metropolis light transport</li>
          <li>VCM / UPBP …</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="future-work">Future work</h3>

<ul>
  <li>Uniformly sampling the hemisphere
    <ul>
      <li>How? and in general, how to sample any function? (sampling theory)</li>
    </ul>
  </li>
  <li>Monte Carlo integration allows arbitrary $pdf$
    <ul>
      <li>Instead of uniform sampling, what’s the best choice of sampling with regards to different shapes of function? (importance sampling)</li>
    </ul>
  </li>
  <li>Do random numbers matter?
    <ul>
      <li>Yes! (low discrepancy sequences)</li>
    </ul>
  </li>
  <li>I can sample the hemisphere and the light
    <ul>
      <li>Can I combine them? Yes! (multiple importance sampling)</li>
    </ul>
  </li>
  <li>The radiance of a pixel is the averange of radiance on all paths passing through it
    <ul>
      <li>Why? If different weighted sampling at the center of pixel or the edge is needed? (pixel reconstruction filter)</li>
    </ul>
  </li>
  <li>Is the radiance of a pixel the color of a pixel?
    <ul>
      <li>No. (gamma correction, curves concerning high dynamic range (HDR) image , color space, photography)</li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/21376124">基于物理着色：BRDF</a></li>
  <li><a href="https://blog.csdn.net/leonwei/article/details/44539217">物理渲染(PBR)-基于物理的光照模型</a></li>
  <li><a href="https://www.zhihu.com/question/28476602/answer/41003204">BRDF单位为sr^-1的原因</a></li>
  <li><a href="https://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/chuckm/chuckm_BRDFs_overview.html">Bi-Directional Reflectance Distribution Functions</a></li>
  <li><a href="https://www.qiujiawei.com/monte-carlo/">蒙特·卡罗积分</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/146144853">蒙特卡洛积分概述</a></li>
  <li><a href="http://www.twistedwg.com/2018/05/29/MC-integral.html">蒙特卡洛（Monte Carlo）法求定积分</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/56443239">Rendering Equation求解</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Advanced Ray Tracing, Radiometry, BRDF, Path Tracing]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅷ</title><link href="https://harrypotterrrr.github.io//2020/11/25/cg-8.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅷ" /><published>2020-11-25T00:00:00-05:00</published><updated>2020-11-25T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2020/11/25/cg-8</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/11/25/cg-8.html"><![CDATA[<p>Whitted Style Ray Tracing, Ray-Surface Intersection, Acceleration of Ray Tracing, Ray-Intersection with AABB, KD-Tree, BVH</p>

<!--more-->

<h2 id="ray-tracing">Ray Tracing</h2>

<ul>
  <li>Rasterization can not handle <strong>global effect</strong> well
    <ul>
      <li>Soft shadows</li>
      <li>Glossy reflection, transparent / subtransparent object</li>
      <li>Indirect illumination when light bounces more than once</li>
    </ul>
  </li>
  <li>Ray tracing is accurate, but is very slow
    <ul>
      <li>Rasterization: real time, Ray tracing: offline</li>
      <li>Time estimation: ~10K CPU core hours to render one frame in video production</li>
    </ul>
  </li>
</ul>

<h3 id="light-rays">Light Rays</h3>

<ul>
  <li>Three ideas about light rays:
    <ul>
      <li>Light travels in straight lines, though this is wrong because light is a wave which has volatility.</li>
      <li>Light rays do not collide with each other if they cross, though this is still not wrong.</li>
      <li>Light rays travel from the light sources to the eye, and the reciprocity of the light means the physics is invariant under path reversal.</li>
    </ul>
  </li>
</ul>

<h3 id="ray-casting">Ray Casting</h3>

<ul>
  <li>
    <p>Pinhole Camera Model</p>
  </li>
  <li>
    <p>Generate an image by casting one ray per pixel</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/17/sr6ECR.jpg" alt="cg-8-1" /></p>

<ul>
  <li>Check for shadows by sending a ray to the light</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/17/sr6FUJ.jpg" alt="cg-8-2" /></p>

<ul>
  <li><strong>Whitted-Style Ray Tracing</strong>: <strong>Recursive</strong> Ray Tracing</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/17/sr6k59.gif" alt="cg-8-3" /></p>

<ul>
  <li><strong>Key idea</strong>: ray tracing is to simulate the ray casting</li>
</ul>

<h3 id="ray-surface-intersection">Ray-Surface Intersection</h3>

<h4 id="ray-equation">Ray Equation</h4>

<ul>
  <li>Ray is defined by its origin and a direction vector</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/17/sr6iE4.jpg" alt="cg-8-4" /></p>

<ul>
  <li>Ray equation $\boldsymbol{r}(t) = \boldsymbol{o} + t\boldsymbol{d}$ where, $0 \leq t &lt; \infty$
    <ul>
      <li>$o$: origin point</li>
      <li>$d$: <strong>normalized</strong> direction of the ray</li>
      <li>$r(t)$: point along the ray</li>
    </ul>
  </li>
</ul>

<h4 id="intersect-with-sphere">Intersect with Sphere</h4>

<ul>
  <li>Ray: $\boldsymbol{r}(t) = \boldsymbol{o} + t\boldsymbol{d}, 0 \leq t &lt; \infty$</li>
  <li>Sphere: $ \boldsymbol{p}:\ (\boldsymbol{p}-\boldsymbol{c})^2-R^2 = 0$</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/17/sr6CbF.jpg" alt="cg-8-5" /></p>

<ul>
  <li>The intersection $p$ must satisfy both ray equation and sphere equation</li>
  <li>Intersection: $ (\boldsymbol{o}+ t\boldsymbol{d}-\boldsymbol{c})^2-R^2 = 0$</li>
</ul>

\[a t^{2}+b t+c=0,\ where\]

\[\left\{\begin{aligned} a=&amp;\mathrm{d} \cdot \mathrm{d} \\
b=&amp;2(\mathbf{o}-\mathbf{c}) \cdot \mathrm{d} \\
c=&amp;(\mathbf{o}-\mathbf{c}) \cdot(\mathbf{o}-\mathbf{c})-R^{2}\end{aligned}\right.\]

\[t=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}\]

<ul>
  <li>Ray intersects with sphere if only if $\varDelta \ge 0$ <strong>and</strong> $t \ge 0$, otherwise no intersection.</li>
  <li>Given the solved $t$, intersection is $\boldsymbol{o} + t\boldsymbol{d}$.</li>
</ul>

<h4 id="intersect-with-implicit-surface">Intersect With Implicit Surface</h4>

<ul>
  <li>Ray: $\boldsymbol{r}(t) = \boldsymbol{o} + t\boldsymbol{d}$, where $0 \leq t &lt; \infty$</li>
  <li>General implicit surface: $\boldsymbol{p}: f(\boldsymbol{p}) = 0$</li>
  <li>Substitute ray equation: $f(\boldsymbol{o}+t\boldsymbol{d}) = 0$</li>
  <li>Solve for real, positive roots of $t$</li>
</ul>

<p class="success"><strong>Info</strong>: we usually don’t care about how to solve the equation, because numerical optimization will always help us to find an approximate solution of the problem corresponding to the given equation of implicit surface.</p>

<h4 id="plane-equation">Plane Equation</h4>

<ul>
  <li>Plane is defined by normal vector and a point on plane</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/s6F3Y6.jpg" alt="cg-8-6" /></p>

<ul>
  <li>Plane Equation: $\boldsymbol{p}: (\boldsymbol{p}-\boldsymbol{p^\prime}) \cdot \boldsymbol{N}=0$
    <ul>
      <li>$p$: all points on plane</li>
      <li>$p^\prime$: one point on plane</li>
      <li>$N$: normal vector</li>
    </ul>
  </li>
  <li>Expland the plane equation will get Plane Equation in Cartesian coordinates: $ax + by + cz + d = 0$</li>
  <li>Solve for intersection
    <ul>
      <li>Set $\boldsymbol{p} = \boldsymbol{r}(t)$ and solve for $t$</li>
      <li>Check if $0 \leq t&lt;\infty$ intersection inside the plane, otherwise not.</li>
    </ul>
  </li>
</ul>

\[\left(\mathbf{p}-\mathbf{p}^{\prime}\right) \cdot \mathbf{N}=\left(\mathbf{o}+t \mathbf{d}-\mathbf{p}^{\prime}\right) \cdot \mathbf{N}=0\]

\[t=\frac{\left(\mathbf{p}^{\prime}-\mathbf{o}\right) \cdot \mathbf{N}}{\mathbf{d} \cdot \mathbf{N}} \quad\]

<ul>
  <li>After get the $t$ and intersection with the plane, check if the intersection is inside triangle
    <ul>
      <li>Use three <a href="/2020/08/11/cg-2#cross-product">cross product</a> and check if the results are all in the same direction</li>
    </ul>
  </li>
</ul>

<h4 id="intersect-with-triangle-mesh">Intersect with Triangle Mesh</h4>

<ul>
  <li>Usage: visiblity, shadows, lighting …</li>
  <li>Geometry: inside / outside test</li>
</ul>

<p class="info"><strong>Tip</strong>: Point in Polygon test is: given a point $p$ and a polygon $Q$, draw a line from the query point $p$ to other point far away in the plane, which should be outside $Q$, then count the number of intersection of this ray with $Q$. If odd number of intersections, the point $p$ is inside polygon $Q$, otherwise outside.</p>

<p><img src="https://z3.ax1x.com/2021/01/18/s6F8fK.jpg" alt="cg-8-7" /></p>

<ul>
  <li>Idea: just intersect ray with each triangle</li>
  <li>Simple, but very slow since for each resolution pixel of the screen will cast a ray and do intersection test with each triangle in mesh</li>
  <li>
    <p>Only care 0, 1 intersection, ignore multiple edge cases</p>
  </li>
  <li>Triangle is in a plane
    <ul>
      <li>Ray-plane intersection</li>
      <li>Test if hit point is inside triangle</li>
    </ul>
  </li>
  <li><strong>Möller Trumbore Algorithm</strong>
    <ul>
      <li>Given barycentric coordinate of triangle, a faster and direct approach to calculate intersection</li>
      <li>Use <a href="https://en.wikipedia.org/wiki/Cramer's_rule">Cramer’s rule</a> to solve for system of linear equations</li>
      <li>The proof can be found <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-rendering-a-triangle/moller-trumbore-ray-triangle-intersection">here</a></li>
    </ul>
  </li>
</ul>

\[\overrightarrow{\mathbf{o}}+t \overrightarrow{\mathbf{d}}=\left(1-b_{1}-b_{2}\right) \overrightarrow{\mathbf{P}}_{0}+b_{1} \overrightarrow{\mathbf{P}}_{1}+b_{2} \overrightarrow{\mathbf{P}}_{2}\]

\[\left[\begin{array}{l}t \\ b_{1} \\ b_{2}\end{array}\right]=\frac{1}{\overrightarrow{\mathbf{S}}_{1} \cdot \overrightarrow{\mathbf{E}}_{1}}\left[\begin{array}{c}\overrightarrow{\mathbf{S}}_{2} \cdot \overrightarrow{\mathbf{E}}_{2} \\ \overrightarrow{\mathbf{S}}_{1} \cdot \overrightarrow{\mathbf{S}} \\ \overrightarrow{\mathbf{S}}_{2} \cdot \overrightarrow{\mathbf{d}}\end{array}\right]\]

\[where\ \left\{\begin{aligned}
\overrightarrow{\mathbf{E}}_{1}=\overrightarrow{\mathbf{P}}_{1}-\overrightarrow{\mathbf{P}}_{0} \\
\overrightarrow{\mathbf{E}}_{2}=\overrightarrow{\mathbf{P}}_{2}-\overrightarrow{\mathbf{P}}_{0} \\
\overrightarrow{\mathbf{S}}=\overrightarrow{\mathbf{o}}-\overrightarrow{\mathbf{P}}_{0} \\
\overrightarrow{\mathbf{S}}_{1}=\overrightarrow{\mathbf{d}} \times \overrightarrow{\mathbf{E}}_{2} \\
\overrightarrow{\mathbf{S}}_{2}=\overrightarrow{\mathbf{S}} \times \overrightarrow{\mathbf{E}}_{1}
\end{aligned}\right.\]

<ul>
  <li>After get $t$, $b_1$,\ $b_2$
    <ul>
      <li>Check if $t \ge 0$ then the intersection is in the plane, otherwise not</li>
      <li>Check if $0 \le b_1,\ b_2,\ 1-b_1-b_2 \le 1$ then the intersection is <a href="/2020/09/07/cg-6#barycentric-coordinate">inside triangle</a>, otherwise not</li>
      <li>If <strong>both</strong> conditions are satisfied, the ray does intersect with the triangle</li>
    </ul>
  </li>
</ul>

<h2 id="acceleration">Acceleration</h2>

<ul>
  <li>Simple ray-scene intersection
    <ul>
      <li>Exhaustively test ray-intersection with every triangle</li>
      <li>Find the closest hit</li>
    </ul>
  </li>
  <li>Problem
    <ul>
      <li>Naive algorithm = num_pixels x num_triangles x num_bounces</li>
      <li>Very slow</li>
    </ul>
  </li>
</ul>

<h3 id="bounding-volumes">Bounding Volumes</h3>

<ul>
  <li>Quick way to avoid intersections: bound complex object with a simple volume
    <ul>
      <li>Object is fully contained in the volume</li>
      <li>If it doesn’t hit the volume, it doesn’t hit the object</li>
      <li>Thus, test bounding volume first, then test object if it hits</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/s6F1Fx.jpg" alt="cg-8-8" /></p>

<h3 id="ray-intersection-with-box">Ray-Intersection With Box</h3>

<ul>
  <li>
    <p>Box is the intersection of 3 pairs of slabs</p>
  </li>
  <li>
    <p><strong>Axis-Aligned Bounding Box</strong> (<strong>AABB</strong>): the smallest enclosing box with any side along either x, y, z axis.</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/s6FQT1.jpg" alt="cg-8-9" width="40%" /></p>

<ul>
  <li>‘Axis-Aligned’ is to reduce computational time
    <ul>
      <li>For the general plane, 3 subtractions, 6 multiplies, 1 division required<br />
\(t=\frac{\left(\mathbf{p}^{\prime}-\mathbf{o}\right) \cdot \mathbf{N}}{\mathbf{d} \cdot \mathbf{N}} \quad\)</li>
      <li>For slabs perpendicular to x-axis, only 1 subtraction, 1 division required<br />
\(t=\frac{\mathbf{p}^\prime_x - \mathbf{o}_x}{\mathbf{d}_x}\)</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEUqe.jpg" alt="cg-8-12" /></p>

<h4 id="intersect-with-2d-aabb">Intersect with 2D AABB</h4>

<ul>
  <li>2D example of computing intersections with slabs and take <strong>intersection</strong> of $t_{min}/ t_{max}$ intervals</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEdVH.jpg" alt="cg-8-10" /></p>

<h4 id="intersect-with-3d-aabb">Intersect with 3D AABB</h4>

<ul>
  <li>A box in 3D = three pairs of infinitely large slabs</li>
  <li>Key ideas
    <ul>
      <li>The ray enters the box <strong>only when</strong> it enters all pairs of slabs</li>
      <li>The ray exits the box <strong>as long as</strong> it exits any pair of slabs</li>
    </ul>
  </li>
  <li>For each pair, calculate the $t_{min}$ and $t_{max}$. <strong>Negative</strong> is fine and will be discussed later</li>
  <li>For the 3D box, $t_{enter} = max \{ t_{min} \}$, $t_{exit} = min \{ t_{max} \} $</li>
  <li>If $t_{enter} \lt t_{exit}$, we know the ray stays a while in the box, thus ray must intersect with bounding box. But this is <strong>NOT</strong> absolutely correct!
    <ul>
      <li>e.g. the following ray won’t intersect with the bounding box</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/s6FJSO.jpg" alt="cg-8-11" width="60%" /></p>

<ul>
  <li>Problem: what about the $t$ is negative?</li>
  <li>Ray is not a line, but a ray
    <ul>
      <li>Should check whether $t$ is negative for physical correctness</li>
    </ul>
  </li>
  <li>What if $t_{exit} \lt 0$
    <ul>
      <li>The box is behind the ray: no intersection</li>
    </ul>
  </li>
  <li>What if $t_{exit} \ge 0$ and $t_{enter} \lt 0$
    <ul>
      <li>The ray’s origin is inside the box: have intersection</li>
    </ul>
  </li>
  <li>In summary, ray and AABB intersect if and only if
    <ul>
      <li>$t_{enter} \lt t_{exit} 0\ \&amp;\&amp;\ t_{exit} \ge 0$</li>
    </ul>
  </li>
</ul>

<h3 id="uniform-spatial-partitions">Uniform Spatial Partitions</h3>

<ul>
  <li>Preprocess to build acceleration grid
    <ul>
      <li>Find bounding box</li>
      <li>Create grid</li>
      <li>Store each object in overlapping cells (test if the object’s surface intersects with cells)</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEtKO.jpg" alt="cg-8-13" /></p>

<ul>
  <li>Ray Scene Intersection
    <ul>
      <li>Step through grid in ray traversal order</li>
      <li>For each grid cell
        <ul>
          <li>Test intersection with all objects stored at that cell</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scENrD.jpg" alt="cg-8-14" /></p>

<ul>
  <li>Grid resolution should not be too small nor too large
    <ul>
      <li>One cell: no speedup</li>
      <li>Many cells: infficiency due to extraneous grid traversal</li>
    </ul>
  </li>
  <li>Heuristic experience
    <ul>
      <li>num_cells = Const * num_objs</li>
      <li>Const ≈ 27 in 3D</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEJxK.jpg" alt="cg-8-15" /></p>

<ul>
  <li>Uniform Grids’ Problem
    <ul>
      <li>Work well on large collections of objects that are distributed evenly in size and space</li>
      <li>But fail on the scene where large ratio of space is empty with no objects or the objects are not evenly distributed, i.e. “Teapot in a stadium” problem</li>
      <li>Spatial Partition to solve this problem!</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scE0IA.jpg" alt="cg-8-16" /></p>

<h3 id="spatial-partitions">Spatial Partitions</h3>

<ul>
  <li>Partition the 3D space using plane
    <ul>
      <li><strong>Oct-Tree</strong>: Recursively partition the space into 8 subspace until specified condition, e.g. the new splitted cells are empty with no objects</li>
      <li><strong>KD-Tree</strong> (short for k-dimensional tree): Alternatively use xy-plane, yz-plane, xz-plane to divide objects into different space</li>
      <li><strong>BSP-Tree</strong> (short for binary space partitioning tree): More general case of KD-Tree. Recursively subdividing a space into two subspace</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEyxf.jpg" alt="cg-8-17" /></p>

<ul>
  <li>Data Structure for KD-Tree
    <ul>
      <li>Internal Nodes store
        <ul>
          <li>split axis: x-, y-, or z-axis</li>
          <li>split position: coordinate of split plane along axis</li>
          <li>children: pointers to child nodes</li>
          <li><strong>No objects</strong> are stored in internal nodes</li>
        </ul>
      </li>
      <li>Leaf nodes store
        <ul>
          <li>list of objects</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEwad.gif" alt="cg-8-18" /></p>

<ul>
  <li>Traversing a KD-Tree</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scZipq.gif" alt="cg-8-19" /></p>

<ul>
  <li>Problem existed in KD-Tree method
    <ul>
      <li>Computing the overlap between triangle and AABB to build KD-Tree is difficult, though intersection algorithm exist, detailed <a href="https://fileadmin.cs.lth.se/cs/Personal/Tomas_Akenine-Moller/code/">here</a></li>
      <li>One objects can be included in multiple cells</li>
    </ul>
  </li>
</ul>

<h3 id="object-partitions">Object Partitions</h3>

<ul>
  <li><strong>Bounding Volume Hierarchy</strong> (BVH): tree structure on a set of geometric objects, which are wrapped in bounding volumes
    <ul>
      <li>Find bounding box</li>
      <li>Recursively split set of objects in two subsets</li>
      <li>Recompute the bounding box of the subsets</li>
      <li>Stop when necessary</li>
      <li>Store objects in each leaf node</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEsRP.gif" alt="cg-8-20" /></p>

<ul>
  <li>The problem existed in KD-Tree are solved
    <ul>
      <li>No need to compute the overlap because overlap doesn’t influence, but precompute the bounding box which may overlap one another</li>
      <li>One object only exist in one node</li>
    </ul>
  </li>
  <li>BVH is the preprocess to accelerate unnecessary computation before ray tracing and shading, but for each time the objects move or displace to somewhere else, BVH will be built once again <strong>for each frame</strong>. Thus, Real-time ray tracing is great difficult.
    <ul>
      <li>Thus it is hard for real-time ray tracing to deal with shape deformation and position transformation, since it is great time-costly to build accelerating structure</li>
    </ul>
  </li>
  <li>Strategy to subdivide a node to build BVH
    <ul>
      <li>Choose a dimension to split: Always choose the longest axis in node, especially when objects in elongated bounding box</li>
      <li>Choose split position: Split node at location of <strong>median</strong> object, which is beneficial to build a more <strong>balanced</strong> tree with more evenly depth of two side.</li>
    </ul>
  </li>
  <li>Termination criteria: stop when node contains few elements (e.g. heuristically 5)</li>
</ul>

<p class="info"><strong>Tip</strong>: Choosing split position of median object can be implement by sorting the barycentric of each triangle and select the median one, which take $O(nlog(n))$ time. While <a href="https://en.wikipedia.org/wiki/Selection_algorithm">quick selection algorithm</a> for finding the kth smallest value can optimize the consuming complexity to $O(n)$.</p>

<ul>
  <li>Data Structure for KD-Tree
    <ul>
      <li>Internal nodes store
        <ul>
          <li>Bounding box</li>
          <li>Children: pointers to child nodes</li>
        </ul>
      </li>
      <li>Leaf nodes store
        <ul>
          <li>Bounding box</li>
          <li>List of objects</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Traversing BVH</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Intersect</span><span class="p">(</span><span class="n">Ray</span> <span class="n">ray</span><span class="p">,</span> <span class="n">BVH</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ray</span> <span class="n">misses</span> <span class="n">node</span><span class="p">.</span><span class="n">bbox</span><span class="p">)</span> 
        <span class="k">return</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">node</span> <span class="n">is</span> <span class="n">a</span> <span class="n">leaf</span> <span class="n">node</span><span class="p">){</span>
        <span class="n">test</span> <span class="n">intersection</span> <span class="n">with</span> <span class="n">all</span> <span class="n">objects</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">closest</span> <span class="n">intersection</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">hit1</span> <span class="o">=</span> <span class="n">Intersect</span><span class="p">(</span><span class="n">ray</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">child1</span><span class="p">);</span>
    <span class="n">hit2</span> <span class="o">=</span> <span class="n">Intersect</span><span class="p">(</span><span class="n">ray</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">child2</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">the</span> <span class="n">closer</span> <span class="n">of</span> <span class="n">hit1</span><span class="p">,</span> <span class="n">hit2</span><span class="p">;</span> <span class="c1">// ATTENTION: closer hit point is returned!</span>
<span class="p">}</span>
</code></pre></div></div>

<p><img src="https://z3.ax1x.com/2021/01/18/scErGt.jpg" alt="cg-8-21" width="60%" /></p>

<h3 id="spatial-vs-object-partitions">Spatial vs Object Partitions</h3>

<ul>
  <li>Spatial partition (e.g. KD-Tree)
    <ul>
      <li>Partition space into non-overlapping regions</li>
      <li>Object can be contained in multiple regions</li>
    </ul>
  </li>
  <li>Object partition (e.g. BVH)
    <ul>
      <li>Partition set of objects into disjoint subsets</li>
      <li>Bounding boxes for each set may overlap in space</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/18/scEcM8.jpg" alt="cg-8-22" width="80%" /></p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://fileadmin.cs.lth.se/cs/Personal/Tomas_Akenine-Moller/code/">Triangle-Box Overlap Test, Tomas Akenine-Möller</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Selection_algorithm">Selection problem</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Whitted Style Ray Tracing, Ray-Surface Intersection, Acceleration of Ray Tracing, Ray-Intersection with AABB, KD-Tree, BVH]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅶ</title><link href="https://harrypotterrrr.github.io//2020/11/19/cg-7.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅶ" /><published>2020-11-19T00:00:00-05:00</published><updated>2020-11-19T00:00:00-05:00</updated><id>https://harrypotterrrr.github.io//2020/11/19/cg-7</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/11/19/cg-7.html"><![CDATA[<p>Geometry, Implicit &amp; Explicit Geometry, Curve, Surface, Subdivision, Mesh Simplification</p>

<!--more-->

<h2 id="geometry">Geometry</h2>

<ul>
  <li>Implicit Geometry
    <ul>
      <li>algebraic surface</li>
      <li>level sets</li>
      <li>distance functions</li>
      <li>…</li>
    </ul>
  </li>
  <li>Explicit Geometry
    <ul>
      <li>point cloud</li>
      <li>polygon mesh</li>
      <li>subdivision, NURBS</li>
      <li>…</li>
    </ul>
  </li>
  <li>Each choice best suited to a different task/type of geometry</li>
</ul>

<h3 id="implicit-representations-of-geometry">Implicit Representations of Geometry</h3>

<ul>
  <li>Based on classifying points: Points satisfy some specified relationship, without given coordinate
    <ul>
      <li>E.g. sphere: all points in 3D, where $x^2+y^2+z^2=1$</li>
      <li>More generally, $f(x, y, z) = 0$</li>
    </ul>
  </li>
</ul>

<p>Take $f(x, y, z)=\left(2-\sqrt{x^{2}+y^{2}}\right)^{2}+z^{2}-1$ as an example:</p>
<ul>
  <li>Implicit Surface sampling can be hard
    <ul>
      <li>What points lie on $f(x, y, z) = 0$</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNoff.jpg" alt="cg-7-1" width="600px" /></p>

<p>Take $f(x, y, z)=x^{2}+y^{2}+z^{2}-1$ as an example:</p>
<ul>
  <li>Implicit Surface inside/outside test is easy
    <ul>
      <li>Is $(3/4, 1/2, 1/4)$ inside the sphere?</li>
      <li>Just plug it in $f(x, y, z) = -1/8 &lt; 0$, so Inside</li>
    </ul>
  </li>
</ul>

<h4 id="implicit-geometry">Implicit Geometry</h4>

<ul>
  <li>Implicit Algebraic Surfaces</li>
  <li>Constructive solid geometry</li>
  <li>Level set methods</li>
  <li>Fractals</li>
</ul>

<h4 id="algebraic-surfaces">Algebraic Surfaces</h4>

<ul>
  <li>Surface is zero set of a polynomial in x, y, z</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNhTI.jpg" alt="cg-7-3" /></p>

<ul>
  <li>But how about using polynomial to represent a more complext object as a cow?</li>
</ul>

<h4 id="constructive-solid-geometry">Constructive Solid Geometry</h4>

<ul>
  <li><strong>Constructive Solid Geometry</strong> (<strong>CSG</strong>): Combine implicit geometry via Boolean operations</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNf0A.jpg" alt="cg-7-4" width="700px" /></p>

<ul>
  <li>Use CSG and operation to define complex geometry</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNWmd.png" alt="cg-7-5" width="400px" /></p>

<h4 id="distance-functions">Distance Functions</h4>

<ul>
  <li>
    <p><strong>Signed Distance Functions</strong> (<strong>SDF</strong>): determines the distance of a given point $x$ from the boundary of $\omega$. The function has positive values at points $x$ outside $\omega$, it decreases in value as $x$ approches the boundary of $\omega$ where the signed distance function is zero.</p>
  </li>
  <li>
    <p>Instead of Booleans, gradually blend surfaces together using Distance functions:</p>
    <ul>
      <li>Give minimum distance (could be signed distance) from anywhere to object</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNItP.jpg" alt="cg-7-6" /></p>

<ul>
  <li>An Example: Blending (linear interp.) a moving boundary
    <ul>
      <li>The above row describes the normal blending of A and B region, which results A overlaps B</li>
      <li>The below row describes the blending using SDF, similar to superposition of the electric field of two same point charges, which results the zero plane right in the middle of A and B</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNH1S.jpg" alt="cg-7-7" width="550px" /></p>

<ul>
  <li>
    <p>Blending any two distance functions: every point in the space can be represented as distance function, blending of two objects in the space is to sum up two distance functions. The boundary forms when the distance function is 0</p>
  </li>
  <li>
    <p>More details about <a href="https://www.youtube.com/watch?v=Cp5WWtMoeKg">distance function</a>, <a href="https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm">SDF code about various primitives</a> and <a href="https://www.iquilezles.org/www/index.htm">useful tutorial</a></p>
  </li>
</ul>

<h4 id="level-set-methods">Level Set Methods</h4>

<ul>
  <li>For Distance Function method, closed-form equations (解析式) are sometimes hard to describe complex shapes.</li>
  <li><strong>Level Set Methods</strong> performs numerical computations involving curves and surfaces on a fixed Cardesian grid without having to <strong>parameterize</strong> objects.
    <ul>
      <li>Level Set Methods is to store a grid of values approximating function.</li>
      <li>The central idea is same to SDF: find the boundary where the value is 0.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJN7p8.jpg" alt="cg-7-8" /></p>

<ul>
  <li>Surface is found where interpolated values equal zero</li>
  <li>
    <p>Provides much more explicit control over shape (like a texture)</p>
  </li>
  <li>Level set encodes distance to air-liquid boundary to simulate physical scene of water dropping, constant tissue density to image medical data like CT, MRI etc.</li>
</ul>

<h4 id="fractal">Fractal</h4>

<ul>
  <li>Self-similarity, detail at all scales</li>
  <li>Hard to control shape</li>
  <li>Sometimes cause the drastic aliasing due to the high frequency of variation</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNb6g.jpg" alt="cg-7-9" /></p>

<h4 id="pros--cons">Pros &amp; Cons</h4>

<ul>
  <li>Pros
    <ul>
      <li>compact description (a function), benefit for storing</li>
      <li>queries easy (inside object, distance to surface)</li>
      <li>good for ray-to-surface intersection</li>
      <li>easy to handle changes in topology (e.g. fluid)</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>difficult to model complex shapes</li>
    </ul>
  </li>
</ul>

<h3 id="explicit-representations-of-geometry">Explicit Representations of Geometry</h3>

<ul>
  <li>All points are given directly or via parameter mapping</li>
</ul>

\[f: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3} ;(u, v) \mapsto(x, y, z)\]

<ul>
  <li>Given any $(u,v)$ coordinate, $(x, y, z)$ can be mapped</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJN5kt.jpg" alt="cg-7-2" width="600px" /></p>

<p>Take $f(u, v)=((2+\cos u) \cos v,(2+\cos u) \sin v, \sin u)$ as an example</p>
<ul>
  <li>Explicit Surface sampling is easy
    <ul>
      <li>What points lie on this surface: Just plug in $(u,v)$ values</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJNoff.jpg" alt="cg-7-1" width="600px" /></p>

<ul>
  <li>Take $f(u, v)=(\cos u \sin v, \sin u \sin v, \cos v)$ as an example
    <ul>
      <li>Is $(3/4, 1/2, 1/4)$ inside the sphere?</li>
    </ul>
  </li>
  <li>No Best Representation: depends on tasks</li>
</ul>

<h4 id="explicit-geometry">Explicit Geometry</h4>

<ul>
  <li>triangle meshes</li>
  <li>Bezier surfaces</li>
  <li>subdivision surfaces</li>
  <li>NURBS</li>
  <li>point clouds</li>
</ul>

<h4 id="polygon-mesh">Polygon Mesh</h4>

<ul>
  <li>Store vertices &amp; polygons (often triangles or quads)</li>
  <li>Easier to do processing / simulation, adaptive sampling</li>
  <li>More complicated data structures</li>
  <li>Perhaps most common representation in graphics</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIepT.jpg" alt="cg-7-1" width="550px" /></p>

<ul>
  <li>The Wavefront Object File (.obj) Format
    <ul>
      <li>a text file that specifies vertices, normals, texture coordinates and their connectivities</li>
    </ul>
  </li>
</ul>

<h4 id="point-cloud">Point Cloud</h4>

<ul>
  <li>Easiest representation: list of points $(x, y, z)$</li>
  <li>Easily represent any kind of geometry</li>
  <li>Useful for large datasets (&gt;&gt; 1 point/pixel)</li>
  <li>Often converted into polygon mesh</li>
  <li>Difficult to draw in undersampled (采样不足的) regions</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIm1U.gif" alt="cg-7-10" /></p>

<h2 id="curves">Curves</h2>

<h3 id="bézier-curve">Bézier Curve</h3>

<ul>
  <li>Defining Cubic Bézier Curve With Tangent</li>
  <li>Start point and the end point is $b_0$ and $b_2$</li>
  <li>Tangent of start and end is the tangent at $b_0$ and $b_2$</li>
</ul>

<p class="warning"><strong>Note</strong>: Bézier Curve is a representation of explicit geometry since it is way of <strong>parameter mapping</strong></p>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIVhV.jpg" alt="cg-7-12" width="75%" /></p>

<h4 id="de-casteljau-algorithm">de Casteljau Algorithm</h4>

<ul>
  <li>Evaluate Bézier Curve
    <ul>
      <li>Consider three points (quadratic Bezier)</li>
      <li>Insert a point using linear interpolation</li>
      <li>Insert on both edges</li>
      <li>Repeat recursively until only one intermediate point exists</li>
      <li>Run the same algorithm for every $t$ in $[0, 1]$</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIMnJ.gif" alt="cg-7-13" width="450px" /></p>

<h4 id="algebraic-formula">Algebraic Formula</h4>

<ul>
  <li>de Casteljau Algorithm gives a pyramid of coefficients</li>
</ul>

\[\begin{aligned} \mathbf{b}_{0}^{1}(t) &amp;=(1-t) \mathbf{b}_{0}+t \mathbf{b}_{1} \\ \mathbf{b}_{1}^{1}(t) &amp;=(1-t) \mathbf{b}_{1}+t \mathbf{b}_{2} \\ \mathbf{b}_{0}^{2}(t) &amp;=(1-t) \mathbf{b}_{0}^{1}+t \mathbf{b}_{1}^{1} \\ \mathbf{b}_{0}^{2}(t) &amp;=(1-t)^{2} \mathbf{b}_{0}+2 t(1-t) \mathbf{b}_{1}+t^{2} \mathbf{b}_{2} \end{aligned}\]

<p><img src="https://z3.ax1x.com/2021/01/12/sJIEt0.jpg" alt="cg-7-14" width="85%" /></p>

<ul>
  <li>General Algebraic Formula</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIncF.jpg" alt="cg-7-15" /></p>

<ul>
  <li>Bernstein polynomials</li>
</ul>

\[B_{i}^{n}(t)=\left(\begin{array}{l}n \\ i\end{array}\right) t^{i}(1-t)^{n-i}\]

<ul>
  <li>For each $t$ moment, the sum of Bernstein Polynomials is 1. Drawing a vertical line at any $t$ moment, the sum of the y-coordinate of the intersection with the curve is 1.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sJIuX4.jpg" alt="cg-7-16" width="350px" /></p>

<ul>
  <li>assume $n=3$ in $R^3$:</li>
</ul>

\[\mathbf{b}^{n}(t)=\mathbf{b}_{0}(1-t)^{3}+\mathbf{b}_{1} 3 t(1-t)^{2}+\mathbf{b}_{2} 3 t^{2}(1-t)+\mathbf{b}_{3} t^{3}\]

<ul>
  <li>The tangent of the curve in cubic case: the slope for Bézier Curve is the derivative with respect to $t$</li>
</ul>

\[\frac{d \mathbf{b}(t)}{d t}=\mathbf{b}^{\prime}(t)=3(1-t)^{2}\left(\mathbf{b}_{1}-\mathbf{b}_{0}\right)+6(1-t) t\left(\mathbf{b}_{2}-\mathbf{b}_{1}\right)+3 t^{2}\left(\mathbf{b}_{3}-\mathbf{b}_{2}\right)\]

<h4 id="properties-of-bézier-curve">Properties of Bézier Curve</h4>

<ul>
  <li>Interpolate endpoints: Start from $B_0$, end at $B_n$</li>
  <li>Tangent to end segment: In cubic case $\mathbf{b}^{\prime}(0)=3\left(\mathbf{b}_{1}-\mathbf{b}_{0}\right)$, $\mathbf{b}^{\prime}(1)=3\left(\mathbf{b}_{3}-\mathbf{b}_{2}\right)$</li>
  <li>Transform curve by transforming control points
    <ul>
      <li>Since the Bézier Curve is the result of linear combinations</li>
      <li>But it doesn’t work for <strong>projection transformation</strong></li>
    </ul>
  </li>
  <li>Convex hull (凸包) property
    <ul>
      <li>Curve is within convex hull of control points</li>
      <li>Convex hull is a concept in computational geometry. Briefly, it can be thought of as a rubber band that fits around all the points in 2D space</li>
    </ul>
  </li>
</ul>

<h4 id="piecewise-bézier-curve">Piecewise Bézier Curve</h4>

<ul>
  <li>Higher-order (high-degree 高阶) Bézier Curve is hard to control: Instead, Piecewise (逐段的) Bézier Curve is put forward
    <ul>
      <li>Chain many low-order Bézier Curves</li>
      <li>Piecewise cubic Bézier Curve is the most common technique and widely used in Apps (Illustrator, SVG, etc.)</li>
    </ul>
  </li>
  <li>For Cubic Curve, each four pair of point defining a piece of Bézier Curve
    <ul>
      <li>The curve is continuous when each piece of curve is connected</li>
      <li>The curve is smooth when the connection of each curve is smooth
        <ul>
          <li><strong>Smooth</strong>: the derivative (slope) of the previous ending point of the curve is the same as the subsequent curve. <code class="language-plaintext highlighter-rouge">Same</code> means both <strong>direction</strong> and <strong>the amount of value</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGH7q.jpg" alt="cg-7-17" width="450px" /></p>

<ul>
  <li>Continuity
    <ul>
      <li>$C^0$ continuity: $a_n = b_0$</li>
      <li>$C^1$ continuity: $a_n = b_0 = \frac{1}{2}(a_{n-1}+b_1)$</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGqA0.jpg" alt="cg-7-18" width="550px" /></p>

<ul>
  <li><a href="http://math.hws.edu/eck/cs424/notes2013/canvas/bezier.html">Animation of Bézier Curve</a></li>
</ul>

<h3 id="spline">Spline</h3>

<ul>
  <li><strong>Spline</strong> (样条): a continuous curve constructed so as to pass through a given set of points and have a certain number of continuous derivatives
    <ul>
      <li>In short, a curve under control directly</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYG7Bn.jpg" alt="cg-7-19" width="450px" /></p>

<h3 id="b-splines">B-splines</h3>

<ul>
  <li><strong>B-spline</strong>: Short for basis splines</li>
  <li>Require more information than Bézier Curve curbes</li>
  <li>Satisfy all important properties that Bézier Curve have (i.e. superset)</li>
  <li><strong># TODO</strong>: refer to <a href="https://www.bilibili.com/video/
av66548502?from=search&amp;seid=65256805876131485">Shi-Min Hu’s course</a></li>
</ul>

<h3 id="nurbs">NURBS</h3>

<ul>
  <li><strong># TODO</strong>: refer to <a href="https://www.bilibili.com/video/
av66548502?from=search&amp;seid=65256805876131485">Shi-Min Hu’s course</a></li>
</ul>

<h2 id="surface">Surface</h2>

<h3 id="bézier-surface">Bézier Surface</h3>

<ul>
  <li>Extend Bézier curves to surfaces under control of 2D control points</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGv3F.gif" alt="cg-7-20" /></p>

<ul>
  <li>For Bicubic Bézier surface patch, evaluate surface position for parameters $(u, v)$
    <ul>
      <li>$(u, v)$ is parameters to describe two-dimensional $t$, which proves again that Bézier (parameter mapping) is explicit geometry representation</li>
      <li>Input: 4x4 control points</li>
      <li>Output: 2D surface parameterized by $(u, v)$ in $[0,1]^2$</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGTns.jpg" alt="cg-7-21" width="550px" /></p>

<ul>
  <li>Bicubic operation: similar to ‘Bicubic interpolation’</li>
  <li>Separable 1D de Casteljau Algorithm
    <ul>
      <li>Goal: evaluate surface position corresponding to $(u, v)$</li>
      <li>(u, v)-separable application of de Casteljau algorithm
        <ul>
          <li>Use de Casteljau to evaluate point $u$ on each of the 4 Bézier curves in $u$. This gives 4 control points for the ‘moving’ Bézier curve</li>
          <li>Use 1D de Casteljau to evaluate point $v$ on the ‘moving’ curve
<img src="https://z3.ax1x.com/2021/01/12/sYGLNV.jpg" alt="cg-7-22" width="600px" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGOhT.jpg" alt="cg-7-23" /></p>

<p class="info"><strong>TIP</strong>: piecewise (片段) in 2D curve vs patch (片) in 3D surface</p>

<ul>
  <li><strong>#TODO</strong>: Continuity</li>
</ul>

<h2 id="mesh-operations">Mesh Operations</h2>

<ul>
  <li>Mesh (网格) Operations: Geometry Processing
    <ul>
      <li>Mesh subdivion (细分): increase resolution (upsampling)</li>
      <li>Mesh simplification: decrease resolution (downsampling) while preserving shape/appearance</li>
      <li>Mesh regularization: modify sample distribution to improve quality</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/12/sYGj9U.jpg" alt="cg-7-24" /></p>

<h3 id="subdivision">Subdivision</h3>

<h4 id="loop-subdivision">Loop Subdivision</h4>

<ul>
  <li>Common subdivison rule for triangle meshes
    <ul>
      <li>First create more triangles (vertices)</li>
      <li>Second, tune their positions</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swSW36.jpg" alt="cg-7-25" width="600px" /></p>

<ul>
  <li>Split each triangle into four</li>
  <li>Assign new vertex positions according to weights
    <ul>
      <li>New / old vertices updated differently</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swSg41.jpg" alt="cg-7-26" width="700px" /></p>

<ul>
  <li>For new vertices: $ v = 3/8 * (A+B) + 1/8 * (C+D)$</li>
  <li>For old vertices:  \(\begin{aligned}v=(1-\beta) * original\_position + \beta * \sum^{n} neighbour\_position, \end{aligned} \\ where:\quad \beta=\left\{\begin{array}{l}\frac{3}{8 \mathrm{n}},\quad when\quad n&gt;3 \\ \frac{3}{16},\quad when\quad \mathrm{n}=3\end{array}\right.\)</li>
</ul>

<p class="info"><strong>Tip</strong>: $n$ is vertex degree. In graph theory, the degree of a vertex is the number of edges that are incident to the vertex.</p>

<p><img src="https://z3.ax1x.com/2021/01/15/swSR9x.jpg" alt="cg-7-27" width="700px" /></p>

<p class="warning"><strong>Note</strong>: Loop is family name of the founder Charles Loop of Loop Subdivision, so it has nothing to do with looping.</p>

<h4 id="catmul-clark-subdivision">Catmul-Clark Subdivision</h4>

<ul>
  <li>Catmul-Clark subdivision is more general than Loop Subdivision since Catmul-Clark Subdivision can handle mesh with not only triangles but quadrangles.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swSHUA.jpg" alt="cg-7-28" width="400px" /></p>

<ul>
  <li>Each subdivision step:
    <ul>
      <li>Keep all original vertex $d$</li>
      <li>Add vertex in each face $V_i$</li>
      <li>Add midpoint on each edge $E_i$</li>
      <li>Connect all new vertices</li>
    </ul>
  </li>
</ul>

\[\mathbf{V}_{i}=\frac{1}{n} \times \sum_{j=1}^{n} \mathbf{d}_{j}\]

\[\mathbf{E}_{i}=\frac{1}{4}\left(\mathbf{d}_{1}+\mathbf{d}_{2 i}+\mathbf{V}_{i}+\mathbf{V}_{i+1}\right)\]

\[\mathbf{d}_{i}^{\prime}=\frac{(n-3)}{n} \mathbf{d}_{i}+\frac{2}{n} \mathbf{R}+\frac{1}{n} \mathbf{S}\]

\[\mathbf{R}=\frac{1}{m} \sum_{i=1}^{m} \mathbf{E}_{i}, \mathbf{S}=\frac{1}{m} \sum_{i=1}^{m} \mathbf{V}_{i}\]

<ul>
  <li>After each subdivision
    <ul>
      <li>No extraordinatry (奇异点) vertices (degree != 4) left</li>
      <li>No non-quad face left</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swS7Ed.jpg" alt="cg-7-29" width="750px" /></p>

<h3 id="mesh-simplification">Mesh Simplification</h3>

<ul>
  <li><strong>Goal</strong>: reduce number of mesh elements while maintaining the overall shape</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swSfgK.jpg" alt="cg-7-30" width="750px" /></p>

<ul>
  <li><strong>Difficulty</strong>: For image simplification (downsampling), take Mipmap as an example, which can be performed by reducing the image resolution for several times, image pyramid (mip hierarchy) is used to represent different level of vision field. While for the geometry, it is difficult to use the similar pattern to divide the hierarchy of geometry, like how to make the number of geometric mesh change smoothly from far to near.</li>
</ul>

<h4 id="edge-collapsing">Edge Collapsing</h4>

<ul>
  <li>Suppose we simplify a mesh using edge collapsing</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swShjO.jpg" alt="cg-7-31" width="550px" /></p>

<ul>
  <li>Geometric error will be introduced by simplification .</li>
  <li>Comparing to average and median simplification, error of quadric is minimum.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/15/swS5uD.jpg" alt="cg-7-32" width="700px" /></p>

<ul>
  <li>Quadric Error Metrics (二次误差度量)
    <ul>
      <li>Not a good idea to perform local averaging or median of vertices</li>
      <li><strong>Quadric error</strong>: new vertex should minimize its <strong>sum of square distance</strong> (<strong>L2 distance</strong>) to previously related triangle planes</li>
      <li>Idea: choose point that minimizes quadric error</li>
    </ul>
  </li>
  <li>Suppose squared distance of point $p$ to plane $q$</li>
</ul>

\[p=(x, y, z, 1)^{T}, q=(a, b, c, d)^{T}\]

\[d i s t(q, p)^{2}=\left(q^{T} p\right)^{2}=p^{T}\left(q q^{T}\right) p=: p^{T} Q_{q} p\]

\[where\quad Q_{q}=\left[\begin{array}{cccc}a^{2} &amp; a b &amp; a c &amp; a d \\ a b &amp; b^{2} &amp; b c &amp; b d \\ a c &amp; b c &amp; c^{2} &amp; c d \\ a d &amp; b d &amp; c d &amp; d^{2}\end{array}\right]\]

<p class="warning"><strong>Note</strong>: $q$ is a regularized parameter of plane, where $a^2+b^2+c^2 = 1$.</p>

<ul>
  <li>Sum distances to planes $q_i$ of vertex’ neighbouring triangles</li>
</ul>

\[\sum_{i} \operatorname{dist}\left(q_{i}, p\right)^{2}=\sum_{i} p^{T} Q_{q_{i}} p=p^{T}\left(\sum_{i} Q_{q_{i}}\right) p=: p^{T} Q_{p} p\]

<ul>
  <li>Point $p^{\star}$ that minimizes the error satisfies
    <ul>
      <li>Which turn into the optimization problem</li>
    </ul>
  </li>
</ul>

\[\left[\begin{array}{cccc}
q_{11} &amp; q_{12} &amp; q_{13} &amp; q_{14} \\
q_{21} &amp; q_{22} &amp; q_{23} &amp; q_{24} \\
q_{31} &amp; q_{32} &amp; q_{33} &amp; q_{34} \\
0 &amp; 0 &amp; 0 &amp; 1 \end{array}\right] \boldsymbol{p}^{*}=\left[\begin{array}{c} 0 \\ 0 \\ 0 \\ 1 \end{array}\right]\]

<ul>
  <li><strong>Simplification via Quadric Error</strong>: assign score with quadric error metric (<em>Garland &amp; Heckbert 1997</em>)
    <ul>
      <li>Approximate distance to surface as sum of distances to planes containing triangles</li>
      <li>Assign score with quadric error metric for <strong>each edge</strong> of the total geometric figure</li>
      <li>Sort the score of <strong>each edge</strong></li>
      <li>Select the smallest score with <strong>least</strong> quadric error metric and perform edge collapsing</li>
      <li>Update the corresponding changed edges with new quadric error</li>
      <li>Iteratively execute until the simplification reaches condition</li>
    </ul>
  </li>
</ul>

<p class="warning"><strong>Note</strong>: For each step, the edge corresponding to the least quadric error is selected to be collasped. Clearly, this is a way of reaching local optimum for each step to achieve global optimum, namely greedy strategy. However strictly speaking, this approch does not prove that the global optimal solution can be reached, but this insignificant error is allowed by default in CG.</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://www.youtube.com/watch?v=Cp5WWtMoeKg">ray marching and distance function</a></li>
  <li><a href="https://www.bilibili.com/read/cv7933990?from=search">光线步进和距离函数</a></li>
  <li><a href="[SPH算法](https://blog.csdn.net/liuyunduo/article/details/84098884)">SPH算法</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics">Smoothed-particle hydrodynamics</a></li>
  <li><a href="http://acko.net/blog/making-mathbox/">Visualization of Bézier Surface</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Geometry, Implicit &amp; Explicit Geometry, Curve, Surface, Subdivision, Mesh Simplification]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅵ</title><link href="https://harrypotterrrr.github.io//2020/09/07/cg-6.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅵ" /><published>2020-09-07T00:00:00-04:00</published><updated>2020-09-07T00:00:00-04:00</updated><id>https://harrypotterrrr.github.io//2020/09/07/cg-6</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/09/07/cg-6.html"><![CDATA[<p>Texture, Texture Mapping, Interpolation, Texture Filtering</p>

<!--more-->

<h2 id="texture-mapping">Texture Mapping</h2>

<ul>
  <li><strong>Texture mapping</strong> is to apply different colors at different places</li>
  <li><strong>Texture</strong> defines property and color for each vertex</li>
</ul>

<h3 id="surface">Surface</h3>

<ul>
  <li>Surfaces are 2D though lives in 3D world space</li>
  <li>Every 3D surface point also has a place where it goes in the 2D image (<strong>texture</strong>)</li>
  <li>In other words, each vertex of primitives in 3D world corresponds to the vertex of the primitive in 2D texture.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sVCEp4.jpg" alt="cg-6-1" /></p>

<h3 id="texture">Texture</h3>

<ul>
  <li>
    <p>Texture is applied to Surface</p>
  </li>
  <li>
    <p>We didn’t care about how mapping relationship of triangles produces between model and texture. The mapping and definition of primitives (triangles) are known.</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sVCZc9.jpg" alt="cg-6-2" /></p>

<ul>
  <li>Generally, texture comes in two ways:
    <ul>
      <li>Art designer design and produce the texture</li>
      <li>Parameterization of triangular meshes</li>
    </ul>
  </li>
  <li><strong>Parameterization</strong> is the process of finding parametric equations of a curve, a surface, a manifold or a variety, defined by an implicit equation.</li>
</ul>

<h3 id="texture-coordinates">Texture Coordinates</h3>

<ul>
  <li>Each triangle vertex is assigned a texture coordinate $(u,v)$.</li>
  <li>Define mapping between points on triangle’s surface (object coordinate space) to points in texture coordinate space.</li>
  <li>Each vertex corresponds to one texture mapping. Again, this mapping is known and we don’t care how this mapping come from.</li>
</ul>

<p class="info"><strong>Tip</strong>: No matter the texture is square or not, $u$ and $v$ are in range of $(0,1)$.</p>

<p><img src="https://z3.ax1x.com/2021/01/06/sVeE90.jpg" alt="cg-6-3" /></p>

<h3 id="texture-tile">Texture Tile</h3>

<ul>
  <li>Textures can be applied multiple times</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sVZlSf.jpg" alt="cg-6-4" /></p>

<ul>
  <li>Well-designed texture is tileable when multiple textures blend together profesionally and seamlessly together, borders of texture is not easily conspicuous.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sVi2kV.png" alt="cg-6-5" /></p>

<h2 id="interpolation">Interpolation</h2>

<h3 id="interpolation-across-triangles-barycentric-coordinates">Interpolation Across Triangles: Barycentric Coordinates</h3>

<ul>
  <li>Why to interpolate
    <ul>
      <li>Specify values at vertices</li>
      <li>Obtain smoothly varying values across triangles</li>
    </ul>
  </li>
  <li>What to interpolate
    <ul>
      <li>Texture coordinates, colors, normal vectors. etc.</li>
    </ul>
  </li>
  <li>How to interpolate
    <ul>
      <li>Barycentric coordinate</li>
    </ul>
  </li>
</ul>

<h3 id="barycentric-coordinate">Barycentric Coordinate</h3>

<ul>
  <li>Any point $(x,y)$ in the plane can be represented as a linear combination of triangular vertices.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/07/sZUVNq.jpg" alt="cg-6-6" /></p>

<ul>
  <li>A coordinate system for triangles $(\alpha, \beta, \gamma)$
    <ul>
      <li>Proved by using <a href="https://zh.wikipedia.org/wiki/%E5%AE%9A%E6%AF%94%E5%88%86%E7%82%B9%E5%85%AC%E5%BC%8F">point-scored formula</a></li>
    </ul>
  </li>
</ul>

\[(x, y)= \alpha A+\beta B+\gamma C\]

\[\alpha+\beta+\gamma=1\]

\[0 \le \alpha,\ \beta,\ \gamma \le 1\]

<ul>
  <li>Point $(x,y)$ is not in the plane of $ABC$ if $\alpha+\beta+\gamma \neq1$</li>
  <li>Point $(x,y)$ is inside the triangle if and only if all three $(\alpha, \beta, \gamma)$ are non-negative.</li>
  <li>
    <p>e.g. $A$ is when $(\alpha, \beta, \gamma) = (1, 0, 0)$</p>
  </li>
  <li>Geometric viewpoint: proportional areas
    <ul>
      <li>It is available to get $(\alpha, \beta, \gamma)$ by given a spcified point $(x,y)$.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned} \alpha &amp;=\frac{A_{A}}{A_{A}+A_{B}+A_{C}} \\ \beta &amp;=\frac{A_{B}}{A_{A}+A_{B}+A_{C}} \\ \gamma &amp;=\frac{A_{C}}{A_{A}+A_{B}+A_{C}} \end{aligned}\]

<ul>
  <li>
    <p><strong>Barycentric coordinate</strong> of centroid: $(\alpha, \beta, \gamma) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$, then $(x, y)=\frac{1}{3} A+\frac{1}{3} B+\frac{1}{3} C$</p>
  </li>
  <li>
    <p>Linear interpolate values at vertices</p>
    <ul>
      <li>$V_A, V_B, V_C$ can be positions, texture coordinates, color, normal, depth, material attributes…</li>
    </ul>
  </li>
</ul>

\[V=\alpha V_{A}+\beta V_{B}+\gamma V_{C}\]

<p class="warning"><strong>Note</strong>: barycentric coordinates are not invariant under projection! Thus interpolate every time after projections.</p>

<h3 id="texture-applying">Texture Applying</h3>

<ul>
  <li>Simple texture mapping: diffuse color</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">each</span> <span class="n">rasterized</span> <span class="n">screen</span> <span class="nf">sample</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">:</span>           <span class="c1">// Usually a pixel's center</span>
    <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">=</span> <span class="n">evaluate</span> <span class="n">texture</span> <span class="n">coordinate</span> <span class="n">at</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span> <span class="c1">// Using barycentric coordinates</span>
    <span class="n">texcolor</span> <span class="o">=</span> <span class="n">texture</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">);</span>                <span class="c1">// get texture (u,v)</span>
    <span class="n">set</span> <span class="n">sample</span><span class="err">'</span><span class="n">s</span> <span class="n">color</span> <span class="n">to</span> <span class="n">texcolor</span><span class="p">;</span>                 <span class="c1">// Usually the diffuse albedo Kd of Blinn-Phong reflectance model</span>

</code></pre></div></div>

<h2 id="texture-filtering">Texture Filtering</h2>

<ul>
  <li>To determine the texture color for a texture mapped pixel, using the colors near by <strong>texels</strong>.</li>
  <li><strong>Texel</strong> (纹素): a pixel on a texture</li>
  <li>Two main categories of texture filtering, depending on the situation texture filtering
    <ul>
      <li><strong>Magnification filtering</strong>: a type of reconstruction filter where sparse data is interpolated to fill gaps</li>
      <li><strong>Minification filtering</strong>: a type of anti-aliasing (AA), where texture samples exist at a higher frequency than required for the sample frequency needed for texture fill</li>
    </ul>
  </li>
</ul>

<h3 id="texture-magnification">Texture Magnification</h3>

<ul>
  <li>Insufficient texture resolution: texture is too small comparing to the object, then the texture has to be enlarged than its actual resolution.</li>
</ul>

<p class="success"><strong>Info</strong>: For each point on the object or scene, mapping it to the corresponding point on the low resolution texture will get non-integer coordinates of texture. Rounding off is the common process to obtain non-floating coordinate texel. As a result, multiple pixels of the object or scene will corresponds to the same texel of the texture, which means the generated figure is obscure of low quality. Thus interpolation handle it.</p>

<p><img src="https://z3.ax1x.com/2021/01/07/sZN7cD.jpg" alt="cg-6-7" /></p>

<h4 id="bilinear-interpolation">Bilinear Interpolation</h4>

<ul>
  <li>Linear interpolation (1D)</li>
</ul>

\[\operatorname{lerp}\left(x, v_{0}, v_{1}\right)=v_{0}+x\left(v_{1}-v_{0}\right)\]

<ul>
  <li>Two horizontal interpolations</li>
</ul>

\[u_{0}=\operatorname{lerp}\left(s, u_{00}, u_{10}\right) \\
u_{1}=\operatorname{lerp}\left(s, u_{01}, u_{11}\right)\]

<ul>
  <li>Final vertical interpolation</li>
</ul>

\[f(x, y)=\operatorname{lerp}\left(t, u_{0}, u_{1}\right)\]

<p><img src="https://z3.ax1x.com/2021/01/07/sZNOHA.jpg" alt="cg-6-8" width="350px" /></p>

<ul>
  <li>Red point: want to sample texture value $f(x,y)$</li>
  <li>
    <p>Black points: indicate texture sample locations (the center of texel)</p>
  </li>
  <li>
    <p>For the nearest method of texture applying, take the red point as an example, any mapping point in the square which the red point and $u_{11}$ locates at will take $u_{11}$ as a texel. When the object or scene is very large comparing to the texture, multiple pixels will map to the same texel as $u_{11}$. That is the reason why jaggies and artifacts are consipicuous in the above nearest figure.</p>
  </li>
  <li>
    <p>For the bilinear interpolation, it takes 4 nearest sample locations with textrue values as labeled, blending the property and information of 4 texels, so the result is more smooth and realistic.</p>
  </li>
  <li>Bilinear interpolation usually gives pretty good results at reasonable costs.</li>
</ul>

<h4 id="bicubic-interpolation">Bicubic Interpolation</h4>

<ul>
  <li>Bicubic interpolation is to use 4 x 4 point to do interpolation, which has better performance but higher costs. (Look into the canthus of the above figure, bilinear create jaggies but bicubic is more realistic)</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/07/sZNjAI.png" alt="cg-6-9" width="650px" /></p>

<h3 id="texture-minification">Texture Minification</h3>

<ul>
  <li>Superadundant texture resolution: Texture is too large comparing to the screen space required, so it has to be shrunken relative to its natural resolution.
    <ul>
      <li>Intuitively, when the texture is large, every information is available. But it is incorrect.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/07/sehxtP.jpg" alt="cg-6-10" /></p>

<ul>
  <li>Screen Pixel’s “Footprint” in Texture
    <ul>
      <li>As the pixel in the screen space get growingly further, the number of corresponding texels in texture gets more and more.</li>
      <li>Take the above image as an example, the pixels close to the horizon represent a large region of texture. The information lost happens when the square which the blue point is inside is selected as texel</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/07/sehvkt.jpg" alt="cg-6-11" /></p>

<ul>
  <li>
    <p>Supersampling to antialiasing: yes, high quality, but costly.</p>
  </li>
  <li>The reason of aliasing when perform minification
    <ul>
      <li>When highly minified, many texels in pixel footprint</li>
      <li>Signal frequency too large in a pixel</li>
      <li>Need even higher sampling frequency</li>
    </ul>
  </li>
  <li>Solution: Not do sampling but get the average value within a range, <strong>Range Query</strong>.</li>
</ul>

<p class="success"><strong>Info</strong>: Some data structure like K-d tree, segment tree, binary indexed tree etc. are good ways to solve range query problem.</p>

<h4 id="mipmap">Mipmap</h4>

<ul>
  <li>Precompute and store the averages of the texture over various areas of different size and position.</li>
  <li>Allowing <strong>fast</strong>, <strong>approximate</strong>, <strong>square</strong> range queries.
    <ul>
      <li>“Mip” comes from the Latin, meaning a multitude in a small space.</li>
      <li>A sequence of textures that contains the same image but at lower and lower resolution. 
 <img src="https://z3.ax1x.com/2021/01/07/sehXTI.jpg" alt="cg-6-12" /></li>
      <li>
        <p>There are total of $log(n)$ images<br />
 <img src="https://z3.ax1x.com/2021/01/07/sehO0A.jpg" alt="cg-6-13" width="400px" /></p>
      </li>
      <li>The total storage overhead of a mipmap is the summation of series: $\frac{4}{3}$ to the original image.</li>
    </ul>
  </li>
</ul>

<p class="success"><strong>Info</strong>: We call this kind of structure, with images that represent the same content at a series of lower and lower sampling rates, <code class="language-plaintext highlighter-rouge">image pyramid</code> in computer vision field.</p>

<ul>
  <li>Computing Mipmap at level $D$: estimate texture footprint using texture coordinates of <strong>neighboring</strong> screen samples</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/08/su7yKx.jpg" alt="cg-6-14" /></p>

<ul>
  <li>
    <p>The distance between the pixel and neighbouring in screen space is 1</p>
  </li>
  <li>
    <p>Denote the pixel $(x,y)$ in screen space and texel $(u,v)$ in texture space</p>
  </li>
  <li>
    <p>Denote transformation $\psi$ the mapping from image sapce to texture space as a linear mapping, the transformation equation, thus $u = \psi_{x}(x, y), v = \psi_{y}(x, y)$.</p>
  </li>
  <li>
    <p><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/the-jacobian-matrix"><em>Jacobian matrix</em></a> $J$ is the best linear approximation of $\psi$ in a neighbourhood of $(u,v)$ where $\psi$ is differentiable.</p>
  </li>
</ul>

\[\lim_{\Delta p \to 0}\psi(\mathbf{p}+\Delta \mathbf{p})=\psi(\mathbf{p} )+ \mathbf{J}\Delta \mathbf{p}\]

\[\mathbf{J}=\frac{\partial (u, v)}{\partial (x, y)}=\left[\begin{array}{ll}\frac{\partial u}{\partial x} &amp; \frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial x} &amp; \frac{\partial v}{\partial y}\end{array}\right]\]

<p><img src="https://z3.ax1x.com/2021/01/08/sKmMo8.gif" alt="cg-6-15" /></p>

<ul>
  <li>
    <p>Recall that in linear transformation \(\left[\begin{array}{ll}a_{x} &amp; b_{x} \\ a_{y} &amp; b_{y}\end{array}\right]\left[\begin{array}{l}x \\ y\end{array}\right]=\left[\begin{array}{l}x^{\prime} \\ y^{\prime}\end{array}\right]\),\(\left[\begin{array}{ll}a_{x} &amp; b_{x} \\ a_{y} &amp; b_{y}\end{array}\right]\) is transformation matrix, the two columns of it \(\left[\begin{array}{l}a_{x} \\ a_{y}\end{array}\right]\) and \(\left[\begin{array}{l}a_{x} \\ a_{y}\end{array}\right]\) are two basis vector of the new space if the original basis is  \(\left[\begin{array}{l}1 \\ 0\end{array}\right]\) and \(\left[\begin{array}{l}0 \\ 1\end{array}\right]\). Thus, we could take Jacobian matrix as transformation matrix mapping the pixel $(x, y)$ in screen space to the texture space $(u, v)$.</p>
  </li>
  <li>
    <p>In screen space, suppose $(u, v)_{10}$ and $(u, v)_{01}$ as $(1, 0)$ and $(0, 1)$ relatively, the corresponding texel coordinate is computed by multiplying the transformation Jacobian matrix $J$:</p>
  </li>
</ul>

\[\left(\begin{array}{l}u^{\prime}_{10} \\ v^{\prime}_{10}\end{array}\right)=\left[\begin{array}{cc}\frac{\partial u}{\partial x} &amp; \frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial x} &amp; \frac{\partial v}{\partial y}\end{array}\right]\left(\begin{array}{l}1 \\ 0\end{array}\right)=\left(\begin{array}{c}\frac{\partial u}{\partial x} \\ \frac{\partial v}{\partial x}\end{array}\right)\]

\[\left(\begin{array}{l}u^{\prime}_{01} \\ v^{\prime}_{01}\end{array}\right)=\left[\begin{array}{cc}\frac{\partial u}{\partial x} &amp; \frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial x} &amp; \frac{\partial v}{\partial y}\end{array}\right]\left(\begin{array}{l}0 \\ 1\end{array}\right)=\left(\begin{array}{c}\frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial y}\end{array}\right)\]

<p class="info"><strong>Tip</strong>: Each column of Jacobian matrix is the new basis vector of the transformed space. <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/jacobian-prerequisite-knowledge">More details</a></p>

<p><img src="https://z3.ax1x.com/2021/01/08/sK3AKg.jpg" alt="cg-6-17" /></p>

<ul>
  <li>Choose the level $D$ so that the size covered by the texels at that level is roughly the same as the overall size of the pixel footprint.</li>
</ul>

\[D=\log _{2} L\]

\[\quad L=\max \left(\sqrt{\left(\frac{d u}{d x}\right)^{2}+\left(\frac{d v}{d x}\right)^{2}}, \sqrt{\left(\frac{d u}{d y}\right)^{2}+\left(\frac{d v}{d y}\right)^{2}}\right)\]

<p><img src="https://z3.ax1x.com/2021/01/08/su7UVU.jpg" alt="cg-6-16" /></p>

<ul>
  <li>$D$ rounded to nearest integer level
    <ul>
      <li>near (in red) corresponds to low level of texture</li>
      <li>far (in blue) corresponds to high level of texture</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/08/sKJYZQ.jpg" alt="cg-6-18" /></p>

<ul>
  <li>
    <p>But Bilinear filtering only, where $D$ is clamped to nearest level, not support floating $D$ and the level $D$ is not continuous.</p>
  </li>
  <li>
    <p><strong>Trilinear Interpolation</strong>: perform two Bilinear interpolation in neighbouring levels and interpolate the interpolated result again.</p>
  </li>
</ul>

\[u_{d}= BilinearInterpolate(p_{d})\\
u_{d+1}= BilinearInterpolate(p_{d+1})\\
f(x, y) = lerp(r, u_{d}, u_{d+1})\]

<p><img src="https://z3.ax1x.com/2021/01/08/sKs5lQ.jpg" alt="cg-6-19" width="650px" /></p>

<h4 id="isotrpic-limitation">Isotrpic Limitation</h4>

<ul>
  <li>Mipmap limitation
    <ul>
      <li>the pixel footprint might be quite different in shape from the area represented by the texel, not always approximately square</li>
      <li>Mipmap is unable to handle the pixel footprint with an elongated shape</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/08/sKs4Sg.jpg" alt="cg-6-20" /></p>

<ul>
  <li>Tilinear (<strong>isotropic</strong>) sampling will cause overblur problem most commonly when the points are on the floor that are far away viewed at very steep angles, which results in the pixel footprint covering much larger square areas.</li>
  <li>As a result, most footprints far from the viewer are averaged over the large area of the texture, which causes <strong>overblur</strong>.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/08/sKsffS.jpg" alt="cg-6-21" /></p>

<h4 id="anisotropic-filtering">Anisotropic filtering</h4>

<ul>
  <li><strong>Anisotropic filtering</strong>(Ripmap, or abbreviated <strong>AF</strong>): use multiple lookups to approximate an elongated footprint better
    <ul>
      <li>select the mipmap level based on the shortest axis of the footprint rather than the largest</li>
      <li>average together several lookups spaced along the long axis</li>
    </ul>
  </li>
  <li>Look up axis-aligned rectangular zones and preserve detail at extreme viewing angles.</li>
  <li>Comparing to isotropic filtering which consume on third of extra space, anisotropic filtering takes three times of extra space consumption.
    <ul>
      <li>Bilinear / trilinear filtering is insotropic and thus will overblur to avoid aliasing</li>
      <li>Anisotropic texture filtering provides higher image quality at higher computation and memory bandwidth cost</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/08/sKsIyj.png" alt="cg-6-22" /></p>

<p class="success"><strong>Info</strong>: N x Anisotropic filtering in video games means the original figure will be copied of reduced size up to N times along horizontal and vertial axis. No matter the N increases to 4x, 8x, 16x or more, the ceiling of the space consumption is 4 times of the original texture. Generally, as long as the graphics memory is enough, the larger anisotropy will has no influence on the computing performance.</p>

<ul>
  <li>Limitation so far: diagonal footprints still a problem</li>
  <li><strong>EWA filtering</strong> (Elliptically Weighted Average Filtering) to enhance further
    <ul>
      <li>Use multiple lookups</li>
      <li>Weighted average</li>
      <li>Mipmap hierarchy still helps</li>
      <li>Can handle irregular footprints</li>
      <li>Trade time for better performance</li>
    </ul>
  </li>
</ul>

<h3 id="short-summary">Short summary</h3>

<ul>
  <li>Texture mapping is a sampling operation and is prone to aliasing.</li>
  <li>Solution: prefilter texture map to eliminate high frequencies in texture signal.</li>
  <li>Mip-map: precompute and store multiple resampled versions of the texture image, each of which has different amounts of low-pass filtering.</li>
  <li>During rendering: dynamically select how much low-pass filtering is required based on distance between nighbouring screen samples in texture space.
    <ul>
      <li>Goal is to retain as much high-frequency content (detail) in the texture as possible, while avoiding aliasing.</li>
    </ul>
  </li>
</ul>

<h2 id="applications-of-texture">Applications of Texture</h2>

<h3 id="general-texturing">General Texturing</h3>

<ul>
  <li>Generalize texturing into many usages</li>
  <li>In modern GPUs, texture = memory + range query (filtering)
    <ul>
      <li>General method to bring data to fragment calculations</li>
    </ul>
  </li>
  <li>Many applications
    <ul>
      <li>Environment lighting</li>
      <li>Store microgeometry</li>
      <li>Procedural textures</li>
      <li>Solid modeling</li>
      <li>Volumne rendering</li>
      <li>etc.</li>
    </ul>
  </li>
  <li>Environment Map: render with the light from the envrionment as texture
    <ul>
      <li>We suppose the light is static and comes from an infinite distance. We only care about the direction of the light without any depth information.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/10/s1iK78.jpg" alt="cg-6-23" /></p>

<p class="success"><strong>Info</strong>: Classic model in CG: Utah teapot, Stanford bunny, Stanford dragon, Cornell Box</p>

<ul>
  <li><strong>Spherical Map</strong>: Sphere is used to store environment light (map). Unfolding the sphere is texture to render realistic lighting<br />
 <img src="https://z3.ax1x.com/2021/01/10/s1iu0f.jpg" alt="cg-6-24" />
    <ul>
      <li>Sphere texture is prone to <strong>distortion</strong> at top and bottom parts<br />
 <img src="https://z3.ax1x.com/2021/01/10/s1innP.jpg" alt="cg-6-25" /></li>
    </ul>
  </li>
  <li><strong>Cubic Map</strong>: A vector map to cube point along the direction. The cube is textured with 6 square texture maps<br />
<img src="https://z3.ax1x.com/2021/01/10/s1iZ6I.jpg" alt="cg-6-26" />
    <ul>
      <li>less distortion</li>
      <li>only need direction to face mapping computation</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/10/s1ieXt.jpg" alt="cg-6-27" /></p>

<h3 id="bump--normal-mapping">Bump / Normal mapping</h3>

<ul>
  <li>Previously, textures only represent colors (influence $k_d$ coefficient)</li>
  <li>In fact, textures may store height / normal or other properties</li>
  <li>Fake the detailed geometry, accordingly change the normal and <strong>affect shading</strong>
    <ul>
      <li>Change how an illuminated surface reacts to light, without modifying the size or shape of the surface</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/10/s1iltg.png" alt="cg-6-28" /></p>

<ul>
  <li>Add surface detail without adding more triangles
    <ul>
      <li>Preturb surface normal per pixel (for shading computations only)</li>
      <li>“Height shift” per texel defined by a texture</li>
    </ul>
  </li>
</ul>

<h4 id="normal-in-flatland-case">Normal in flatland case</h4>

<ul>
  <li><strong>Flatland</strong> (2D): texture in 1D and space in 2D</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/10/s1iQAS.jpg" alt="cg-6-29" /></p>

<ul>
  <li>Suppose original surface normal $\vec p = (0,1)$ directs upward</li>
  <li>Approximate derivative at $p$ is $dp = c * [h(p+1) - h(p)]$
    <ul>
      <li>$c$ is a constant coefficient to scale the influence of bump texture</li>
    </ul>
  </li>
  <li>Perturbed normal is then $\vec{n_p} = (-dp, 1).normalized()$
    <ul>
      <li>$(1, dp)$ rotate 90 degrees counterclockwise using rotate matrix and normalize</li>
    </ul>
  </li>
</ul>

<h4 id="normal-in-3d-case">Normal in 3D case</h4>

<ul>
  <li>texture in 2D and space in 3D</li>
  <li>Suppose original surface normal $\vec p = (0, 0, 1)$</li>
  <li>
    <p>Approximate derivatives at $p$ are
\(dp/du = c_1 * [h(u+1) - h(u)] \\ dp/dv = c_2 * [h(v+1) - h(v)]\)</p>
  </li>
  <li>Perturbed normal is $\vec{n_p} = (-dp/du, -dp/dv, 1).normalized()$
    <ul>
      <li>// not the cross product result of $(dp/du, 0, 1) \times (dp/dv, 0, 1)$ ?</li>
    </ul>
  </li>
</ul>

<p class="warning"><strong>Note</strong>: These are all in <strong>local coordinate</strong>. Thus, the perturbed normal need to be transformed to <strong>world coordinate</strong>.</p>

<h3 id="displacement-mapping">Displacement mapping</h3>

<ul>
  <li>Actually moves the vertices (instead of fake changing the normal only)
    <ul>
      <li>Actual geometric position of vertices over the textured are displaced</li>
    </ul>
  </li>
  <li>Uses the same texture as in bumping mapping</li>
  <li>Different with Bump / Normal mapping, Displacement mapping permits particular <strong>silhouettes</strong>, <strong>self-occlusion</strong>, <strong>self-shadowing</strong></li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/11/s3yRFs.jpg" alt="cg-6-30" width="600px" /></p>

<ul>
  <li>Require primitives in object elaborated and small enough so that the triangles in object catch up with the high frequency in displacement mapping.
    <ul>
      <li>Displacement mapping only changes the vertex properties, thus if the primitive is large and displacement mapping is not able to change the properties inside of the triangle, displacementm mapping will cause poor result.</li>
      <li>In order to render texture with delicated details, the sampling rate must be high enough.</li>
      <li>Subdivision (细分) is needed to introduce more triangles to increase sampling rate.</li>
    </ul>
  </li>
  <li>Costly computation owing to the large amount of additional geometry</li>
</ul>

<p class="success"><strong>Info</strong>: In DirectX on Windows system, <strong>dynamic tessellation</strong> (动态曲面细分) is self-adaptive techniques to tessellate object geometry in displacement mapping stage, which dynamically splits the primitives of object into smaller part of triangle to match the frequency requirements of displacement mapping.</p>

<h3 id="noise-function-texture">Noise function Texture</h3>

<ul>
  <li>3D procedural noise and solid modeling produce texture.</li>
  <li>Defining the noise function in 3D space, each point of the texture spreading over this space can be calculated by analysis formula of the noise and corresponding operations (binaryzation, linear operation).</li>
  <li><code class="language-plaintext highlighter-rouge">Perlin noise</code> is a good example of noise function to generate marble crack, mountain fluctuation etc.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/11/s3ygoj.jpg" alt="cg-6-31" width="650px" /></p>

<h3 id="precomputed-shading">Precomputed Shading</h3>

<ul>
  <li>Texture provide precomputed shading
    <ul>
      <li>Precomputed ambnient occlusion and shadow provided by the texture to save time of calculating shadow and occlusion later</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/11/s3ycwQ.jpg" alt="cg-6-32" width="700px" /></p>

<h3 id="3d-texture-and-volumn-rendering">3D Texture and Volumn Rendering</h3>

<ul>
  <li>Texture can be generalized to 3D space, storing the information and properties (far more than colors) which is to be used and processed in Shader program.</li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://www3.cs.stonybrook.edu/~gu/lectures/2020/">计算共形几何, 顾险峰，丘成桐</a></li>
  <li><a href="https://zh.wikipedia.org/wiki/%E5%AE%9A%E6%AF%94%E5%88%86%E7%82%B9%E5%85%AC%E5%BC%8F">定比分点公式</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/65495373">三角形重心坐标</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Texture_filtering">Texture filtering</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian matrix and determinant</a></li>
  <li><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/the-jacobian-matrix">Jacobian matrix, Khan Academy</a></li>
  <li><a href="https://www.cnblogs.com/bigmonkey/p/8665498.htm">雅可比行列式</a></li>
  <li><a href="https://www.cnblogs.com/bigmonkey/p/8665498.html">多变量微积分———变量替换</a></li>
  <li><a href="https://kaba.hilvi.org/homepage/cg/ewa/ewa.htm">Enhanced EWA filtering</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Texture, Texture Mapping, Interpolation, Texture Filtering]]></summary></entry><entry><title type="html">The Notes of Computer Graphics Ⅴ</title><link href="https://harrypotterrrr.github.io//2020/09/02/cg-5.html" rel="alternate" type="text/html" title="The Notes of Computer Graphics Ⅴ" /><published>2020-09-02T00:00:00-04:00</published><updated>2020-09-02T00:00:00-04:00</updated><id>https://harrypotterrrr.github.io//2020/09/02/cg-5</id><content type="html" xml:base="https://harrypotterrrr.github.io//2020/09/02/cg-5.html"><![CDATA[<p>Shading, Blinn-Phong Shading Model, Shading Frequency, Shadow Mapping, Graphics Pipeline</p>

<!--more-->

<h2 id="shading">Shading</h2>

<h3 id="what-we-done-so-far">What we done so far</h3>

<ul>
  <li>Model transformation</li>
  <li>View transformation</li>
  <li>Projection transformation</li>
  <li>Viewport transformation</li>
  <li>Rasterization</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwP9U.jpg" alt="cg-5-1" width="600px" /></p>

<ul>
  <li>The color applied to each pixel has not been determined yet.</li>
</ul>

<h3 id="blinn-phong-reflectance-model">Blinn-Phong Reflectance Model</h3>

<p><strong>Shading</strong>: The process of applying a material to an object.</p>

<ul>
  <li><strong>Specular highlight</strong>: the bright spot of light that apperas on shiny objects when illuminated. Direct light</li>
  <li><strong>Diffuse lighting</strong>: when a surface that faces an angle 90 degrees or less of the light, it will get a percentage of the light source. Direct light, but diverted from the light source</li>
  <li><strong>Ambient lighting</strong>: light of the environment, most of which comes from reflected surfaces (by diffusion). Indirect light and not in the path of light source</li>
</ul>

<p class="info"><strong>Tip</strong>: We often assume ambient lighting is a constant.</p>

<p><img src="https://z3.ax1x.com/2021/01/05/skw9hT.jpg" alt="cg-5-2" width="600px" /></p>

<h3 id="shading-point">Shading point</h3>

<ul>
  <li>Compute light reflected toward camera at a specific <strong>shading point</strong></li>
  <li>Inputs:
    <ul>
      <li>Viewer direction $v$</li>
      <li>Surface normal $n$</li>
      <li>Light direction $l$</li>
      <li>Surface parameters, properties (color, shininess, etc.)</li>
    </ul>
  </li>
</ul>

<p class="error"><strong>Caveat</strong>: Above are all <strong>unit</strong> vectors.</p>

<p><img src="https://z3.ax1x.com/2021/01/05/skwFc4.jpg" alt="cg-5-3" width="400px" /></p>

<ul>
  <li>
    <p>Shading is local, so <strong>No Shadows</strong> will be generated! (<strong>shading ≠ shadow</strong>)</p>
  </li>
  <li>
    <p>The below region in red rectangular is blocked by the object from the light source, but we still shade color to it since <strong>shading is local</strong>. The shadow of this area will be discussed <a href="">later</a></p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwi3F.jpg" alt="cg-5-4" width="400px" /></p>

<h2 id="blinn-phong-model">Blinn-Phong Model</h2>

<h3 id="diffuse-reflection">Diffuse reflection</h3>

<ul>
  <li>Light is scattered uniformly in all directions</li>
  <li>Surface color is the same for all viewing directions</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwkjJ.jpg" alt="cg-5-5" width="400px" /></p>

<ul>
  <li>Lambert’s cosine law: determine how much light is received</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwZH1.jpg" alt="cg-5-6" /></p>

<h3 id="light-falloff">Light Falloff</h3>

<ul>
  <li>Assume that ideally there is no loss of energy, for each moment of energy transmission, each spherical shell carries the same amount of luminous energy. For each unit space $dS$ on the spherical shell, we assume the energy is the same, so the for each spherical shell, the total amount of energy is $\oint IdS = 4\pi r^2 I$. Thus $I$ is inversely proportional to the square of the distance $r$.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwEu9.jpg" alt="cg-5-7" /></p>

<p class="warning"><strong>Note</strong>: Above proof is not strict, but intuitive. More strict prove is as follows:
We assume there is no energy loss, so the luminous energy $Q$ from the light source per unit time is the same. According to $\Phi = dQ / dt$, we can get the luminous flux $\Phi$ per time unit is the same. From the definition of luminance: $L_{v} = d^2\Phi / d\Omega dA cos\theta$, the total of luminance $4\pi r^2 L_{v} = d\Phi / d\Omega cos\theta$ should be the same. Thus luminance $L_{v}$ is inversely proportional to the square of  the distance $r$.</p>

<p class="error"><strong>Caveat</strong>: More detail and the relationship between Luminous intensity $l_{v}$ and Luminance $L_{v}$ will be discussed in <a href="/2020/11/25/cg-8.html#radiometry">Radiometry part</a>.</p>

<h3 id="diffuse-term">Diffuse Term</h3>

<ul>
  <li>
    <p>Lambertian Shading (Diffuse shading)</p>
  </li>
  <li>
    <p>Shading <strong>independent</strong> of view direction $v$</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skwVBR.jpg" alt="cg-5-8" /></p>

<ul>
  <li>$\max (0, \mathbf{n} \cdot \mathbf{l})$ is to ensure the shading value is always positive even if the light comes from the back of the object</li>
</ul>

<p class="success"><strong>info</strong>: The energy received is relative to the angle between normal $n$ and light $l$. This is also the reason why we feel cold in winter and hot in summer.</p>

<ul>
  <li><strong>Diffuse coefficient</strong>: often defined as 3 or 4 dimensional vector to store the percentage of energy the shading point reflects (not absorb in). Indirectly contain the material information of the object.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/skDi38.jpg" alt="cg-5-9" /></p>

<h3 id="specular-term">Specular Term</h3>

<ul>
  <li>Intensity <strong>depends</strong> on view direction
    <ul>
      <li>Specular highlight close to mirror reflection direction</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAURfS.jpg" alt="cg-5-10" width="450px" /></p>

<ul>
  <li>$v$ close to mirror direction $\Leftrightarrow$ half vector close to normal</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUhlQ.jpg" alt="cg-5-11" /></p>

<ul>
  <li>
    <p><strong>Specular coefficient</strong>: similar to <strong>diffuse coefficient</strong>, to describe the property and material of the object, but specular coefficient always set to 1, which means the shading point emmits white color when the specular reflection happens.</p>
  </li>
  <li>
    <p><strong>phong exponent</strong>: increasing phong exponent $p$ will narrow the reflection lobe and accelerate the decay rate of the specular reflection effect as the angle increased.</p>
    <ul>
      <li>When the angle between the reflected light and view direction more than 3~5 degree, we suppose specular reflection disappears.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUfSg.jpg" alt="cg-5-12" /></p>

<p><img src="https://z3.ax1x.com/2021/01/05/sAU2Y8.jpg" alt="cg-5-13" width="780px" /></p>

<p><strong>Tip</strong>: Compared to diffuse reflection, we ignore the <code class="language-plaintext highlighter-rouge">energy received term</code> for simplicity. Note that Blinn-Phong model is just an empirical model.</p>

<h3 id="ambient-term">Ambient Term</h3>

<ul>
  <li>Shading that does not depend on anything
    <ul>
      <li>Independent of light $l$ and view direction $v$</li>
      <li>Add constant color to account for disregarded illumination and fill in black shadows</li>
      <li>This is approximate / fake!</li>
    </ul>
  </li>
  <li><strong>Ambient coefficient</strong>: similar to <strong>specular coefficient</strong> and <strong>diffuse coefficient</strong>, but this is always a constant.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUgFf.jpg" alt="cg-5-14" width="750px" /></p>

<h3 id="blinn-phong-model-1">Blinn-Phong Model</h3>

<p><img src="https://z3.ax1x.com/2021/01/05/sAU5Os.jpg" alt="cg-5-15" /></p>

\[\begin{aligned} L &amp;=L_{a}+L_{d}+L_{s} \\ &amp;=k_{a} I_{a}+k_{d}\left(I / r^{2}\right) \max (0, \mathbf{n} \cdot \mathbf{l})+k_{s}\left(I / r^{2}\right) \max (0, \mathbf{n} \cdot \mathbf{h})^{p} \end{aligned}\]

<h2 id="shading-frequencies">Shading Frequencies</h2>

<ul>
  <li>Flat (Face) shading (逐片元)</li>
  <li>Gouraud (Vertex) shading (逐顶点)</li>
  <li>Phong (Pixel) shading (逐像素)</li>
</ul>

<h3 id="flat-shading">Flat shading</h3>

<ul>
  <li>Shading for each triangle</li>
  <li><code class="language-plaintext highlighter-rouge">normal vector</code> of each face is caluculated by cross product of triangle’s two edges</li>
  <li>Not good for smooth surfaces</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUomn.jpg" alt="cg-5-16" width="500px" /></p>

<h3 id="gouraud-shading">Gouraud shading</h3>

<ul>
  <li>Shading for each vertices and <strong>interpolate</strong> triangle from vertices</li>
  <li>Each vertex has a normal vector</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAU7T0.jpg" alt="cg-5-17" width="500px" /></p>

<ul>
  <li>It is easy to obtain vertex normal from the underlying known geometry. e.g. sphere</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAaOEt.jpg" alt="cg-5-20" width="700px" /></p>

<ul>
  <li>Otherwise vertex normal can be inferred from surrounding triangle:
    <ul>
      <li>average surrounding face normals</li>
      <li>weighted average surrounding face normals according to the area of each triangle</li>
    </ul>
  </li>
</ul>

\[N_{v}=\frac{\sum_{i} N_{i}}{\left\|\sum_{i} N_{i}\right\|}\]

<h3 id="phong-shading">Phong shading</h3>

<ul>
  <li>Shading for each pixels across triangle</li>
  <li><code class="language-plaintext highlighter-rouge">normal vector</code> of each pixels is interpolated by vertex vector</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUTwq.jpg" alt="cg-5-18" width="500px" /></p>

<p class="info"><strong>Tip</strong>: <code class="language-plaintext highlighter-rouge">Phong shading</code> is distinct from <code class="language-plaintext highlighter-rouge">Blinn-Phong reflectance model</code></p>

<ul>
  <li>pixel vector is computed by Barycentric interpolation of vertex normal</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUXpF.jpg" alt="cg-5-22" width="650px" /></p>

<p class="error"><strong>Caveat</strong>: remember to normalize each normal vector after each step of interpolation.</p>

<h3 id="shading-difference">Shading difference</h3>

<ul>
  <li>As the model becomes more complex and has more vertices, the difference between three shading models become smaller</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/05/sAUqYT.jpg" alt="cg-5-19" width="750px" /></p>

<h2 id="shadow-mapping">Shadow Mapping</h2>

<ul>
  <li>Draw shadows using rasterization</li>
  <li>
    <p>Shadow gives people the feeling of objects in contact with each other</p>
  </li>
  <li>An Image-space Algorithm
    <ul>
      <li>no knowledge of scene’s geometry required during shadow computation</li>
      <li>must deal with aliasing artifaces</li>
    </ul>
  </li>
  <li>Key idea: the points NOT in shadow must be seen both by the light and by the camera</li>
</ul>

<p class="warning"><strong>Note</strong>: Suppose the point light is the only source of light, which means the shadow of the object is 0 or 1, that is so-called <strong>hard shadow</strong>.</p>

<ul>
  <li><strong>Step 1</strong>: Assume the camera is at the point light source, perform depth test to record the depth information on the buffer.</li>
  <li><strong>Step 2</strong>: From the view of the real camera, project all point in the view back to the point source light. If the the distance from the point to the light matches the depth, the point is visible, otherwise the point is blocked by other objects.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBaJCF.jpg" alt="cg-5-25" width="80%" /></p>

<h3 id="visualizing-shadow-mapping">Visualizing Shadow Mapping</h3>

<ul>
  <li>A complex scene with shadows and without shadows</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBa68e.jpg" alt="cg-5-26" width="80%" /></p>

<ul>
  <li>The depth buffer from the light’s point of view</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBa3NT.jpg" alt="cg-5-27" width="80%" /></p>

<ul>
  <li>The shadow from the camera’s point of view
    <ul>
      <li>Green is where the distance from the light to shading point approximates to depth on buffer</li>
      <li>Non-green is where shadows should be</li>
      <li>Quality of shadow mapping is pretty bad due to the error caused by equality comparison of floating value</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBa1EV.jpg" alt="cg-5-28" width="80%" /></p>

<h3 id="problems-with-shadow-mapping">Problems with Shadow Mapping</h3>

<ul>
  <li>Only hard shadows in point light source can be mapped
    <ul>
      <li>If the light source is not limited to one and no more point-like, the shadow will be not hard.</li>
    </ul>
  </li>
  <li>Quality greatly depends on shadow map resolution, which is general problem with image-based rasterization techniques</li>
</ul>

<p class="success"><strong>Info</strong>: In many games settings, the <strong>shadow quality</strong> refers to the resolution of shadow map. The higher shadow quality sets, the better and more realistic the shadow will be mapped, and the higher it will cost.</p>

<ul>
  <li>Errors involves equality comparison of floating point
    <ul>
      <li>Sometimes use <code class="language-plaintext highlighter-rouge">bias</code> or <code class="language-plaintext highlighter-rouge">eps</code> to approximate the equality between distance and depth in buffer</li>
    </ul>
  </li>
</ul>

<h3 id="hard-vs-soft-shadow">Hard vs. Soft Shadow</h3>

<ul>
  <li>Soft shadow is caused by the <strong>size</strong> of light source.
    <ul>
      <li>The light is not able to reach anywhere in <strong>Umbra region</strong>.</li>
      <li>Part of light is blocked but still some light can illuminate <strong>Penumbra</strong> region.</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBaQH0.jpg" alt="cg-5-29" width="65%" /></p>

<ul>
  <li>Soft shadow is the transition from the umbra to penumbra to the illuminated region.
    <ul>
      <li>The shadow corresponding to the edge of the object is gradually vague and soft due to the light illumination.</li>
    </ul>
  </li>
  <li>Soft shadow is more realistic and natural because light source is not always point-like.</li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/16/sBaY34.jpg" alt="cg-5-30" width="80%" /></p>

<h2 id="graphics-pipeline">Graphics Pipeline</h2>

<h3 id="real-time-rendering-pipeline">Real-time Rendering pipeline</h3>

<p><img src="https://z3.ax1x.com/2021/01/06/sEh9XR.jpg" alt="cg-5-22" /></p>

<ul>
  <li>
    <p>According to different shading frequency (face, vertex, pixel), Shading happens in Vertex Processing or Fragment Processing.</p>
  </li>
  <li>
    <p>Texture mapping also happens in Vertex Processing and Fragment Processing.</p>
  </li>
  <li>
    <p>Texture mapping is to assign vertices with different colors (properties, materials). This is the reason why objects look different in color and material after rendering. Detail in <a href="/2020/09/07/cg-6">Texture</a> later.</p>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sEhPn1.png" alt="cg-5-23" /></p>

<p class="warning"><strong>Note</strong>: Fragment (片元) is widely used in OpenGL and other modern API, which commonly means Pixel. Fragment shading (processing) = Pixel shading (processing).</p>

<h3 id="shader-programs">Shader Programs</h3>

<ul>
  <li>Modern GPU allows to custom various shader by writing <strong>shader program</strong>
    <ul>
      <li>Program vertex and fragment processing stages</li>
      <li>Describe operation on a single vertex or fragment. Shader function executes once per fragment, thus there is no need to loop or traverse each vertex or fragment</li>
      <li>Outputs color of surface at the current fragment’s screen sample position</li>
    </ul>
  </li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Example GLSL fragment shader program</span>
<span class="n">uniform</span> <span class="n">sampler2D</span> <span class="n">myTexture</span><span class="p">;</span>        <span class="c1">// texture property</span>
<span class="n">uniform</span> <span class="n">vec3</span> <span class="n">lightDir</span><span class="p">;</span>              <span class="c1">// inversed light direction vector</span>
<span class="n">varying</span> <span class="n">vec2</span> <span class="n">uv</span><span class="p">;</span>                    <span class="c1">// perfragment value (interp. by rasterizer)</span>
<span class="n">varying</span> <span class="n">vec3</span> <span class="n">norm</span><span class="p">;</span>                  <span class="c1">// norm vector, perfragment value (interp. by rasterizer)</span>

<span class="kt">void</span> <span class="nf">diffuseShader</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">vec3</span> <span class="n">kd</span><span class="p">;</span>                        <span class="c1">// vector3d to store color value</span>
    <span class="n">kd</span> <span class="o">=</span> <span class="n">texture2d</span><span class="p">(</span><span class="n">myTexture</span><span class="p">,</span> <span class="n">uv</span><span class="p">);</span>  <span class="c1">// material color and property from texture</span>
    <span class="n">kd</span> <span class="o">*=</span> <span class="n">clamp</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="err">–</span><span class="n">lightDir</span><span class="p">,</span> <span class="n">norm</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">);</span> <span class="c1">// diffuse shading</span>
    <span class="n">gl_FragColor</span> <span class="o">=</span> <span class="n">vec4</span><span class="p">(</span><span class="n">kd</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">);</span>   <span class="c1">// assign fragment color value to gl_FragColor</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>This shader performs a texture lookup to obtain the surface’s material color at this point</li>
  <li>Then performs a diffuse lighting calculation</li>
  <li>Vertex shader (顶点着色器) programs to each vertex, fragment shader （片元/像素着色器) programs to each pixel</li>
</ul>

<p><strong>Info</strong>: Incredible shader program and website: shadertoy.com</p>

<h3 id="highly-complex-3d-scenes-in-realtime">Highly Complex 3D Scenes in Realtime</h3>

<ul>
  <li>Modern GPU could handle much complex 3D scenes in realtime by deploying great amount of computation in parallel.
    <ul>
      <li>thousands to millions of triangles in a scene</li>
      <li>Complex vertex and fragment shader computations</li>
      <li>High resolution (2-4 megapixel and supersampling)</li>
      <li>30-60 fps (frames per second) and even higher for VR</li>
    </ul>
  </li>
  <li><strong>Game engine</strong> (architecture) is designed for developers to focus more on constructing games instead of techniques of graphics or rendering. The core functionality typically by a game engine includes a rendering engine (renderer) for 2D or 3D graphics (including shadows, global illumination etc.), a physics engine including collision detection and response, animation, artifical intelligence, sounding, memory management, threading, precomputation etc.</li>
</ul>

<h3 id="gpu-graphics-pipeline-implementation">GPU: Graphics Pipeline Implementation</h3>

<ul>
  <li>Specialized processors for executing graphics pipeline computations</li>
  <li>Heterogeneous, Multi-core Processor</li>
  <li>Emerge different new shaders:
    <ul>
      <li>Geometry shader: govern the processing of primitives and produce more triangles</li>
      <li>Compute shader: more general purpose for different computing</li>
    </ul>
  </li>
</ul>

<p><img src="https://z3.ax1x.com/2021/01/06/sVF90I.jpg" alt="cg-5-24" /></p>

<ul>
  <li>FLOPS (floating point operatins per second): measure of computer performance:</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Prefix</th>
      <th style="text-align: center">Abbreviation</th>
      <th style="text-align: center">Order of magnitude</th>
      <th style="text-align: center">Computer performance</th>
      <th style="text-align: center">Storage capacity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">mega-</td>
      <td style="text-align: center">M</td>
      <td style="text-align: center">$10^6$</td>
      <td style="text-align: center">megaFLOPS (MFLOPS)</td>
      <td style="text-align: center">megabyte (MB)</td>
    </tr>
    <tr>
      <td style="text-align: center">giga-</td>
      <td style="text-align: center">G</td>
      <td style="text-align: center">$10^9$</td>
      <td style="text-align: center">gigaFLOPS (GFLOPS)</td>
      <td style="text-align: center">gigabyte (GB)</td>
    </tr>
    <tr>
      <td style="text-align: center">tera-</td>
      <td style="text-align: center">T</td>
      <td style="text-align: center">$10^{12}$</td>
      <td style="text-align: center">teraFLOPS (TFLOPS)</td>
      <td style="text-align: center">terabyte (TB)</td>
    </tr>
    <tr>
      <td style="text-align: center">peta-</td>
      <td style="text-align: center">P</td>
      <td style="text-align: center">$10^{15}$</td>
      <td style="text-align: center">petaFLOPS (PFLOPS)</td>
      <td style="text-align: center">petabyte (PB)</td>
    </tr>
    <tr>
      <td style="text-align: center">exa-</td>
      <td style="text-align: center">E</td>
      <td style="text-align: center">$10^{18}$</td>
      <td style="text-align: center">exaFLOPS (EFLOPS)</td>
      <td style="text-align: center">exabyte (EB)</td>
    </tr>
    <tr>
      <td style="text-align: center">zetta-</td>
      <td style="text-align: center">Z</td>
      <td style="text-align: center">$10^{21}$</td>
      <td style="text-align: center">zettaFLOPS (ZFLOPS)</td>
      <td style="text-align: center">zettabyte (ZB)</td>
    </tr>
    <tr>
      <td style="text-align: center">yotta-</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">$10^{24}$</td>
      <td style="text-align: center">yottaFLOPS (YFLOPS)</td>
      <td style="text-align: center">yottabyte (YB)</td>
    </tr>
  </tbody>
</table>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html">GAMES101, Lingqi Yan</a></li>
  <li><a href="https://www.zhihu.com/question/53080536/answer/133398317">光能、光通量、光强、亮度</a></li>
</ul>]]></content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="computer graphics" /><category term="notes" /><summary type="html"><![CDATA[Shading, Blinn-Phong Shading Model, Shading Frequency, Shadow Mapping, Graphics Pipeline]]></summary></entry></feed>