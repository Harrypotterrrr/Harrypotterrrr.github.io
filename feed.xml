<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-08-26T21:22:11+08:00</updated><id>/feed.xml</id><title type="html">Haolin Jia’s Homepage</title><subtitle>Your Site Description23333
</subtitle><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><entry><title type="html">Vim cookbook</title><link href="/2019/08/03/Vim-cookbook.html" rel="alternate" type="text/html" title="Vim cookbook" /><published>2019-08-03T00:00:00+08:00</published><updated>2019-08-03T00:00:00+08:00</updated><id>/2019/08/03/Vim-cookbook</id><content type="html" xml:base="/2019/08/03/Vim-cookbook.html">&lt;p&gt;This is a cheatsheet for vim commands and operations.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;h3 id=&quot;display-line-numbers&quot;&gt;Display line numbers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;in the current file&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:set number &lt;span class=&quot;c&quot;&gt;# to enable line numbers&lt;/span&gt;
:set nonumber &lt;span class=&quot;c&quot;&gt;# to disable line numbers&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;to change config&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set number&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.vimrc&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-mouse-mode-on&quot;&gt;set mouse mode on&lt;/h3&gt;

&lt;p&gt;add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set mouse=a&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.vimrc&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-tab-size&quot;&gt;set tab size&lt;/h3&gt;

&lt;p&gt;add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set tabstop=4&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;navigating&quot;&gt;Navigating&lt;/h2&gt;

&lt;h3 id=&quot;basic-movement&quot;&gt;Basic movement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt; move to the left by one position&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; move to the right by one position&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; move to the downward direction by one line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; move to the upward direction by one line&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inline-movement&quot;&gt;Inline movement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; move to the start of the current word&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;e&lt;/code&gt; move to the end of the current word&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt; move to the start of the next word&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine movement with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3w&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9k&lt;/code&gt; is the same as pressing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt; three times, pressing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; nine times, respectively.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;^&lt;/code&gt; move to the first non-blank character&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g_&lt;/code&gt; move to the last non-blank character&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n + Space&lt;/code&gt; move to the nth character of the current line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; move to the &lt;strong&gt;start column&lt;/strong&gt; of the current line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$&lt;/code&gt; move to the &lt;strong&gt;end column&lt;/strong&gt; of the current line&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-movement&quot;&gt;Text movement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; move to the first non-blank character of the next line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-&lt;/code&gt; move to the first non-blank character of the previous line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:n&lt;/code&gt; jump to the nth line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:+n&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n + Enter&lt;/code&gt; jump down n lines&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:-n&lt;/code&gt; jump up n lines&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gg&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:0&lt;/code&gt; move to the &lt;strong&gt;first line&lt;/strong&gt; of the file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:$&lt;/code&gt; move to the &lt;strong&gt;end line&lt;/strong&gt; of the file&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine movement with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3G&lt;/code&gt; move to the third line of the file&lt;/p&gt;

&lt;h3 id=&quot;screen-movement&quot;&gt;screen movement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + e&lt;/code&gt; scroll down a line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + y&lt;/code&gt; scroll up a line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + d&lt;/code&gt; scroll down the half of page&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + u&lt;/code&gt; scroll up the half page&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + f&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Page down&lt;/code&gt; scroll down the entire page&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + b&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Page up&lt;/code&gt; scroll up the entire page&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + o&lt;/code&gt; jump to the previous position&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + i&lt;/code&gt; jump to the next position&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;editing&quot;&gt;Editing&lt;/h2&gt;

&lt;h3 id=&quot;insert&quot;&gt;Insert&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; insert the text before the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I&lt;/code&gt; insert the text at the beginning of the line&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;open-a-new-line&quot;&gt;Open a new line&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;o&lt;/code&gt; open new line below the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt; open new line above the cursor&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;append&quot;&gt;Append&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; append the text after the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; append the text at the end of line&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;delete&quot;&gt;Delete&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; delete the character on the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; delete the character before the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dw&lt;/code&gt; delete a word beginning at the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d0&lt;/code&gt; delete from the beginning of current line to the cursor position (including current character)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d$&lt;/code&gt; delete from current position to the end of the line (including current character)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt; delete entire line beginning at the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dd&lt;/code&gt; delete entire line，&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d1G&lt;/code&gt; delete from the first line to the current line (including current line)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dG&lt;/code&gt; delete from the current line to the end (including current line)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine movement with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3dw&lt;/code&gt; delete 3 words at a time, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4D&lt;/code&gt; delete 4 lines at a time&lt;/p&gt;

&lt;p class=&quot;notice--warning&quot;&gt;&lt;strong&gt;Mention:&lt;/strong&gt; all of these commands is will ‘cut’ the text to the clipboard, so the following &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; will works!&lt;/p&gt;

&lt;h3 id=&quot;delete-and-insert&quot;&gt;Delete and insert&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; delete the character under the cursor and switch to insert mode&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&lt;/code&gt; delete the whole line and switch to insert mode&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; delete the following text after the cursor and switch to insert mode&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;replace&quot;&gt;Replace&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r&lt;/code&gt; replace the character on the cursor but not switch to insert mode&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; replace the following text&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;copy--paste&quot;&gt;Copy &amp;amp; Paste&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; copy a single character on the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yy&lt;/code&gt; copy entire line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; paste text after the cursor&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; paste text before the cursor&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine movement with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3yy&lt;/code&gt; copy the following three lines including the current line&lt;/p&gt;

&lt;h3 id=&quot;join-lines&quot;&gt;Join lines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;J&lt;/code&gt; remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;line breaks&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whitespace&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine inserting with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;5igo&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; wil insert ‘go’ five times&lt;/p&gt;

&lt;h3 id=&quot;undo--redo&quot;&gt;Undo &amp;amp; Redo&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt; undo single action&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Info:&lt;/strong&gt; combine inserting with a number, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3u&lt;/code&gt; will undo action five times&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + r&lt;/code&gt; redo single action&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;searching&quot;&gt;Searching&lt;/h2&gt;

&lt;h3 id=&quot;inline-searching&quot;&gt;Inline searching&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f&amp;lt;char&amp;gt;&lt;/code&gt; jump to the next occurrence of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;char&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&amp;lt;char&amp;gt;&lt;/code&gt; jump before the next occurrence of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;char&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;notice--success&quot;&gt;&lt;strong&gt;e.g. delete until specified char:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dt&quot;&lt;/code&gt; delete text until &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;global-searching&quot;&gt;Global searching&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; search the next occurrence of the word cursor on&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; search the previous occurrence of the word cursor on&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&amp;lt;expression&amp;gt;&lt;/code&gt; search the expression in forward direction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&amp;lt;expression&amp;gt;&lt;/code&gt; search the expression in backward direction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; find the next/previous occurence in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&amp;lt;expression&amp;gt;&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&amp;lt;expression&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; find the previous/next occurence in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&amp;lt;expression&amp;gt;&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&amp;lt;expression&amp;gt;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;//&lt;/code&gt; repeat the previous searching&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advance&quot;&gt;Advance&lt;/h2&gt;

&lt;h3 id=&quot;replace-1&quot;&gt;Replace&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&amp;lt;line1&amp;gt;,&amp;lt;line2&amp;gt;s/&amp;lt;word1&amp;gt;/&amp;lt;word2&amp;gt;/g&lt;/code&gt; replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word1&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word2&lt;/code&gt; between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;line1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;line2&lt;/code&gt;, flag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt; refers to global&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:%s/&amp;lt;word1&amp;gt;/&amp;lt;word2&amp;gt;/gc&lt;/code&gt; replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word1&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word2&lt;/code&gt; to the whole text, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&lt;/code&gt; means for every line, flag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gc&lt;/code&gt; refers to global and need confirmation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ddp&lt;/code&gt; swap the current line to the next one&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visual-mode&quot;&gt;Visual mode&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt; switch to visual mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;notice--success&quot;&gt;&lt;strong&gt;e.g. delete the whole word:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt; for visual mode, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;e&lt;/code&gt; jump to the end of the current word, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt; to delete&lt;/p&gt;

&lt;h3 id=&quot;visual-block-mode&quot;&gt;Visual block mode&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + v&lt;/code&gt; switch to visual block mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;notice--success&quot;&gt;&lt;strong&gt;e.g. Comment quickly:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl + v&lt;/code&gt; for block mode, select some lines, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I&lt;/code&gt; to insect at the beginning of each line, input the annotation symbol, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; twice finally comment several lines quickly&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:10,20s#^#//#g&lt;/code&gt; comment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;//&lt;/code&gt; from line 10 to line 20&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:10,20s#^//##g&lt;/code&gt; uncomment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;//&lt;/code&gt; from line 10 to line 20&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:10,20s/^/#/g&lt;/code&gt; comment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; from line 10 to line 20&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:10,20s/^#//g&lt;/code&gt; uncomment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; from line 10 to line 20&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:%s/$/\r/g&lt;/code&gt; add a newline to each line&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;others&quot;&gt;Others&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:! &amp;lt;command&amp;gt;&lt;/code&gt; run the command and show the output outside of vim environment&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="cookbook" /><summary type="html">This is a cheatsheet for vim commands and operations.</summary></entry><entry><title type="html">Object-driven Text-to-Image Synthesis via Adversarial Training</title><link href="/2019/07/16/ObjGAN.html" rel="alternate" type="text/html" title="Object-driven Text-to-Image Synthesis via Adversarial Training" /><published>2019-07-16T00:00:00+08:00</published><updated>2019-07-16T00:00:00+08:00</updated><id>/2019/07/16/ObjGAN</id><content type="html" xml:base="/2019/07/16/ObjGAN.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1902.10740&quot;&gt;&lt;em&gt;Object-driven Text-to-Image Synthesis via Adversarial Training&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This paper proposed an obj-GAN which has an object-driven attentive image generator and a new Fast R-CNN based object-wise discriminator. The object-driven generator is able to pay attention to the most relevant words and semantic layout to synthesize salient objects. The object-wise discriminator is to ensure the synthesized object matches the text description and the pre-generated layout.&lt;/p&gt;

&lt;p&gt;Compared to the previous work, which do not specifically model the relations between objects, and thus struggle to generate complex scenes except for simple datasets, such as birds and flowers, this paper first constructs a semantic layout from the text and then synthesze the image by a deconvolutional generator.&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Object-driven Text-to-Image Synthesis via Adversarial Training</summary></entry><entry><title type="html">StoryGAN: A Sequential Conditional GAN for Story Visualization</title><link href="/2019/07/14/storyGAN.html" rel="alternate" type="text/html" title="StoryGAN: A Sequential Conditional GAN for Story Visualization" /><published>2019-07-14T00:00:00+08:00</published><updated>2019-07-14T00:00:00+08:00</updated><id>/2019/07/14/storyGAN</id><content type="html" xml:base="/2019/07/14/storyGAN.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1812.02784&quot;&gt;&lt;em&gt;StoryGAN: A Sequential Conditional GAN for Story Visualization&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Given a paragraph consisting of multiple sentences, this paper proposed a &lt;em&gt;sequential conditional GAN framework&lt;/em&gt; to generate a sequence of images, one for each sentence, to depict the total story.&lt;/p&gt;

&lt;p&gt;Different from the video generation, story visualization mainly focuses less on the continuity of motions in frames, such as a running man move dynamically which is smoothly represented in the clip, but more on global consistency across scenes and characters.&lt;/p&gt;

&lt;h2 id=&quot;main-challenge&quot;&gt;Main challenge&lt;/h2&gt;

&lt;p&gt;There are two main challenges for story visualization:&lt;/p&gt;

&lt;p&gt;One is that the sequence of images should be coherently describe the total story. In other words, if the model is highly based on the previous strategy where each generation is solely based on the single sentence, the final result will be inconsistent. For instance, ‘A rectangular is at the center. Then add a round at the right’. In this case, the result will fail to represent the relationship between two object.&lt;/p&gt;

&lt;p&gt;Another challenge is how to keep the appearance and layout of objects and background in a coherent way as the story progresses. For example, Harry and the surrounding around him appeared in the paragraph description should be uniform in some way.&lt;/p&gt;

&lt;h2 id=&quot;difference-between-video-generation-and-story-visualization&quot;&gt;Difference between video generation and story visualization&lt;/h2&gt;

&lt;p&gt;For video generation, models focus on extract dynamic feature to maintain motions, like the man is running. It pay attention to the body movement as time goes by, while story visualization is devoted to a sequence of key static frames representing the whole plots, like this man runs from the garden with many flowers to the bus station and inquiry the people around him. From this point of view, motion features of story visualization are less important while scene changes with variation of multiple objects should be captured, comparing to the relatively static background in video clips.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;The architecture is relatively easy to understand, as shown below. It is composed of &lt;em&gt;Story Encoder&lt;/em&gt;, &lt;em&gt;Context Enocder&lt;/em&gt;, &lt;em&gt;Image Generator&lt;/em&gt;, &lt;em&gt;Image Discriminator&lt;/em&gt; and &lt;em&gt;Story Discriminator&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/storyGAN/net_arch.jpg&quot; alt=&quot;net_arch&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;story-encoder&quot;&gt;Story encoder&lt;/h3&gt;

&lt;p&gt;Story encoder is to map the story description into the low dimensional vector, story context $h_{0}$, which contains all information of the story.&lt;/p&gt;

&lt;p&gt;Specifically, … &lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To smooth the representation manifold in latent semantic space, KL divergence is imported.&lt;/p&gt;

\[\mathcal{L}_{K L}=K L\left(\mathcal{N}\left(\boldsymbol{\mu}(\boldsymbol{S}), \operatorname{diag}\left(\boldsymbol{\sigma}^{2}(\boldsymbol{S})\right)\right) \| \mathcal{N}(\mathbf{0}, \boldsymbol{I})\right)\]

&lt;p&gt;where $S$ is the encoded feature vecotrs for the paragraph, different with the notation in the image for the convenience of interpretation.&lt;/p&gt;

&lt;h3 id=&quot;context-encoder&quot;&gt;Context encoder&lt;/h3&gt;

&lt;p&gt;A deep RNN based Context encoder is imported to capture contextual information during sequential generation. It contains two parts, standard GRU cells and Text2Gist cells.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/storyGAN/context_encoder.jpg&quot; alt=&quot;context_encoder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each time $t$, GRU takes as input sentence $s_{t}$ and noise $\epsilon_{t}$, and outputs $i_{t}$ which contains the local information of each sentence.&lt;/p&gt;

&lt;p&gt;Then Text2Gist module combines $i_{t}$ with the story context $h_{t}$ to generate so-called &lt;em&gt;‘Gist’ vector&lt;/em&gt; $o_{t}$ which contains both global and local context information.&lt;/p&gt;

\[\begin{aligned} \boldsymbol{i}_{t}, \boldsymbol{g}_{t} &amp;amp;=\operatorname{GRU}\left(\boldsymbol{s}_{t}, \boldsymbol{\epsilon}_{t}, \boldsymbol{g}_{t-1}\right) \\ \boldsymbol{o}_{t}, \boldsymbol{h}_{t} &amp;amp;=\operatorname{Text} 2 \operatorname{Gist}\left(\boldsymbol{i}_{t}, \boldsymbol{h}_{t-1}\right) \end{aligned}\]

&lt;p&gt;To be explicit for the Text2Gist, as shown below:&lt;/p&gt;

\[\begin{aligned} \boldsymbol{z}_{t} &amp;amp;=\sigma_{z}\left(\boldsymbol{W}_{z} \boldsymbol{i}_{t}+\boldsymbol{U}_{t} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{z}\right) \\ \boldsymbol{r}_{t} &amp;amp;=\sigma_{r}\left(\boldsymbol{W}_{r} \boldsymbol{i}_{t}+\boldsymbol{U}_{r} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{r}\right) \\ \boldsymbol{h}_{t} &amp;amp;=\left(\mathbf{1}-\boldsymbol{z}_{t}\right) \odot \boldsymbol{h}_{t-1} +\boldsymbol{z}_{t} \odot \sigma_{h}\left(\boldsymbol{W}_{h} \boldsymbol{i}_{t}+\boldsymbol{U}_{h}\left(\boldsymbol{r}_{t} \odot \boldsymbol{h}_{t-1}\right)+\boldsymbol{b}_{h}\right) \\ \boldsymbol{o}_{t} &amp;amp;=\text { Filter }\left(\boldsymbol{i}_{t}\right) * \boldsymbol{h}_{t} \end{aligned}\]

&lt;p&gt;$z_{t}$ and $r_{t}$ are the update and reset gates, respectively. The update gate decide how much information from the previous step should be kept, while the reset gate determines what to forget from $h_{t-1}$.&lt;/p&gt;

&lt;p&gt;$Filter\left(\boldsymbol{i}_{t}\right)$ is used to choose the necessary part for image generation. &lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;discriminator&quot;&gt;Discriminator&lt;/h3&gt;

&lt;p&gt;Image and Story discriminator is introduced to ensure the local and global consistency of the story, respectively.&lt;/p&gt;

&lt;p&gt;Image discriminator measures whether the generated image $\hat{\boldsymbol{x}}_{t}$ matches the sentecne $s_{t}$ by comparing triplet $\left\{s_{t}, h_{0}, \hat{x}_{t}\right\}$ to the real $\left\{s_{t}, h_{0}, x_{t}\right\}$&lt;/p&gt;

&lt;p&gt;Story discriminator enforce the global consistency of image sequence given story $S$. It differs from video generation using 3D convolution to smooth the changes between frames. The framework is shown as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/storyGAN/story_dis.jpg&quot; alt=&quot;story_dis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each side of image or text descriptions are firstly embedded and concatenated separately in to two single vectors, shown as the blue and red one.&lt;/p&gt;

\[D_{S}=\sigma\left(\boldsymbol{w}^{\top}\left(E_{i m g}(\boldsymbol{X}) \odot E_{t x t}(\boldsymbol{S})\right)+b\right)\]

&lt;p&gt;where $\odot$ is element-wise product.&lt;/p&gt;

&lt;h2 id=&quot;training-strategy&quot;&gt;Training strategy&lt;/h2&gt;

&lt;p&gt;The objective for StoryGAN is:&lt;/p&gt;

\[\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\psi}_{I}, \boldsymbol{\psi}_{S}} \alpha \mathcal{L}_{\text {Image}}+\beta \mathcal{L}_{\text {Story}}+\mathcal{L}_{K L}\]

\[\begin{aligned}
\mathcal{L}_{\text {Image}} &amp;amp;=\sum_{t=1}^{T}\left(\mathbb{E}_{\left(\boldsymbol{x}_{t}, \boldsymbol{s}_{t}\right)}\left[\log D_{I}\left(\boldsymbol{x}_{t}, \boldsymbol{s}_{t}, \boldsymbol{h}_{0} ; \boldsymbol{\psi}_{I}\right)\right]\right.+\mathbb{E}_{\left(\boldsymbol{\epsilon}_{t}, \boldsymbol{s}_{t}\right)}\left[\log \left(1-D_{I}\left(G\left(\boldsymbol{\epsilon}_{t}, \boldsymbol{s}_{t} ; \boldsymbol{\theta}\right), \boldsymbol{s}_{t}, \boldsymbol{h}_{0} ; \boldsymbol{\psi}_{I}\right)\right)\right] ) \\ 
\mathcal{L}_{S t o r y} &amp;amp;=\mathbb{E}_{(\boldsymbol{X}, \boldsymbol{S})}\left[\log D_{S}\left(\boldsymbol{X}, \boldsymbol{S} ; \boldsymbol{\psi}_{S}\right)\right]+\mathbb{E}_{(\epsilon, \boldsymbol{S})}\left[\log \left(1-D_{S}\left(\left[G\left(\boldsymbol{\epsilon}_{t}, \boldsymbol{s}_{t} ; \boldsymbol{\theta}\right)\right]_{t=1}^{T}, \boldsymbol{S} ; \boldsymbol{\psi}_{S}\right)\right)\right]
\end{aligned}\]

&lt;p&gt;where $\theta$, $\psi_{I}$ and $\psi_{S}$ denotes the parametes of generator, image discriminator and story discriminator, respectively.&lt;/p&gt;

&lt;p&gt;The algorithm outlines is given:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/storyGAN/algo.jpg&quot; alt=&quot;algo&quot; /&gt;&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of StoryGAN: A Sequential Conditional GAN for Story Visualization</summary></entry><entry><title type="html">Image generation from scene graphs</title><link href="/2019/07/13/Sg2im.html" rel="alternate" type="text/html" title="Image generation from scene graphs" /><published>2019-07-13T00:00:00+08:00</published><updated>2019-07-13T00:00:00+08:00</updated><id>/2019/07/13/Sg2im</id><content type="html" xml:base="/2019/07/13/Sg2im.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1804.01622&quot;&gt;&lt;em&gt;Image generation from scene graphs&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Previous work only performs well on &lt;strong&gt;limited domains&lt;/strong&gt; such as followers or birds, but struggle to generate complex sentences with &lt;strong&gt;many objects and relationships&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This paper proposes a method for generating images from scene graphs with graph convolution which enable reasoning about objects and their relationships.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/sg2im/comp_stackG_sg2im.jpg&quot; alt=&quot;comp_stackG_sg2im&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;p&gt;In summary, &lt;em&gt;image generation network&lt;/em&gt; $f$ takes a scene graph $G$ and noise $z$ as input and outputs and image $\hat{I}=f(G, z)$.&lt;/p&gt;

&lt;p&gt;Firstly, embedding vectors representing objects and edges were generated from graph convolution network, which is then concatenated to predict bounding boxes and segementation masks for each predicted object.&lt;/p&gt;

&lt;p&gt;After obtaining the scene layout, which contains several bounding-boxes and masks, this high-dimensional representation is fed into &lt;em&gt;cascaded refinement network&lt;/em&gt; &lt;strong&gt;TODO&lt;/strong&gt; combined with noise to generate the final result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/sg2im/net_arch.jpg&quot; alt=&quot;net_arch&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;scene-graph&quot;&gt;Scene graph&lt;/h3&gt;

&lt;p&gt;As the first stage of processing, each node and edge of the graph from the dataset is converted to a dense vector by an embedding layer, analogous to the embedding layer which is used to get word-embeddings in neural language model.&lt;/p&gt;

&lt;h3 id=&quot;graph-convolution-network&quot;&gt;Graph Convolution Network&lt;/h3&gt;

&lt;p&gt;A 2D convolution layer takes the above vectors as an input, and produces the output vectors with neighbourhood information of its corresponding input vector. In this way, convolution operation aggregates the information across the neighbourhood of the input.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/sg2im/graph_conv.jpg&quot; alt=&quot;graph_conv&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the picture shown and concretely speaking, the output vectors $v_{i}^{\prime}$ for edges is simply computed by $v_{r}^{\prime}=g_{p}\left(v_{i}, v_{r}, v_{j}\right)$, while the calculation of objects vectors is a bit more complicated since the same object may participate in many relationships, e.g. A car is on the street. The street is besides the buildings. where ‘street’ acts as the starting node in the first edge while being the ending node in the second.&lt;/p&gt;

&lt;p&gt;To this end, $g_{s}$ and $g_{o}$ is used to compute a &lt;strong&gt;candidate vector&lt;/strong&gt;:&lt;/p&gt;

\[\begin{array}{l}{V_{i}^{s}=\left\{g_{s}\left(v_{i}, v_{r}, v_{j}\right) :\left(o_{i}, r, o_{j}\right) \in E\right\}} \\ {V_{i}^{o}=\left\{g_{o}\left(v_{j}, v_{r}, v_{i}\right) :\left(o_{j}, r, o_{i}\right) \in E\right\}}\end{array}\]

&lt;p&gt;Then the ouput vector $v_{i}^{\prime}$ for object is computed as $v_{i}^{\prime}=h\left(V_{i}^{s} \cup V_{i}^{o}\right)$ where $h$ is a symmetric function &lt;strong&gt;TODO&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;scene-layout&quot;&gt;Scene layout&lt;/h3&gt;

&lt;p&gt;After embedding vectors for objects was obtained, which aggregates all information across all objects and relationships in the edge, &lt;em&gt;scene layout&lt;/em&gt; will be calculated to give the coarse 2D location information.&lt;/p&gt;

&lt;p&gt;As below image shown, &lt;em&gt;mask regression network&lt;/em&gt; and &lt;em&gt;box regression network&lt;/em&gt; are built to predict a soft binary mask and a bounding-box respectively. The previous consists of several transpose convolutions terminating in sigmoid nonlinear function so that the mask lies in the range $(0,1)$ while the second is a MLP.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/07/sg2im/scene_layout.jpg&quot; alt=&quot;scene_layout&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The mask embedding of shape $D \times M \times M$ &lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;cascaded-refinement-network&quot;&gt;Cascaded refinement network&lt;/h3&gt;

&lt;p&gt;This network consists of a series of convolutional refinement modules, doubling the resolution and proceeding in a corase-to-fine manner.&lt;/p&gt;

&lt;p&gt;Each module receives both the &lt;em&gt;scene layout&lt;/em&gt; and the output from the prvious module as its input, concatenated them channelwise, upsampling them with nearest-neighbor interpolation and then passing it to the next, and finally generate the output result.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;h3 id=&quot;discriminator&quot;&gt;Discriminator&lt;/h3&gt;

&lt;p&gt;A pair of discriminator is built.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The patch-based image discriminator &lt;strong&gt;TODO&lt;/strong&gt; $D_{img}$ ensures the overall appearance of generated images are realistic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The object discriminator $D_{obj}$ has two functions:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ensuring objects in the image appears realistic by taking picels of object as inputs.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ensuring each object is recognizable by an auxiliary classifier to predict category.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt;

&lt;p&gt;The loss consists of six components.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Box loss $\mathcal{L}_{b o x}=\sum_{i=1}^{n}\left\|b_{i}-\hat{b}_{i}\right\|_{1}$  penalizing the $L_{1}$ distance between ground-truth and predicted boxes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mask loss $\mathcal{L}_{\text { mask }}$ penalizing differences between ground-truth and predicted masks with pixelwise cross-entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overal pixel loss $\mathcal{L}_{p i x}=\|I-\hat{I}\|_{1}$ penalizing the $L_{1}$ distance between ground-truth and generated images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image adversarial loss $\mathcal{L}_{G A N}^{i m g}$ from $D_{img}$ encouraging generated image patches to appear realistic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Object adversarial loss $\mathcal{L}_{G A N}^{o b j}$ from $D_{obj}$ encouraging each generated object to look realistic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Auxiliarly classifier loss $\mathcal{L}_{A C}^{o b j}$ from $D_{obj}$ ensuring each generated object can be classified by $D_{obj}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Compared to previous work, this paper uses structured text description to reason more explictly about objects and its relationships and generate more complex images with multiple objects.&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Image generation from scene graphs</summary></entry><entry><title type="html">Tmux tutorial</title><link href="/2019/06/20/Tmux-cookbook.html" rel="alternate" type="text/html" title="Tmux tutorial" /><published>2019-06-20T00:00:00+08:00</published><updated>2019-06-20T00:00:00+08:00</updated><id>/2019/06/20/Tmux-cookbook</id><content type="html" xml:base="/2019/06/20/Tmux-cookbook.html">&lt;p&gt;This is the tutorial of Tmux.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Tmux is a nice and user-friendly software used on command line. It ensures the current task running even if the SSH is broken down abnormally, that is this software can replace the tradition &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nohup&lt;/code&gt; to some extent.&lt;/p&gt;

&lt;p&gt;Briefly speaking, there are two features in this software.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;split the windows into different and independent sub-terminal, and raise the efficiency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reduce the risks of disconnection of SSH and increase security.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;component&quot;&gt;Component&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;session&lt;/li&gt;
  &lt;li&gt;windows&lt;/li&gt;
  &lt;li&gt;pane&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;session-management&quot;&gt;session management&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;create new session&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tmux new &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &amp;lt;session_name&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;list session&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tmux &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;connect to the last &lt;the designated=&quot;&quot;&gt; session&lt;/the&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tmux a &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &amp;lt;session_name&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;close the session&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tmux kill-session &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &amp;lt;session_name&amp;gt;]
tmux kill-server &lt;span class=&quot;c&quot;&gt;# close all&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;list session in tmux: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + s&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rename the session: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + $&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;detach from the present session: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + d&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;windows-management&quot;&gt;windows management&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;create a new windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + c&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rename the windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + ,&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;list windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + w&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;next/previous/last windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + n/p/l&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;choose windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + 0~9&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;close windows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + &amp;amp;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pane-management&quot;&gt;pane management&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;create horizonal pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + %&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;create vertical pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + &quot;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;switch pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + up/down/left/right&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;display the pane number: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + q&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;close pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + x&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rearrange pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + space&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;show time: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + t&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;zone in the pane: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix + z&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;create a tmux config file&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim ~/.tmux.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;reconfigure the prefix and mouse mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;append the following content to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.tmux.conf&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# remap prefix from 'C-b' to 'C-a'&lt;/span&gt;
set-option &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; prefix C-x
unbind-key C-b
bind-key C-x send-prefix

&lt;span class=&quot;c&quot;&gt;# Use Alt-arrow keys to switch panes&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; M-Left &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-pane&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; M-Right &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-pane&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; M-Up &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-pane&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; M-Down &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-pane&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Mouse mode&lt;/span&gt;
setw &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; mode-mouse on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;restart the tmux&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tmux source-file ~/.tmux.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="cookbook" /><summary type="html">This is the tutorial of Tmux.</summary></entry><entry><title type="html">Semi-parametric Image Synthesis</title><link href="/2019/05/28/Semi-param_img_syn.html" rel="alternate" type="text/html" title="Semi-parametric Image Synthesis" /><published>2019-05-28T00:00:00+08:00</published><updated>2019-05-28T00:00:00+08:00</updated><id>/2019/05/28/Semi-param_img_syn</id><content type="html" xml:base="/2019/05/28/Semi-param_img_syn.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1804.10992v1&quot;&gt;&lt;em&gt;Semi-parametric Image Synthesis&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;It presents a semi-parametric approach of photographic image synthesis from semantic layouts.&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Parametric method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Current parametric method generally refers to using deep networks to represent all data concerning photographic appearance in weights. It has advantages of end-to-end training over highly expressive models.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Non-parametric method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Non-parametric method in the past draw on the database of image segments which is used to retrieve photographic references provided as source material. It has an ability to draw on large databases of original photographic content at test time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semi-parametric method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This paper combines the complementary strengths of parametric and nonparametric techniques. Given a semantic layout, the system retrieves compatible segments from the database. The retrieved segments are used as raw material for synthesis and then composited onto the canvas. Deep networks help to align the components, resolve occlusion relationships and rectify the canvas to the final photographic image as output.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Analogy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, it draws an analogy that the practices of human painter who do not draw purely on memory, comparing to the weights of neural networks, but also use external references from around actual world, in contrast to the database of image segmentations, to reproduce the detailed object appearance.&lt;/p&gt;

&lt;h2 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h2&gt;

&lt;p&gt;The first and crucial thing is to build the External memory bank (database) $M$. A set of color images and corresponding semantic layouts make a pair to generate $M$ with different semantic categories.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/semi_param_img_syn/canvas_build.png&quot; alt=&quot;canvas_build&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At test time&lt;/strong&gt;, Semantic map $L \in{0,1}^{h\times w \times c} $, where $h \times w$ is the size of images and $c$ is the number of semantic classes, which was not seen during training, is decomposed into connected components $\left\{L_{i}\right\}$. For each $\left\{L_{i}\right\}$, a compatible segment $P_{i}$ from $M$ is retrieved based on shape, location and context and then aligned to $L_{i}$ by a spatial Transformation network. With the help of the Ordering network, relative front-back order of segments is determined and canvas $C$ is synthesized with deliberately elided boundaries of retrieved segments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/semi_param_img_syn/img_synthesis.png&quot; alt=&quot;img_synthesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Synthesis network $f$ uses the canvas $C$ and the input layout $L$ as input, inpainting the missing regions, harmonizing retrieved segments, blending boundaries, synthesizing shadows and otherwise adjusting the final appearance.&lt;/p&gt;

&lt;p&gt;Furthermore, &lt;a href=&quot;https://arxiv.org/abs/1707.09405&quot;&gt;cascaded refinement network&lt;/a&gt; is used to convert coarse imcomplete layouts to dense pixelwise layouts.&lt;/p&gt;

&lt;h2 id=&quot;canvas-building&quot;&gt;Canvas building&lt;/h2&gt;

&lt;h3 id=&quot;representation-of-memory-bank&quot;&gt;Representation of memory bank&lt;/h3&gt;

&lt;p&gt;A segment $P_{i}$ is associated with a tuple $\left(P_{i}^{\text {color}}, P_{i}^{\text {mask}}, P_{i}^{\text {cont}}\right)$, where $P_{i}^{c o l o r} \in \mathbb{R}^{h \times w \times 3}$ is a color image that contains the segment (other pixels are zeroed out as showed in the first image), $P_{i}^{m a s k} \in{0,1}^{h \times w \times c}$ is a binary mask that specifies the segment’s footprint, and $P_{i}^{c o n t} \in{0,1}^{h \times w \times c}$ is a semantic map representing the semantic context around $P_{i}$ within a bounding box enlarged by 25% to the $P_{i}^{\text {color}}$.&lt;/p&gt;

&lt;h3 id=&quot;segment-retrieval&quot;&gt;Segment retrieval&lt;/h3&gt;

&lt;p&gt;Given a semantic layout $L$ at test time, $L_{j}^{\text {mask}}$ and $L_{j}^{\text {cont}}$ is computed for each semantic segment $L_{j}$, by analogy with the definitions of $P_{i}$. Then the most compatible segment $P_{\sigma(j)}$ is computed over the intersection-over-union score &lt;strong&gt;TOREAD&lt;/strong&gt; $\operatorname{IoU}$ as follows where the first term measures the overlap of the shapes while another measures the similarity of surrounding which helps the retrieve when surrounding affects appearance. $i$ iterates over segments in $M$ that have the same semantic class as $L_{i}$.&lt;/p&gt;

\[\sigma(j)=\underset{i}{\arg \max } \operatorname{IoU}\left(P_{i}^{\operatorname{mask}}, L_{j}^{\operatorname{mask}}\right)+\operatorname{IoU}\left(P_{i}^{c o n t}, L_{j}^{c o n t}\right)\]

&lt;h3 id=&quot;transformation-network&quot;&gt;Transformation network&lt;/h3&gt;

&lt;p&gt;The transformation network $T$ is designed to transform $P_{\sigma(j)}$ to align $L_{j}$ via translation, rotation, scaling and clipping while preserving the integrity of the appearance.&lt;/p&gt;

&lt;p&gt;By simulating the inconsistencies in shape, scale and location that $T$ encounters at test time, $T$ can be trained with the input $\hat{P}_{i}^{\text { color }}$ which is applied random affine transformations and cropped from $P_{i}^{\text { color }}$.&lt;/p&gt;

&lt;p&gt;The loss function for $T$ is as follows, and is defined over the color images rather than the mask so as to be more &lt;strong&gt;specific&lt;/strong&gt; and better to constran the transformation.&lt;/p&gt;

\[\mathcal{L}_{T}\left(\theta^{T}\right)=\sum_{P_{i} \in \mathbf{M}}\left\|P_{i}^{c o l o r}-T\left(P, P_{i}^{m a s k}, \hat{P}_{i}^{c o l o r} ; \theta^{T}\right)\right\|_{1}\]

&lt;h3 id=&quot;ordering-network&quot;&gt;Ordering network&lt;/h3&gt;

&lt;p&gt;The ordering network is to determine the front-back ordering of adjacent object segments whose output is a $c$-dimensional one-hot vector as multi-classcification problem with &lt;strong&gt;absolute&lt;/strong&gt; cross-entropy loss.&lt;/p&gt;

&lt;p&gt;For the training of this network, it is totally take the advantage of datasets like Cityscapes and NYU.&lt;/p&gt;

&lt;h2 id=&quot;image-synthesis&quot;&gt;Image synthesis&lt;/h2&gt;

&lt;h3 id=&quot;encoder-decoder-architecture&quot;&gt;Encoder-decoder architecture&lt;/h3&gt;

&lt;p&gt;The synthesis network $f$ has an encoder-decoder structure with skip connections to synthesize the final photo.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder constructs a multiscale representation of the input $\left(C, L \right)$ based on VGG-19, and capture long-range correlations that can help the decoder harmonize color, lighting, and texture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder uses this representation to synthesize progressively finer feature maps, culmiating in full-resolution output. It also based on the &lt;a href=&quot;https://arxiv.org/abs/1707.09405&quot;&gt;cascaded refinement network&lt;/a&gt; as the framework showed above.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;To train the network $f$, artifacts of canvas with poor quality at test time must be simulated. Given a semantic layout $L$ and a corresponding color image $I$ from the training set, stenciling, color transfer and boundary elision to the pair $\left(I, L \right)$ is implemented to synthesize the simulated canvas $C^{\prime}$ . Thus $f$ is trained to take the pair $\left(C^{\prime}, L\right)$ and recover the original image $I$.&lt;/p&gt;

\[\mathcal{L}_{f}\left(\theta^{f}\right)=\sum_{(I, L) \in \mathcal{D}} \sum_{l} \lambda_{l}\left\|\Phi_{l}(I)-\Phi_{l}\left(f\left(C^{\prime}, L\right) ; \theta^{f}\right)\right\|_{1}\]

&lt;p&gt;where $\Phi_{l}$ is the feature tensor in layer $l$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/semi_param_img_syn/training_pipeline.png&quot; alt=&quot;training_pipeline&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stenciling: for each training image $I$ which is masked out the region of retrieved segments from set training, the network $f$ will learn to generate the context and foreground.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Color transfer: different segments on the canvas generally have inconsistent tone and illumination since xxxx. Therefore, to modify the color distribution of $P_{j}$ in $C^{\prime}$, segment $P_{i}$ with the same semantic class fom $M$ is retrieved and used as the &lt;strong&gt;target&lt;/strong&gt; of transferring.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boundary elision: segment boundary is masked out randomly so as that the network $f$ is forced to learn to synthesize content near boundaries. Inconsistencies along boundries arise not only inside segments. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;SIMS approach demonstrate to produce considerably more realistic image than recent purely parametric techniques (entirely depend on weights of neural network without database).&lt;/p&gt;

&lt;p&gt;SIMS in a sense lower-bounded by the performance of parametric methods, that if the memory bank is not useful, the network $f$ can simply ignore the canvas and perform parametric synthesis based on the input semantic layout. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Future work includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;acceleration of SIMS&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;other forms of input can be used, like semantic instance segmentation or textual descriptions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;not end-to-end&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the frontier of video synthesis&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Semi-parametric Image Synthesis</summary></entry><entry><title type="html">Text2Scene: Generating Compositional Scenes from Textual Descriptions</title><link href="/2019/05/22/Text_to_scene.html" rel="alternate" type="text/html" title="Text2Scene: Generating Compositional Scenes from Textual Descriptions" /><published>2019-05-22T00:00:00+08:00</published><updated>2019-05-22T00:00:00+08:00</updated><id>/2019/05/22/Text_to_scene</id><content type="html" xml:base="/2019/05/22/Text_to_scene.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/pdf/1809.01110&quot;&gt;&lt;em&gt;Text2Scene: Generating Compositional Scenes from Textual Descriptions&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Unlike the previous work, this paper doesn’t adopt Generative adversarial network, but a combination of encoder-decoder architecture with a semi-parametric retrieval-based approach &lt;strong&gt;TOREAD&lt;/strong&gt;. Under minor modification, this model performs decent generation of different forms of scene representation, including clip-art generation on &lt;em&gt;Abstract Scenes&lt;/em&gt;, semantic layout on &lt;em&gt;COCO&lt;/em&gt; and compositional image generation on &lt;em&gt;COCO&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;difficulties-encountered&quot;&gt;Difficulties encountered&lt;/h3&gt;

&lt;p&gt;Generating rich textual representation has two main challenges, one is indirect hint at the presense of attributes from input textual description (e.g. “Mike is surprised” should change facial attributes on the generated object “Mike”), and another is relative spatial configurations within the text (e.g. “Mike is runing towards to his girlfriend” confines the orientation of “Mike” dependent on “his girlfriend”)&lt;/p&gt;

&lt;h3 id=&quot;related-work&quot;&gt;Related work&lt;/h3&gt;

&lt;p&gt;Relative work is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.10992&quot;&gt;&lt;em&gt;Semi-parametric Image Synthesis&lt;/em&gt;&lt;/a&gt; proposed a retrieval-based semi-parametric method for image synthesis given an input by a human. But different from the previous work using ground-truth semantic layout as input, this work learns to &lt;strong&gt;predict&lt;/strong&gt; the location and layout of the object indirectly from the text.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;&lt;em&gt;Image generation from scene graphs&lt;/em&gt;&lt;/a&gt;, it proposed a graph-convolution model to generate from structured scene graph where objects and their relationship are provided as inputs, while this work the presence of objects is inferred from text.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;&lt;em&gt;Inferring semantic layout for hierarchical text-to-image synthesis&lt;/em&gt;&lt;/a&gt; generates the layout as the intermediate representations in separably trained modules, but this work generate pixel-wise outputs with semi-parametric retrieval module without advesarial training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;&lt;em&gt;Visual dialog for collaborative drawing&lt;/em&gt;&lt;/a&gt; performs pictorial generation from chat logs, compared to out works that need text much considerably more underspecified.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;network-structure&quot;&gt;Network structure&lt;/h2&gt;

&lt;p&gt;Using a sequence-to-sequence approach, this model arranges the generated object sequentially along with their attributes (locations, sizes, aspect ratios, pose, appearance and more) on an initially empty canvas.&lt;/p&gt;

&lt;p&gt;Generally, Text2Scene model consists of&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a text encoder that maps input sentence to a set of embedding representations&lt;/li&gt;
  &lt;li&gt;an object decoder that predicts the next generated object conditioned on the current scene state&lt;/li&gt;
  &lt;li&gt;an attribute decoder that determines the attribute of the next object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/text_to_scene/network_arch.png&quot; alt=&quot;network_arch&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;text-encoder&quot;&gt;Text encoder&lt;/h3&gt;

&lt;p&gt;To compute for each word $i$ given input text&lt;/p&gt;

\[h_{i}^{E}=\operatorname{BiGRU}\left(x_{i}, h_{i-1}^{E}, h_{i+1}^{E}\right)\]

&lt;p&gt;here BIGRU is a bidirectional GRU cell, $ x_{i} $ is a word embedding and $h_{i}^{E}$ is a vector encoding the current word and its context.&lt;/p&gt;

&lt;h3 id=&quot;image-encoder-and-recurrent-module&quot;&gt;Image encoder and recurrent module&lt;/h3&gt;

&lt;p&gt;To use a convolutional network $\Omega$ to encode current canvas ${B}_{t}$ into a $\mathcal{C} \times H \times W$ feature map representing the current scene state, and to model the history of the scene state $ \left\{ h_{t}^{D} \right\} $ by a convolutional GRU. $ h_{t}^{D}$ is an important representation of both temporal and spatial dynamical information.&lt;/p&gt;

\[h_{t}^{D}=\operatorname{ConvGRU}\left(\Omega\left(B_{t}\right), h_{t-1}^{D}\right)\]

&lt;p&gt;In case that $h_{t}^{D}$ fails to capture small objects, previous step $ {o}_{t-1} $ is provided as input to the downstream decoders.&lt;/p&gt;

&lt;h3 id=&quot;attention-based-object-decoder&quot;&gt;Attention-based object decoder&lt;/h3&gt;

&lt;p&gt;The object decoder based on attention mechanism outputs the likelihhod score on all objects in object vocabulary library $\mathcal{V}$, and takes as input:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the current scene state $ \left\{ h_{t}^{D} \right\} $&lt;/li&gt;
  &lt;li&gt;the text features $ \left\{\left(h_{i}^{E}, x_{i}\right)\right\} $&lt;/li&gt;
  &lt;li&gt;the preiviously predicted object $ o_{t-1} $&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;spatial-attention-convolutional-network&quot;&gt;Spatial attention convolutional network&lt;/h4&gt;

&lt;p&gt;where $\Psi$ is a convolutional network with &lt;strong&gt;spatial attention&lt;/strong&gt; on $ \left\{ h_{t}^{D} \right\} $ to collect spatial contexts for the object prediction, e.g. what objects have already been added. Then by average pooling layer, the attended features are fused into a vector $u_{t}^{o}$.&lt;/p&gt;

\[u_{t}^{o}=\text { AvgPooling }\left(\Psi^{o}\left(h_{t}^{D}\right)\right)\]

&lt;h4 id=&quot;text-based-attention-module&quot;&gt;Text-based attention module&lt;/h4&gt;

&lt;p&gt;Text-based attention module $\Phi$ uses $u_{t}^{o}$ to attend to the text features $ \left\{\left(h_{i}^{E}, x_{i}\right)\right\} $, and encodes the knowledge of all described objects having been added to the scene thus far ideally.&lt;/p&gt;

\[c_{t}^{o}=\Phi^{o}\left(\left[u_{t}^{o} ; o_{t-1}\right],\left\{\left(h_{i}^{E}, x_{i}\right)\right\}\right)\]

&lt;h4 id=&quot;likelihood-predicting-perceptrons&quot;&gt;Likelihood predicting perceptrons&lt;/h4&gt;

&lt;p&gt;$\Theta^{o}$ is two-layers perceptrons to predict &lt;em&gt;the likelihhod of the next object&lt;/em&gt; $p\left(o_{t}\right)$ using a softmax function.&lt;/p&gt;

\[p\left(o_{t}\right) \propto \Theta^{o}\left(\left[u_{t}^{o} ; o_{t-1} ; c_{t}^{o}\right]\right)\]

&lt;h3 id=&quot;attention-based-attribute-decoder&quot;&gt;Attention-based attribute decoder&lt;/h3&gt;

&lt;p&gt;For each spatial location in $h_{t}^{D}$, This part predicts both location likelihood $ \left\{l_{t}^{i}\right\}_{i=1 \ldots N} $ and attribute likelihoods $\left\{R_{t}^{k}\right\}$ to the object $o_{t}$. Here possible locations are discretized into the same resolution of $h_{t}^{D}$. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;zoom-in-module&quot;&gt;“Zoom in” module&lt;/h4&gt;

&lt;p&gt;$\Phi^{a}$ is to zoom in the language context of $o_{t}$ by attending to the input text feature.&lt;/p&gt;

\[{c_{t}^{a}=\Phi^{a}\left(o_{t},\left\{\left(h_{i}^{E}, x_{i}\right)\right\}\right)}\]

&lt;h4 id=&quot;location-prediction-convolution-network&quot;&gt;Location prediction convolution network&lt;/h4&gt;

&lt;p&gt;Compared to $c_{t}^{o}$ which contain information of objects that have not been added yet, $c_{t}^{a}$ focuses more specifically on contents realted to the current object $o_{t}$.&lt;/p&gt;

\[{u_{t}^{a}=\Psi^{a}\left(\left[h_{t}^{D} ; c_{t}^{a}\right]\right)}\]

&lt;p&gt;Then $\Psi^{a}$, a CNN spatially attending to $h_{t}^{D}$, is to find an affordable location to append $o_{t}$.&lt;/p&gt;

&lt;h4 id=&quot;likelihood-predicting-convolutional-network&quot;&gt;Likelihood predicting convolutional network&lt;/h4&gt;

&lt;p&gt;$\Theta^{a}$ is implemented by a convolutional network with softmax classifiers over each value of $l_{t}$ and the discrete $R_{t}^{k}$.&lt;/p&gt;

\[{p\left(l_{t},\left\{R_{t}^{k}\right\}\right)=\Theta^{a}\left(\left[u_{t}^{a} ; o_{t} ; c_{t}^{a}\right]\right)}\]

&lt;h3 id=&quot;foreground-patch-embedding&quot;&gt;Foreground patch embedding&lt;/h3&gt;

&lt;p&gt;For the third mission to generate images composed of patches retrieved from others, a particular $Q_{t}$ is proposed to predict every location in the output feature map but is used at test time to retrieve similar patches from pre-computed collection of object segments from other images &lt;strong&gt;TOREAD&lt;/strong&gt;. A patch embedding network using a CNN reduces the foreground patch of target image into a 1D vector $F_{t}$. To minimize the $\ell_{2}$-distance between $Q_{t}$ and $F_{t}$, it uses the triplet embedding loss ($P^\text {color}$, $P^\text {mask}$, $P^\text {context}$) to minimize the distance of $\left\|Q_{t}, F_{t}\right\|_{2}$ and maximize the distance of $\left\|Q_{t}, F_{k}\right\|_{2}$. Here $F_{k}$ is the feature of a &lt;em&gt;negative&lt;/em&gt; patch randonly selected from the same category of $F_{t}$. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

\[L_{\text {triplet}}\left(Q_{t}, F_{t}\right)=\max \left\{\left\|Q_{t}, F_{t}\right\|_{2}-\left\|Q_{t}, F_{k}\right\|_{2}+\alpha, 0\right\}\]

&lt;p&gt;where $\alpha$ is a margin hyper-parameter.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;

&lt;p&gt;The loss function with reference values $\left(O_{t}, l_{t},\left\{R_{t}^{k}\right\}, F_{t}\right)$ is:&lt;/p&gt;

\[L= -w_{o} \sum_{t} \log p\left(o_{t}\right)-w_{l} \sum_{t} \log p\left(l_{t}\right) -\sum_{k} w_{k} \sum_{t} \log p\left(R_{t}^{k}\right)\\ 
+w_{e} \sum_{t} L_{t r i p l e t}\left(Q_{t}, F_{t}\right) +w_{a}^{O} L_{a t t n}^{O}+w_{a}^{A} L_{a t t n}^{A}\]

&lt;p&gt;where $L_{a t t n}^{*}$ are regularization terms inspired by the doubly stochastic attention module propose in section 4.2.1 of &lt;a href=&quot;https://arxiv.org/abs/1502.03044&quot;&gt;&lt;em&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/em&gt;&lt;/a&gt; and $w$ are hyper-parameters controlling the relative contribution of each loss.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Text2Scene model demonstrates the capacity on both abstract and real images, which opens the possibility for future work on transfer learning across domains.&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Text2Scene: Generating Compositional Scenes from Textual Descriptions</summary></entry><entry><title type="html">Learning where and what to draw</title><link href="/2019/05/18/Text_to_image-2.html" rel="alternate" type="text/html" title="Learning where and what to draw" /><published>2019-05-18T00:00:00+08:00</published><updated>2019-05-18T00:00:00+08:00</updated><id>/2019/05/18/Text_to_image-2</id><content type="html" xml:base="/2019/05/18/Text_to_image-2.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1610.02454&quot;&gt;&lt;em&gt;Learning where and what to draw&lt;/em&gt;&lt;/a&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Previous method so far only used conditioning variable such as a class label or non-localized caption, and didn’t allow for controlling the location information of objects.&lt;/p&gt;

&lt;p&gt;This model learns to perform content controllable and location controllable image synthesis, that is what and where. There are two ways to encode spatial constraints, one is incorporating spatial masking and cropping modules into a text-conditional GAN with spatial transformers, another is locating part of the object by a set of normalized coordinates (x, y) with multiplicative gating mechanism.&lt;/p&gt;

&lt;h2 id=&quot;previous-work&quot;&gt;Previous work&lt;/h2&gt;

&lt;p&gt;Two types of transformation from text to image has been proposed, trained to learn one-to-one mappings from the latent space to pixel space, and to learn probabilistic models to approximate the distribution of each pixel &lt;strong&gt;TOREAD&lt;/strong&gt;. Besides, GAN has relatively better sharpness compared to VAE models in general.&lt;/p&gt;

&lt;p&gt;Spatial Transformer Networks (STN) is an effective visual attention mechanism&lt;/p&gt;

&lt;h2 id=&quot;network-structure&quot;&gt;Network structure&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;GAN&lt;/strong&gt; and &lt;strong&gt;Joint embedding structure&lt;/strong&gt; used in this model is articulated in the note of &lt;a href=&quot;/posts/2019/05/text_to_image_1/&quot;&gt;Generative Adversarial Text to Image Synthesis&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;bounding-box-conditional-model&quot;&gt;Bounding-box conditional model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/text_to_image_2/bounding-box-model.png&quot; alt=&quot;bounding-box model&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;keypoint-conditional-model&quot;&gt;Keypoint conditional model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/text_to_image_2/keypoint-model.png&quot; alt=&quot;keypoint model&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conditional-keypoint-generation-model&quot;&gt;Conditional keypoint generation model&lt;/h3&gt;

&lt;p&gt;The purpose is to have acceess to all of the conditional distributions of keypoints, given a subset of observed keypoints and the text description. However, a simple autoencoder is sparse and would not suffice.&lt;/p&gt;

&lt;p&gt;The keypoint generation GAN is proposed to build datasets with full keypoints using gating mechanism given only a few of visible variable.&lt;/p&gt;

&lt;p&gt;The discriminator $D$ of the GAN is simple which only distinguish real keypoints and text $ \left( \mathbf{k}_{real}, \mathbf{t}_{real} \right) $  from synthetic keypoints.&lt;/p&gt;

&lt;p&gt;The generator $G$ is relatively complicated and formulated as follows:&lt;/p&gt;

\[G_{k}(z, \mathbf{t}, \mathbf{k}, \mathbf{s}) :=\mathbf{s} \odot \mathbf{k}+(1-\mathbf{s}) \odot f(z, \mathbf{t}, \mathbf{k})\]

&lt;p&gt;where $k_{i}:=\left\{ x_{i}, y_{i}, v_{i} \right\}$, $i = 1,…,K$ and $x$ and $y$ indicate thee rows and columns position, $v$ is a bit set to 1 if the part is absolutely visible and 0 otherwise. Note that $\mathbf{k} \in[0,1]^{K \times 3}$ encode the keypoints into a matrix. Switch units $\mathbf{s} \in{0,1}^{K}$ is to specified previously by for example user, which set the $i$-th entry of $K$ to 1 if the corresponding $i$-th part is 1 and 0 otherwise. $\odot$ denotes pointwise multiplication and $f$ is a 3-layer fully-connected network to transform concatenated $z$, $t$ and flattened $k$ of shape $\mathbb{R}^{Z+T+3 K}$ to the shape of $\mathbb{R}^{3 K}$&lt;/p&gt;

&lt;p&gt;As stated above, parameters of $f$ is trainable. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order for $G_{k}$ to capture all of the conditional distributions over keypoints, during training switch units $s$ is randomly sampled.&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Learning where and what to draw</summary></entry><entry><title type="html">Generative Adversarial Text to Image Synthesis</title><link href="/2019/05/11/Text_to_image-1.html" rel="alternate" type="text/html" title="Generative Adversarial Text to Image Synthesis" /><published>2019-05-11T00:00:00+08:00</published><updated>2019-05-11T00:00:00+08:00</updated><id>/2019/05/11/Text_to_image-1</id><content type="html" xml:base="/2019/05/11/Text_to_image-1.html">&lt;p&gt;The note of &lt;a href=&quot;https://arxiv.org/abs/1605.05396&quot;&gt;&lt;em&gt;Generative Adversarial Text to Image Synthesis&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;For text to image, there are many plausible value to one pixel that correctly illustrate the description of the text, since the the distribution of image conditioned on the text is highly multimodal, e.g. small changes of text description may affect pixel values with low correlation across a wide range.&lt;/p&gt;

&lt;p&gt;For image to text, it is much practical to decompose the sequence according to the chain rule to generate captions. &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;gan&quot;&gt;GAN&lt;/h3&gt;

&lt;p&gt;It is common to maximize $\log (D(G(z)))$ for the generator, while it is found to be more effective to minimize maximize $\log (1-D(G(z)))$ instead.&lt;/p&gt;

\[\begin{aligned} \min _{G} \max _{D} V(D, G)=&amp;amp; \mathbb{E}_{x \sim p_{\text { data }}(x)}[\log D(x)]+\mathbb{E}_{x \sim p_{z}(z)}[\log (1-D(G(z)))] \end{aligned}\]

&lt;h3 id=&quot;joint-embedding-with-symmetric-structure&quot;&gt;Joint embedding with symmetric structure&lt;/h3&gt;

&lt;p&gt;The purpose of this structure is to obtain $\varphi(t)$ which can encode text description to visually-discriminative vector.&lt;/p&gt;

&lt;p&gt;Denote the classifier as followed, where $\phi(v)$ is the image encoder, and $\varphi(t)$ is the text encoder. Inner-product of vectors can be interpreted to some degree as the distance or similarity of two vectors.&lt;/p&gt;

\[f_{v}(v) =\underset{y \in \mathcal{Y}}{\arg \max } \mathbb{E}_{t \sim \mathcal{T}(y)}\left[\phi(v)^{T} \varphi(t)\right]\]

\[f_{t}(t) =\underset{y \in \mathcal{Y}}{\arg \max } \mathbb{E}_{v \sim \mathcal{V}(y)}\left[\phi(v)^{T} \varphi(t)\right]\]

&lt;p&gt;Then optimize the following structured loss:&lt;/p&gt;

\[\frac{1}{N} \sum_{n=1}^{N} \Delta\left(y_{n}, f_{v}\left(v_{n}\right)\right)+\Delta\left(y_{n}, f_{t}\left(t_{n}\right)\right)\]

&lt;p&gt;The intuition here is that a text encoding should have a higher compatibility score with image of the corresponding class and vice-versa &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;network-structure&quot;&gt;Network structure&lt;/h2&gt;

&lt;p&gt;There is no particular to the total architecture of network, except for the conditions which is encoded from the text description and concatenated to noise afterwards. Two different full-connected layers are implemented to encode text description both in upsampling and downsampling model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/text_to_image_1/network_arch.png&quot; alt=&quot;network_arch&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gan-cls&quot;&gt;GAN-CLS&lt;/h3&gt;

&lt;p&gt;Two sources of error should be considered: unrealistic (bad generation) images, and realistic image yet dismatched with the text.&lt;/p&gt;

&lt;p&gt;For this reason, apart from origin losses: real image with right text and fake image with false text, additional loss: real image with false text, is added to train the discriminator to have the ability of discriminate whether the generated image match the text or not, instead of only to measure the quality of output image.&lt;/p&gt;

\[\begin{aligned}
  s_{r} \leftarrow &amp;amp; D(x, h)\{\text { real image, right text }\} \\ 
  s_{w} \leftarrow &amp;amp; D(x, \hat{h})\{\text { real image, wrong text }\} \\ 
  s_{f} \leftarrow &amp;amp; D(\hat{x}, h)\{\text { fake image, right text }\} \\ 
  \mathcal{L}_{D} \leftarrow &amp;amp; \log \left(s_{r}\right)+\left(\log \left(1-s_{w}\right)+\log \left(1-s_{f}\right)\right) / 2
\end{aligned}\]

&lt;h3 id=&quot;gan_int&quot;&gt;GAN_INT&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deep networks have been shown to learn representations in which interpolations between embedding pairs tend to
be near the data manifold  (Bengio et al., 2013) &lt;strong&gt;TOREAD&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then laryge amount of additional text embeddings can be generated by simply interpolation between captions of training samples.&lt;/p&gt;

&lt;p&gt;The generator can learn an ability of generate the ‘interpolated image’ and fill in gaps on the data manifold between training points &lt;strong&gt;TOREAD&lt;/strong&gt;, while discriminator can learn to predict whether image and text pairs match or not.&lt;/p&gt;

&lt;p&gt;Note that there is no need to add label of interpolated vector for they are synthetic and fake.&lt;/p&gt;

\[\mathbb{E}_{t_{1}, t_{2} \sim p_{d a t a}}\left[\log \left(1-D\left(G\left(z, \beta t_{1}+(1-\beta) t_{2}\right)\right)\right)\right]\]

&lt;h3 id=&quot;disentangling-style-and-content&quot;&gt;Disentangling style and content&lt;/h3&gt;

&lt;p&gt;It is a &lt;strong&gt;validation method&lt;/strong&gt; to qualitify the ability of each proposed model to use the noise as style information, in other words, to determine the strength of the effect of noise z in different training methods&lt;/p&gt;

&lt;p&gt;By content, we mean the visual attributes of the bird itself, such as shape, size and color of each body part. By style, we mean all of the other factors of variation in the image such as background color and the pose orientation of the bird.&lt;/p&gt;

&lt;p&gt;Therefore, generative model must learn to use noise, the prior-condition, to represent style variations in order to generate realistic image.&lt;/p&gt;

&lt;p&gt;Inverting generative model $S$ for style transfer is designed to transfer the generative image back into noise z with the following loss,&lt;/p&gt;

\[\mathcal{L}_{\text {style}}=\mathbb{E}_{t, z \sim \mathcal{N}(0,1)}\|z-S(G(z, \varphi(t)))\|_{2}^{2}\]

&lt;blockquote&gt;
  &lt;p&gt;It is a bit difficult to understand the implication this model. To make sense of this, we can interpret this inverted model $S$ as to measure the ability of $G$ to output a generative image with both decent content and style in convergence case where $\mathcal{L}_{\text {style}}$ comes close to 0.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019/05/text_to_image_1/ROC_curve.png&quot; alt=&quot;ROC_curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For evaluation, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cosine similarity&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ROC&lt;/code&gt; is used as showed in the above figure. Images with similar and dissimilar content and style is selected and constructed as a pair, and if GAN has disentangled ability, the similarity between noises inverted from images of the same style (e.g. similar pose) should be higher than that of different styles (e.g. different pose).&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="paper read" /><category term="computer vision" /><category term="text2image" /><summary type="html">The note of Generative Adversarial Text to Image Synthesis</summary></entry><entry><title type="html">Git cookbook</title><link href="/2019/02/10/Git-cookbook.html" rel="alternate" type="text/html" title="Git cookbook" /><published>2019-02-10T00:00:00+08:00</published><updated>2019-02-10T00:00:00+08:00</updated><id>/2019/02/10/Git-cookbook</id><content type="html" xml:base="/2019/02/10/Git-cookbook.html">&lt;p&gt;This cookbook is built for quick and easy search for Git commands
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SSH reset&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; rsa &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;youremail@example.com&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# -C: comment&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -t: type&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;configure global/local user and email&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; user.name &lt;span class=&quot;s2&quot;&gt;&quot;Your Name&quot;&lt;/span&gt;
git config &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; user.email &lt;span class=&quot;s2&quot;&gt;&quot;email@example.com&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;set ssh-key&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;add ssh_pub key&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;list the content of config file&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config --list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;set the exectuable of files&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config core.filemode false  # local
git config --global core.fileMode false # global
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;set editor from nano to vim&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; git config --global core.editor &quot;vim&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or append &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;editor = vim&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;core&lt;/code&gt; of config file&lt;/p&gt;

&lt;h2 id=&quot;elementary&quot;&gt;Elementary&lt;/h2&gt;

&lt;h3 id=&quot;three-sectors&quot;&gt;Three sectors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;working tree: editing space&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;index file: cache after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;repository: storage after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commit&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;basic-pipeline&quot;&gt;Basic pipeline&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;initialization&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git init  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;add file contents of working tree to index files&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git add &amp;lt;file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;record changes to the repository&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git commit -m &quot;&amp;lt;message&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;add files to index files and record to the repo&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git commit -am &quot;&amp;lt;message&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;check the status of the repository&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;show changes between commits, commit and working tree&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# compare the working tree with index files
git diff
# compare the index files with HEAD repo (&amp;lt;commit_id&amp;gt; repo)
git diff --cached &amp;lt;commit_id&amp;gt;
# compare the two repos
git diff &amp;lt;commit_id&amp;gt; &amp;lt;commit_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;discard contents of working tree after editing&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git checkout -- &amp;lt;file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;discard files of index file after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git reset HEAD file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;discard new commit before &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;push&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git reset --hard HEAD^
git reset --hard &amp;lt;commit_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;discard a commit after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;push&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reset&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git reset --hard &amp;lt;commit_id&amp;gt;
...
git push --force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rebase&lt;/code&gt; &lt;strong&gt;TODO&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git rebase -i HEAD^2 # where 2 refers to the 2 most latest commit
# delete the most latest commit
git push --force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;revert&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git revert &amp;lt;commit_id&amp;gt; # traceback to the history commit with a new commit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;tail the log&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git log
git log --pretty=oneline --abbrev-commit --graph
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;tail the command log&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git reflog
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;remove&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git rm &amp;lt;file&amp;gt;
git commit -m &quot;xxxx&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;remove files of index files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: remove the cache after delete the directory if it has committed, or add .gitignore after commit the directory&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git rm -r --cached $DIR
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;branch&quot;&gt;Branch&lt;/h2&gt;

&lt;h3 id=&quot;basic&quot;&gt;Basic&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;create branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &amp;lt;new_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;rename the branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch -m [&amp;lt;old_name&amp;gt;] &amp;lt;new_name&amp;gt; # omitted old_name will signify the current branch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;switch branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git checkout &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;switch to the created branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git checkout -b &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;merge&quot;&gt;Merge&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;merge branch to the current branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git merge &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;delete local branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch -d &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;delete branch by force before merge&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch -D &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;delete remote branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git push origin --delete &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;check the relation between local branch and remote branch&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch -vv 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;fetch-remote-branch&quot;&gt;Fetch remote branch&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;change current branch to specified branch after clone&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clone&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone -b &amp;lt;remote_branch_name&amp;gt; &amp;lt;repo_link&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkout&lt;/code&gt; after initialization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/856ce249ed78&quot;&gt;reference link&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git init
git remote add origin &amp;lt;repo_link&amp;gt;
git checkout -b &amp;lt;new_name&amp;gt; &amp;lt;origin/remote_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;advanced&quot;&gt;Advanced&lt;/h2&gt;

&lt;h3 id=&quot;modify-the-commit&quot;&gt;Modify the commit&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;modify the commit log&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference &lt;a href=&quot;https://blog.csdn.net/sodaslay/article/details/72948722&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git commit --amend # to modify the
git push -f origin master # to push by force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;modify the commit date&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git commit --amend --date=&quot;$WEEK, $DAY $MONTH $YEAR $HOUR:$MINUTE:$SECOND +0800&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where +0800 refers to Beijing time region&lt;/p&gt;

&lt;h1 id=&quot;创建远程仓库&quot;&gt;创建远程仓库&lt;/h1&gt;

&lt;h2 id=&quot;添加远程库&quot;&gt;添加远程库&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;关联远程仓库：首次在Github上创建repository
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote add origin git@server-name:path/repo-name.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;删除关联远程仓库&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote rm origin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;首次 push 到远程仓库
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; git push -u origin master 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;非首次推送到远程仓库
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git push origin master 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;从远程仓库克隆
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone &amp;lt;gitURL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;branch-1&quot;&gt;Branch&lt;/h1&gt;

&lt;h2 id=&quot;创建与合并分支&quot;&gt;创建与合并分支&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;查看分支
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;创建分支
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;解决冲突&quot;&gt;解决冲突&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;禁止使用 fast forward merge 方式：当master 与 branch分别更改无法快速合并时，用普通模式合并，合并后的历史有分支，能看出来曾经做过合并
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git merge --no-ff -m &quot;xxx&quot;  &amp;lt;branch name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;img src=&quot;https://cdn.liaoxuefeng.com/cdn/files/attachments/001384909115478645b93e2b5ae4dc78da049a0d1704a41000/0&quot; alt=&quot;new_branch&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;git 查看分支情况
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git log --graph --pretty=oneline --abbrev-commit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;解决后：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.liaoxuefeng.com/cdn/files/attachments/001384909115478645b93e2b5ae4dc78da049a0d1704a41000/0&quot; alt=&quot;new_branch&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bug分支&quot;&gt;Bug分支&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;隐藏现场及回复现场
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git stash #保存现场
git stash apply  #保存现场
git stash drop  #删除现场
git stash pop #恢复并删除现场
git stash list #查看现场
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;多人协作&quot;&gt;多人协作&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote #查看远程库信息
git remote -v #查看远程库信息详细
git push origin master #推送本地 master 分支
git checkout -b dev origin/dev #创建本地 dev 并关联远程 dev 分支
git branch --set-upstream branch-name origin/branch-name #建立本地分支与远程分支得关联
git pull #抓取远程分支
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;创建标签
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git tag &amp;lt;branch_name&amp;gt;  #创建标签
git tag #查看所有标签
git tag v0.9 6224937 #对某一次 commit 打标签
git show &amp;lt;tagname&amp;gt; #查看标签信息
git tag -a v0.1 -m &quot;version 0.1 released&quot; 3628164 #创建有说明的标签
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;操作标签
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git tag -d v0.1 #删除标签
git push origin &amp;lt;tagname&amp;gt; #推送标签到远程
git push origin --tags #推送本地所有未推送到远程的标签
git push origin :refs/tags/&amp;lt;tagname&amp;gt; #删除远程标签
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;自定义git
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config --global color.ui true #配置颜色开启
git config --global alias.st status
git config --global alias.co checkout
git config --global alias.ci commit
git config --global alias.br branch
git config --global alias.unstage 'reset HEAD'
git config --global alias.last 'log -1'
git config --global alias.lg &quot;log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&amp;lt;%an&amp;gt;%Creset' --abbrev-commit&quot;
git config --global core.quotepath false # 设置显示中文文件名
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;清楚ssh密码
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh-keygen -p
当提升你输入新的密码的时候，按enter就可以啦，继续确认enter就可以
当然上面还有别的办法自己也可以试一下。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;git rm与git rm –cached的区别
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;当我们需要删除暂存区或分支上的文件, 同时工作区也不需要这个文件了, 可以使用
git rm file_path
git commit -m 'delete somefile'
git push
当我们需要删除暂存区或分支上的文件, 但本地又需要使用, 只是不希望这个文件被版本控制, 可以使用
git rm --cached file_path
git commit -m 'delete remote somefile'
git push
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;git-命令行删除远程分支&quot;&gt;git 命令行删除远程分支&lt;/h1&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch -r -d origin/branch-name
git push origin :branch-name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;git默认路径修改&quot;&gt;Git默认路径修改&lt;/h1&gt;
&lt;p&gt;在/etc/profile中最后加入&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# set Project Path
proj=&quot;G:\Github&quot; 
cd $proj
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;git-仓库清空&quot;&gt;Git 仓库清空&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.mamicode.com/info-detail-2158130.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;git-删除历史内容&quot;&gt;Git 删除历史内容&lt;/h1&gt;
&lt;p&gt;首先找出git中前五大的文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -g | tail -5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一行的字母其实相当于文件的id,用以下命令可以找出id 对应的文件名&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;git rev-list --objects --all | grep 8f10eff91bb6aa2de1f5d096ee2e1687b0eab007
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除匹配*.db的所有文件&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch *.db' --prune-empty --tag-name-filter cat -- --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除操作&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch &amp;lt;FILL YOUR FILE NAME OR ID HERE&amp;gt;' --prune-empty --tag-name-filter cat -- --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;收回空间&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;rm -rf .git/refs/original/ 
git reflog expire --expire=now --all
git gc --aggressive --prune=now
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;强行push&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gitbash&quot;&gt;git push -f origin master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/lout/p/6111739.html&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://blog.csdn.net/u010295496/article/details/72862671&quot;&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Haolin Jia</name><email>jiahaolin19971119@gmail.com</email></author><category term="cookbook" /><summary type="html">This cookbook is built for quick and easy search for Git commands</summary></entry></feed>